{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2 : Training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Training a neural network with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAD0CAYAAAB5LvVrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAFt5JREFUeJzt3X+QXXV9xvHnYUNAEFiFbQcSJotFGdFpNnRL6zD8RgsVJXWcKVitm6kTp62aHZ1R6R8mtDPtdKZj17ZObYqwGUGpgO6qI1Icw1RmBNlAIsaIg5g1ETSXkS0//BGBT/+4N51rDLnn/viec88379fMTnb33v1+P2c3z+bZsyf3OiIEAAAA5OqoqgcAAAAAUqLwAgAAIGsUXgAAAGSNwgsAAICsUXgBAACQNQovAAAAskbh7YHtnbYvqnqOw7E9ZfuegvfdZPumHvfp+WOBMpDXwXwsUBYyO5iPxa+j8PYgIl4TEXdXPUfd2F5u+zbbu23HsH9DQx7Ia29s/6Htu2z/1HbD9q22T616LuSPzPbG9tm2F2w/2Xr5qu2zq55rWFB4UbZ7JL1d0o+rHgTAYb1M0mZJ45JWSXpa0o1VDgTgsB6T9FZJL5d0iqQvSLql0omGCIW3B60zlJe1Xt/UOvNxk+2nbT9k+1W2r7W9z/Ye229o+9h1tne17vuo7XcftPYHbT9u+zHb72qdCT2zddsxtv/J9g9t/8T2J2y/pODMH2vN8pTtbbbPP+gux9r+r9ZcD9he3faxp9m+vXWW5we239fL5y0i9kfETETcI+n5XtYAukVee87rHRFxa0Q8FRE/k/Rvks7rZS2gG2S258wuRcTuaD6FrtX8d/bMXtbKEYV3MN4k6VNqnhF5UNKdan5uV0j6W0n/0XbffZKulHSipHWS/tn2OZJk+3JJ75d0mZp/SS88aJ9/lPQqSROt21dI+kjBGe9vfdzLJX1a0q22j227/SpJt7bdPmf7aNtHSfqipB2t/S6VNG37jw61ie1v2X5bwZmAKpDXli7zeoGknQXvCwwSmW0pklnbS5J+IelfJf19wfnzFxG8dPkiabeky1qvb5J0V9ttb5L0jKSR1tsnSApJoy+y1pykDa3Xb5D0D223ndn62DPV/GntWUm/03b76yT94EXWnZJ0z2GO4UlJq9uO4d62246S9Lik8yX9gaQfHvSx10q6se1jb+rhc7hX0kVVfy15yf+FvA4kr78r6aeSzq/668lL/i9kdiCZPV7SX0l6Y9Vfz2F5WSYMwk/aXv+5pCci4vm2tyXppZKWbF8haaOaP0UeJek4SQ+17nOapIW2tfa0vT7Wuu822wfeZ0kjRQa0/QFJ72rtEWr+9HvKofaKiBds722772mtnxgPGJH09SL7AkOIvHah9eveO9QsDeQeVSCzXYqIZ21/QlLD9qsjYl8/6+WAwlsi28dIul3Sn0uaj4hf2Z5TM1RS8ye+lW0fcnrb60+oGezXRMSPutz3fEkfUvNXJTtbYXuybd9f26v1K5aVal4A/5yaP+G+sps9gbojr5LtVZK+KunvIuJTg1gTSIXM/oYDhX+Fmpd6HNG4hrdcyyUdI6kh6bnWT6JvaLv9s5LW2X617ePUdu1QRLwg6T/VvB7ptyTJ9ooXu87nICeoGaqGpGW2P6LmT5/tfs/2W2wvkzQt6ZeS7pX0TUlP2f6Q7ZfYHrH9Wtu/3/3h//9/CjhwXdNy28e67cdpYIgc0Xm1vULS1yR9PCI+0e3HAxU40jP7ettrWmucKOmjal5asavbtXJE4S1RRDwt6X1qhu5JSW9T82FDDtx+h6R/kbRV0iOSvtG66ZetPz/Uev+9tp9S88zLWQW2vlPNX0l+T9Kimhez7znoPvOS/rQ11zskvSUiftX6tdGb1LwY/wdq/hR8vaSTDrWRmw8Y/meHmeVhNX+KXtGa6+dqPuQRMFTIq94l6RWSNtp+5sBLgfmBSpBZjUr6jKT/lfR9Na9NvjwiflHgGLLn1sXNGEK2Xy3p25KOiYjnqp4HwIsjr0C9kNkjC2d4h4ztP3HzGclepuZDpHyRIALDibwC9UJmj1wU3uHzbjWvA/q+mg8a/ZfVjgPgMMgrUC9k9gjFJQ0AAADIGmd4AQAAkDUKLwAAALKW5IknTjnllBgfH0+xNLrw0EMPdb5Tn0ZGCj0JTV/OOqvIo8L0roxj2LZt2xMRMZZ8ox6Q12KWlpY636kPe/Yc/ChGg7d8+fLke5xxxhlJ1y/jGMhrWmX827R///6k65fx9/C0005LvsfJJ5+cfI/UiuY1SeEdHx/XwsJC5zsiqTK+KY6OjibfY+vWrUnXL+MYbC8m36RH5LWY+fn5pOtv2LAh6fpSOd8TZmdnk65fxjGQ17TK+BouLqb9Ep566qlJ15ekjRs3Jt9jamoq+R6pFc0rlzQAAAAgaxReAAAAZI3CCwAAgKxReAEAAJA1Ci8AAACyRuEFAABA1ii8AAAAyFqhwmv7ctsP237E9odTDwWgd+QVqBcyC6TXsfDaHpH0cUlXSDpb0jW2z049GIDukVegXsgsUI4iZ3jPlfRIRDwaEfsl3SLpqrRjAegReQXqhcwCJShSeFdIan+S972t9/0a2+ttL9heaDQag5oPQHfIK1AvHTNLXoH+FSm8PsT74jfeEbE5IiYjYnJsbKz/yQD0grwC9dIxs+QV6F+RwrtX0ultb6+U9FiacQD0ibwC9UJmgRIUKbz3S3ql7TNsL5d0taQvpB0LQI/IK1AvZBYowbJOd4iI52y/R9KdkkYk3RARO5NPBqBr5BWoFzILlKNj4ZWkiPiypC8nngXAAJBXoF7ILJAez7QGAACArFF4AQAAkDUKLwAAALJG4QUAAEDWKLwAAADIGoUXAAAAWaPwAgAAIGuFHocXaczPzyddf3FxMen6Ze2xtLSUdP3R0dGk6yO9mZmZ5Hts2rQp6frT09NJ15ek2dnZ5Hvs3r076frj4+NJ10d6Zfxd3759e9L1t2zZknR9SVq3bl3yPSYmJmq9fjc4wwsAAICsUXgBAACQNQovAAAAskbhBQAAQNYovAAAAMgahRcAAABZo/ACAAAgax0Lr+0bbO+z/e0yBgLQHzIL1Ad5BcpR5AzvrKTLE88BYHBmRWaBupgVeQWS61h4I+J/JP20hFkADACZBeqDvALl4BpeAAAAZG1ghdf2etsLthcajcaglgWQAHkF6oO8Av0bWOGNiM0RMRkRk2NjY4NaFkAC5BWoD/IK9I9LGgAAAJC1Ig9L9hlJ35B0lu29tv8i/VgAekVmgfogr0A5lnW6Q0RcU8YgAAaDzAL1QV6BcnBJAwAAALJG4QUAAEDWKLwAAADIGoUXAAAAWaPwAgAAIGsUXgAAAGSNwgsAAICsdXwcXqSzYcOGqkfo24UXXph8j/Hx8eR7oN5GR0eT77F9+/ak6y8tLSVdX5Lm5uaS7zExMZF8D9Tb9PR08j3m5+eTrr9ly5ak65flSPr3lTO8AAAAyBqFFwAAAFmj8AIAACBrFF4AAABkjcILAACArFF4AQAAkDUKLwAAALJG4QUAAEDWOhZe26fb3mp7l+2dtuv/bAlApsgrUC9kFihHkWdae07SByLiAdsnSNpm+66I+E7i2QB0j7wC9UJmgRJ0PMMbEY9HxAOt15+WtEvSitSDAegeeQXqhcwC5ejqGl7b45LWSLrvELett71ge6HRaAxmOgA9I69AvbxYZskr0L/Chdf2SyXdLmk6Ip46+PaI2BwRkxExOTY2NsgZAXSJvAL1crjMklegf4UKr+2j1QzizRHxubQjAegHeQXqhcwC6RV5lAZL+qSkXRHx0fQjAegVeQXqhcwC5Shyhvc8Se+QdInt7a2XP048F4DekFegXsgsUIKOD0sWEfdIcgmzAOgTeQXqhcwC5eCZ1gAAAJA1Ci8AAACyRuEFAABA1ii8AAAAyBqFFwAAAFmj8AIAACBrFF4AAABkrePj8A6rpaWlpOtPT08nXV+SFhcXk+8BHAmmpqaS75H6e87atWuTri9JMzMzyfcYHR1NvgfQyerVq6seoW8bN25MvseRlFfO8AIAACBrFF4AAABkjcILAACArFF4AQAAkDUKLwAAALJG4QUAAEDWKLwAAADIWsfCa/tY29+0vcP2TtvXlTEYgO6RV6BeyCxQjiJPPPFLSZdExDO2j5Z0j+07IuLexLMB6B55BeqFzAIl6Fh4IyIkPdN68+jWS6QcCkBvyCtQL2QWKEeha3htj9jeLmmfpLsi4r60YwHoFXkF6oXMAukVKrwR8XxETEhaKelc2689+D6219tesL3QaDQGPSeAgsgrUC+dMktegf519SgNEbEk6W5Jlx/its0RMRkRk2NjYwMaD0CvyCtQLy+WWfIK9K/IozSM2R5tvf4SSZdJ+m7qwQB0j7wC9UJmgXIUeZSGUyVtsT2iZkH+bER8Ke1YAHpEXoF6IbNACYo8SsO3JK0pYRYAfSKvQL2QWaAcPNMaAAAAskbhBQAAQNYovAAAAMgahRcAAABZo/ACAAAgaxReAAAAZI3CCwAAgKwVeeKJobR79+5ary9Jq1atSrr+4uJi0vUlaWJiIvkeQCdLS0vJ95iamkq6/kUXXZR0/bL2AIbB+Ph40vUvvPDCpOtL0tzcXPI9pqenk64/OjqadP1ucIYXAAAAWaPwAgAAIGsUXgAAAGSNwgsAAICsUXgBAACQNQovAAAAskbhBQAAQNYovAAAAMha4cJre8T2g7a/lHIgAP0jr0B9kFcgvW7O8G6QtCvVIAAGirwC9UFegcQKFV7bKyW9UdL1accB0C/yCtQHeQXKUfQM74ykD0p6IeEsAAaDvAL1QV6BEnQsvLavlLQvIrZ1uN962wu2FxqNxsAGBFAceQXqg7wC5Slyhvc8SW+2vVvSLZIusX3TwXeKiM0RMRkRk2NjYwMeE0BB5BWoD/IKlKRj4Y2IayNiZUSMS7pa0tci4u3JJwPQNfIK1Ad5BcrD4/ACAAAga8u6uXNE3C3p7iSTABgo8grUB3kF0uIMLwAAALJG4QUAAEDWKLwAAADIGoUXAAAAWaPwAgAAIGsUXgAAAGSNwgsAAICsdfU4vMNkYmIi6fp333130vUlaX5+Pun6a9euTbq+JM3OzibfY2ZmJvkeSGvTpk1J17/uuuuSri9Jq1evTrr+3Nxc0vUBDE7q72mSdPHFFyffI/W/4dPT00nX7wZneAEAAJA1Ci8AAACyRuEFAABA1ii8AAAAyBqFFwAAAFmj8AIAACBrFF4AAABkjcILAACArBV64gnbuyU9Lel5Sc9FxGTKoQD0jrwC9UJmgfS6eaa1iyPiiWSTABgk8grUC5kFEuKSBgAAAGStaOENSf9te5vt9Ye6g+31thdsLzQajcFNCKBb5BWol8NmlrwC/StaeM+LiHMkXSHpr21fcPAdImJzRExGxOTY2NhAhwTQFfIK1MthM0tegf4VKrwR8Vjrz32SPi/p3JRDAegdeQXqhcwC6XUsvLaPt33CgdclvUHSt1MPBqB75BWoFzILlKPIozT8tqTP2z5w/09HxFeSTgWgV+QVqBcyC5SgY+GNiEclrS5hFgB9Iq9AvZBZoBw8LBkAAACyRuEFAABA1ii8AAAAyBqFFwAAAFmj8AIAACBrFF4AAABkjcILAACArBV54gkkctJJJ1U9Qt9GR0erHgE1sGbNmqTrr1q1Kun6krRjx46k669duzbp+pI0MzOTfI/x8fHke6De5ufnk++xdevWpOvPzc0lXR+DxxleAAAAZI3CCwAAgKxReAEAAJA1Ci8AAACyRuEFAABA1ii8AAAAyBqFFwAAAFkrVHhtj9q+zfZ3be+y/brUgwHoDXkF6oXMAukVfeKJj0n6SkS81fZyScclnAlAf8grUC9kFkisY+G1faKkCyRNSVJE7Je0P+1YAHpBXoF6IbNAOYpc0vAKSQ1JN9p+0Pb1to9PPBeA3pBXoF7ILFCCIoV3maRzJP17RKyR9KykDx98J9vrbS/YXmg0GgMeE0BB5BWol46ZJa9A/4oU3r2S9kbEfa23b1MznL8mIjZHxGRETI6NjQ1yRgDFkVegXjpmlrwC/etYeCPix5L22D6r9a5LJX0n6VQAekJegXohs0A5ij5Kw3sl3dz636OPSlqXbiQAfSKvQL2QWSCxQoU3IrZLmkw8C4ABIK9AvZBZID2eaQ0AAABZo/ACAAAgaxReAAAAZI3CCwAAgKxReAEAAJA1Ci8AAACyRuEFAABA1oo+8QQSmJiYSLr+6tWrk64vSTt27Ei+x9LSUtL1R0dHk64P6aqrrqr1+pI0Oztb6/Ulae3atcn3SH0cqb9vIr2NGzcm36OMf5tSe+c735l8j6mpqeR7DAvO8AIAACBrFF4AAABkjcILAACArFF4AQAAkDUKLwAAALJG4QUAAEDWKLwAAADIGoUXAAAAWetYeG2fZXt728tTtqfLGA5Ad8grUC9kFihHx2dai4iHJU1Iku0RST+S9PnEcwHoAXkF6oXMAuXo9pKGSyV9PyIWUwwDYKDIK1AvZBZIpNvCe7WkzxzqBtvrbS/YXmg0Gv1PBqBf5BWol0NmlrwC/StceG0vl/RmSbce6vaI2BwRkxExOTY2Nqj5APSAvAL1crjMklegf92c4b1C0gMR8ZNUwwAYGPIK1AuZBRLqpvBeoxf59SiAoUNegXohs0BChQqv7eMkvV7S59KOA6Bf5BWoFzILpNfxYckkKSJ+JunkxLMAGADyCtQLmQXS45nWAAAAkDUKLwAAALJG4QUAAEDWKLwAAADIGoUXAAAAWaPwAgAAIGsUXgAAAGTNETH4Re2GpMUuPuQUSU8MfJBycQzDYxiPY1VEjFU9xKGQ11rL4TiG8Rhyyqs0nJ/jbnEMw2EYj6FQXpMU3m7ZXoiIyarn6AfHMDxyOY5hlcPnN4djkPI4jhyOYdjl8DnmGIZDnY+BSxoAAACQNQovAAAAsjYshXdz1QMMAMcwPHI5jmGVw+c3h2OQ8jiOHI5h2OXwOeYYhkNtj2EoruEFAAAAUhmWM7wAAABAEpUWXtuX237Y9iO2P1zlLL2yfbrtrbZ32d5pe0PVM/XK9ojtB21/qepZemF71PZttr/b+nq8ruqZclP3zJLX4UFe0yOvw6PueZXqn9nKLmmwPSLpe5JeL2mvpPslXRMR36lkoB7ZPlXSqRHxgO0TJG2TtLZuxyFJtt8vaVLSiRFxZdXzdMv2Fklfj4jrbS+XdFxELFU9Vy5yyCx5HR7kNS3yOlzqnlep/pmt8gzvuZIeiYhHI2K/pFskXVXhPD2JiMcj4oHW609L2iVpRbVTdc/2SklvlHR91bP0wvaJki6Q9ElJioj9dQpiTdQ+s+R1OJDXUpDXIVH3vEp5ZLbKwrtC0p62t/eqhn+R29kel7RG0n3VTtKTGUkflPRC1YP06BWSGpJubP3a6Hrbx1c9VGayyix5rRR5TY+8Do+651XKILNVFl4f4n21fcgI2y+VdLuk6Yh4qup5umH7Skn7ImJb1bP0YZmkcyT9e0SskfSspNpdszbkssksea0ceU2PvA6BTPIqZZDZKgvvXkmnt729UtJjFc3SF9tHqxnGmyPic1XP04PzJL3Z9m41f+11ie2bqh2pa3sl7Y2IAz/936ZmODE4WWSWvA4F8poeeR0OOeRVyiCzVRbe+yW90vYZrYufr5b0hQrn6Yltq3lNy66I+GjV8/QiIq6NiJURMa7m1+FrEfH2isfqSkT8WNIe22e13nWppNr9x4YhV/vMktfhQF5LQV6HQA55lfLI7LKqNo6I52y/R9KdkkYk3RARO6uapw/nSXqHpIdsb2+9728i4ssVznSkeq+km1vf3B+VtK7iebKSSWbJ6/AgrwmRVyRQ68zyTGsAAADIGs+0BgAAgKxReAEAAJA1Ci8AAACyRuEFAABA1ii8AAAAyBqFFwAAAFmj8AIAACBrFF4AAABk7f8ATUtO5XrpaxYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22fe6b370f0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = np.array([11, 50, 62])\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i in np.arange(sample_index.shape[0]):\n",
    "    plt.subplot(131 + i)\n",
    "    plt.imshow(digits.images[sample_index[i]], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "    plt.title(\"image label: %d\" % digits.target[sample_index[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115008"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.keys()\n",
    "digits.data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.,  0.,  0.,  0., 11., 16.,\n",
       "        9.,  0.,  0.,  0.,  0.,  3., 15., 16.,  6.,  0.,  0.,  0.,  7.,\n",
       "       15., 16., 16.,  2.,  0.,  0.,  0.,  0.,  1., 16., 16.,  3.,  0.,\n",
       "        0.,  0.,  0.,  1., 16., 16.,  6.,  0.,  0.,  0.,  0.,  1., 16.,\n",
       "       16.,  6.,  0.,  0.,  0.,  0.,  0., 11., 16., 10.,  0.,  0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO, output X_train with preprocessing and Y_train as a one-hot enconding vectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "x_train,x_test,y_train,y_test=train_test_split(digits.data,digits.target)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "X_train=scaler.transform(x_train)\n",
    "X_test=scaler.transform(x_test)\n",
    "Y_train=to_categorical(y_train)\n",
    "Y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1347 samples, validate on 450 samples\n",
      "Epoch 1/100\n",
      "1347/1347 [==============================] - 1s 656us/step - loss: 2.1602 - acc: 0.2895 - val_loss: 1.7362 - val_acc: 0.4556\n",
      "Epoch 2/100\n",
      "1347/1347 [==============================] - 0s 80us/step - loss: 1.4648 - acc: 0.5486 - val_loss: 1.2660 - val_acc: 0.6422\n",
      "Epoch 3/100\n",
      "1347/1347 [==============================] - 0s 80us/step - loss: 1.0925 - acc: 0.7001 - val_loss: 0.9963 - val_acc: 0.7356\n",
      "Epoch 4/100\n",
      "1347/1347 [==============================] - 0s 75us/step - loss: 0.8681 - acc: 0.7817 - val_loss: 0.8252 - val_acc: 0.8022\n",
      "Epoch 5/100\n",
      "1347/1347 [==============================] - 0s 80us/step - loss: 0.7191 - acc: 0.8344 - val_loss: 0.7064 - val_acc: 0.8333\n",
      "Epoch 6/100\n",
      "1347/1347 [==============================] - 0s 75us/step - loss: 0.6109 - acc: 0.8671 - val_loss: 0.6202 - val_acc: 0.8489\n",
      "Epoch 7/100\n",
      "1347/1347 [==============================] - 0s 76us/step - loss: 0.5316 - acc: 0.8931 - val_loss: 0.5560 - val_acc: 0.8622\n",
      "Epoch 8/100\n",
      "1347/1347 [==============================] - 0s 78us/step - loss: 0.4693 - acc: 0.9050 - val_loss: 0.5032 - val_acc: 0.8867\n",
      "Epoch 9/100\n",
      "1347/1347 [==============================] - 0s 79us/step - loss: 0.4203 - acc: 0.9131 - val_loss: 0.4622 - val_acc: 0.8933\n",
      "Epoch 10/100\n",
      "1347/1347 [==============================] - 0s 88us/step - loss: 0.3803 - acc: 0.9220 - val_loss: 0.4286 - val_acc: 0.9067\n",
      "Epoch 11/100\n",
      "1347/1347 [==============================] - 0s 89us/step - loss: 0.3472 - acc: 0.9280 - val_loss: 0.3986 - val_acc: 0.9111\n",
      "Epoch 12/100\n",
      "1347/1347 [==============================] - 0s 84us/step - loss: 0.3189 - acc: 0.9310 - val_loss: 0.3737 - val_acc: 0.9133\n",
      "Epoch 13/100\n",
      "1347/1347 [==============================] - 0s 79us/step - loss: 0.2954 - acc: 0.9384 - val_loss: 0.3530 - val_acc: 0.9178\n",
      "Epoch 14/100\n",
      "1347/1347 [==============================] - 0s 79us/step - loss: 0.2748 - acc: 0.9399 - val_loss: 0.3343 - val_acc: 0.9200\n",
      "Epoch 15/100\n",
      "1347/1347 [==============================] - 0s 96us/step - loss: 0.2568 - acc: 0.9451 - val_loss: 0.3203 - val_acc: 0.9222\n",
      "Epoch 16/100\n",
      "1347/1347 [==============================] - 0s 91us/step - loss: 0.2411 - acc: 0.9495 - val_loss: 0.3059 - val_acc: 0.9244\n",
      "Epoch 17/100\n",
      "1347/1347 [==============================] - 0s 86us/step - loss: 0.2276 - acc: 0.9547 - val_loss: 0.2936 - val_acc: 0.9311\n",
      "Epoch 18/100\n",
      "1347/1347 [==============================] - 0s 82us/step - loss: 0.2153 - acc: 0.9547 - val_loss: 0.2828 - val_acc: 0.9311\n",
      "Epoch 19/100\n",
      "1347/1347 [==============================] - 0s 84us/step - loss: 0.2043 - acc: 0.9592 - val_loss: 0.2728 - val_acc: 0.9333\n",
      "Epoch 20/100\n",
      "1347/1347 [==============================] - 0s 85us/step - loss: 0.1944 - acc: 0.9607 - val_loss: 0.2637 - val_acc: 0.9333\n",
      "Epoch 21/100\n",
      "1347/1347 [==============================] - 0s 91us/step - loss: 0.1855 - acc: 0.9607 - val_loss: 0.2551 - val_acc: 0.9333\n",
      "Epoch 22/100\n",
      "1347/1347 [==============================] - 0s 87us/step - loss: 0.1772 - acc: 0.9614 - val_loss: 0.2470 - val_acc: 0.9356\n",
      "Epoch 23/100\n",
      "1347/1347 [==============================] - 0s 87us/step - loss: 0.1698 - acc: 0.9621 - val_loss: 0.2404 - val_acc: 0.9378\n",
      "Epoch 24/100\n",
      "1347/1347 [==============================] - 0s 79us/step - loss: 0.1626 - acc: 0.9636 - val_loss: 0.2346 - val_acc: 0.9378\n",
      "Epoch 25/100\n",
      "1347/1347 [==============================] - 0s 82us/step - loss: 0.1562 - acc: 0.9644 - val_loss: 0.2293 - val_acc: 0.9378\n",
      "Epoch 26/100\n",
      "1347/1347 [==============================] - 0s 101us/step - loss: 0.1502 - acc: 0.9644 - val_loss: 0.2242 - val_acc: 0.9422\n",
      "Epoch 27/100\n",
      "1347/1347 [==============================] - 0s 107us/step - loss: 0.1447 - acc: 0.9659 - val_loss: 0.2196 - val_acc: 0.9422\n",
      "Epoch 28/100\n",
      "1347/1347 [==============================] - 0s 81us/step - loss: 0.1397 - acc: 0.9688 - val_loss: 0.2147 - val_acc: 0.9422\n",
      "Epoch 29/100\n",
      "1347/1347 [==============================] - 0s 81us/step - loss: 0.1349 - acc: 0.9696 - val_loss: 0.2102 - val_acc: 0.9422\n",
      "Epoch 30/100\n",
      "1347/1347 [==============================] - 0s 84us/step - loss: 0.1304 - acc: 0.9696 - val_loss: 0.2058 - val_acc: 0.9422\n",
      "Epoch 31/100\n",
      "1347/1347 [==============================] - 0s 86us/step - loss: 0.1263 - acc: 0.9710 - val_loss: 0.2021 - val_acc: 0.9444\n",
      "Epoch 32/100\n",
      "1347/1347 [==============================] - 0s 86us/step - loss: 0.1223 - acc: 0.9703 - val_loss: 0.1986 - val_acc: 0.9467\n",
      "Epoch 33/100\n",
      "1347/1347 [==============================] - 0s 84us/step - loss: 0.1187 - acc: 0.9718 - val_loss: 0.1954 - val_acc: 0.9467\n",
      "Epoch 34/100\n",
      "1347/1347 [==============================] - 0s 110us/step - loss: 0.1152 - acc: 0.9710 - val_loss: 0.1921 - val_acc: 0.9489\n",
      "Epoch 35/100\n",
      "1347/1347 [==============================] - 0s 90us/step - loss: 0.1119 - acc: 0.9740 - val_loss: 0.1893 - val_acc: 0.9467\n",
      "Epoch 36/100\n",
      "1347/1347 [==============================] - 0s 80us/step - loss: 0.1088 - acc: 0.9755 - val_loss: 0.1867 - val_acc: 0.9467\n",
      "Epoch 37/100\n",
      "1347/1347 [==============================] - 0s 78us/step - loss: 0.1059 - acc: 0.9770 - val_loss: 0.1841 - val_acc: 0.9444\n",
      "Epoch 38/100\n",
      "1347/1347 [==============================] - 0s 78us/step - loss: 0.1031 - acc: 0.9777 - val_loss: 0.1817 - val_acc: 0.9467\n",
      "Epoch 39/100\n",
      "1347/1347 [==============================] - 0s 80us/step - loss: 0.1004 - acc: 0.9785 - val_loss: 0.1792 - val_acc: 0.9467\n",
      "Epoch 40/100\n",
      "1347/1347 [==============================] - 0s 81us/step - loss: 0.0978 - acc: 0.9792 - val_loss: 0.1770 - val_acc: 0.9467\n",
      "Epoch 41/100\n",
      "1347/1347 [==============================] - 0s 77us/step - loss: 0.0954 - acc: 0.9807 - val_loss: 0.1749 - val_acc: 0.9467\n",
      "Epoch 42/100\n",
      "1347/1347 [==============================] - 0s 80us/step - loss: 0.0930 - acc: 0.9814 - val_loss: 0.1732 - val_acc: 0.9467\n",
      "Epoch 43/100\n",
      "1347/1347 [==============================] - 0s 86us/step - loss: 0.0908 - acc: 0.9814 - val_loss: 0.1713 - val_acc: 0.9467\n",
      "Epoch 44/100\n",
      "1347/1347 [==============================] - 0s 108us/step - loss: 0.0887 - acc: 0.9822 - val_loss: 0.1690 - val_acc: 0.9489\n",
      "Epoch 45/100\n",
      "1347/1347 [==============================] - 0s 126us/step - loss: 0.0867 - acc: 0.9822 - val_loss: 0.1673 - val_acc: 0.9489\n",
      "Epoch 46/100\n",
      "1347/1347 [==============================] - 0s 82us/step - loss: 0.0847 - acc: 0.9822 - val_loss: 0.1655 - val_acc: 0.9511\n",
      "Epoch 47/100\n",
      "1347/1347 [==============================] - 0s 85us/step - loss: 0.0828 - acc: 0.9844 - val_loss: 0.1638 - val_acc: 0.9511\n",
      "Epoch 48/100\n",
      "1347/1347 [==============================] - 0s 103us/step - loss: 0.0811 - acc: 0.9844 - val_loss: 0.1622 - val_acc: 0.9511\n",
      "Epoch 49/100\n",
      "1347/1347 [==============================] - 0s 95us/step - loss: 0.0794 - acc: 0.9837 - val_loss: 0.1606 - val_acc: 0.9511\n",
      "Epoch 50/100\n",
      "1347/1347 [==============================] - 0s 108us/step - loss: 0.0777 - acc: 0.9844 - val_loss: 0.1593 - val_acc: 0.9511\n",
      "Epoch 51/100\n",
      "1347/1347 [==============================] - 0s 120us/step - loss: 0.0761 - acc: 0.9852 - val_loss: 0.1579 - val_acc: 0.9511\n",
      "Epoch 52/100\n",
      "1347/1347 [==============================] - 0s 96us/step - loss: 0.0745 - acc: 0.9852 - val_loss: 0.1577 - val_acc: 0.9511\n",
      "Epoch 53/100\n",
      "1347/1347 [==============================] - 0s 99us/step - loss: 0.0731 - acc: 0.9866 - val_loss: 0.1564 - val_acc: 0.9511\n",
      "Epoch 54/100\n",
      "1347/1347 [==============================] - 0s 98us/step - loss: 0.0716 - acc: 0.9866 - val_loss: 0.1552 - val_acc: 0.9533\n",
      "Epoch 55/100\n",
      "1347/1347 [==============================] - 0s 86us/step - loss: 0.0702 - acc: 0.9866 - val_loss: 0.1538 - val_acc: 0.9533\n",
      "Epoch 56/100\n",
      "1347/1347 [==============================] - 0s 88us/step - loss: 0.0689 - acc: 0.9889 - val_loss: 0.1525 - val_acc: 0.9533\n",
      "Epoch 57/100\n",
      "1347/1347 [==============================] - 0s 85us/step - loss: 0.0676 - acc: 0.9896 - val_loss: 0.1514 - val_acc: 0.9533\n",
      "Epoch 58/100\n",
      "1347/1347 [==============================] - 0s 82us/step - loss: 0.0663 - acc: 0.9896 - val_loss: 0.1507 - val_acc: 0.9533\n",
      "Epoch 59/100\n",
      "1347/1347 [==============================] - 0s 78us/step - loss: 0.0651 - acc: 0.9896 - val_loss: 0.1496 - val_acc: 0.9533\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347/1347 [==============================] - 0s 75us/step - loss: 0.0638 - acc: 0.9903 - val_loss: 0.1487 - val_acc: 0.9533\n",
      "Epoch 61/100\n",
      "1347/1347 [==============================] - 0s 79us/step - loss: 0.0626 - acc: 0.9911 - val_loss: 0.1476 - val_acc: 0.9533\n",
      "Epoch 62/100\n",
      "1347/1347 [==============================] - 0s 94us/step - loss: 0.0614 - acc: 0.9911 - val_loss: 0.1468 - val_acc: 0.9533\n",
      "Epoch 63/100\n",
      "1347/1347 [==============================] - 0s 98us/step - loss: 0.0604 - acc: 0.9926 - val_loss: 0.1459 - val_acc: 0.9533\n",
      "Epoch 64/100\n",
      "1347/1347 [==============================] - 0s 94us/step - loss: 0.0592 - acc: 0.9926 - val_loss: 0.1450 - val_acc: 0.9533\n",
      "Epoch 65/100\n",
      "1347/1347 [==============================] - 0s 94us/step - loss: 0.0582 - acc: 0.9926 - val_loss: 0.1439 - val_acc: 0.9533\n",
      "Epoch 66/100\n",
      "1347/1347 [==============================] - 0s 86us/step - loss: 0.0572 - acc: 0.9926 - val_loss: 0.1429 - val_acc: 0.9533\n",
      "Epoch 67/100\n",
      "1347/1347 [==============================] - 0s 79us/step - loss: 0.0563 - acc: 0.9933 - val_loss: 0.1422 - val_acc: 0.9533\n",
      "Epoch 68/100\n",
      "1347/1347 [==============================] - 0s 79us/step - loss: 0.0553 - acc: 0.9933 - val_loss: 0.1416 - val_acc: 0.9533\n",
      "Epoch 69/100\n",
      "1347/1347 [==============================] - 0s 73us/step - loss: 0.0544 - acc: 0.9933 - val_loss: 0.1408 - val_acc: 0.9533\n",
      "Epoch 70/100\n",
      "1347/1347 [==============================] - 0s 70us/step - loss: 0.0534 - acc: 0.9933 - val_loss: 0.1400 - val_acc: 0.9533\n",
      "Epoch 71/100\n",
      "1347/1347 [==============================] - 0s 70us/step - loss: 0.0526 - acc: 0.9933 - val_loss: 0.1393 - val_acc: 0.9533\n",
      "Epoch 72/100\n",
      "1347/1347 [==============================] - 0s 75us/step - loss: 0.0517 - acc: 0.9933 - val_loss: 0.1387 - val_acc: 0.9533\n",
      "Epoch 73/100\n",
      "1347/1347 [==============================] - 0s 75us/step - loss: 0.0509 - acc: 0.9933 - val_loss: 0.1381 - val_acc: 0.9511\n",
      "Epoch 74/100\n",
      "1347/1347 [==============================] - 0s 77us/step - loss: 0.0500 - acc: 0.9933 - val_loss: 0.1378 - val_acc: 0.9511\n",
      "Epoch 75/100\n",
      "1347/1347 [==============================] - 0s 73us/step - loss: 0.0492 - acc: 0.9933 - val_loss: 0.1371 - val_acc: 0.9511\n",
      "Epoch 76/100\n",
      "1347/1347 [==============================] - 0s 79us/step - loss: 0.0484 - acc: 0.9941 - val_loss: 0.1346 - val_acc: 0.9533\n",
      "Epoch 77/100\n",
      "1347/1347 [==============================] - 0s 76us/step - loss: 0.0479 - acc: 0.9948 - val_loss: 0.1348 - val_acc: 0.9511\n",
      "Epoch 78/100\n",
      "1347/1347 [==============================] - 0s 74us/step - loss: 0.0469 - acc: 0.9955 - val_loss: 0.1342 - val_acc: 0.9511\n",
      "Epoch 79/100\n",
      "1347/1347 [==============================] - 0s 72us/step - loss: 0.0461 - acc: 0.9948 - val_loss: 0.1338 - val_acc: 0.9511\n",
      "Epoch 80/100\n",
      "1347/1347 [==============================] - 0s 76us/step - loss: 0.0454 - acc: 0.9955 - val_loss: 0.1334 - val_acc: 0.9511\n",
      "Epoch 81/100\n",
      "1347/1347 [==============================] - 0s 73us/step - loss: 0.0448 - acc: 0.9955 - val_loss: 0.1333 - val_acc: 0.9511\n",
      "Epoch 82/100\n",
      "1347/1347 [==============================] - 0s 76us/step - loss: 0.0440 - acc: 0.9955 - val_loss: 0.1324 - val_acc: 0.9511\n",
      "Epoch 83/100\n",
      "1347/1347 [==============================] - 0s 79us/step - loss: 0.0434 - acc: 0.9955 - val_loss: 0.1319 - val_acc: 0.9511\n",
      "Epoch 84/100\n",
      "1347/1347 [==============================] - 0s 80us/step - loss: 0.0427 - acc: 0.9955 - val_loss: 0.1315 - val_acc: 0.9511\n",
      "Epoch 85/100\n",
      "1347/1347 [==============================] - 0s 72us/step - loss: 0.0421 - acc: 0.9955 - val_loss: 0.1311 - val_acc: 0.9511\n",
      "Epoch 86/100\n",
      "1347/1347 [==============================] - 0s 81us/step - loss: 0.0415 - acc: 0.9955 - val_loss: 0.1306 - val_acc: 0.9511\n",
      "Epoch 87/100\n",
      "1347/1347 [==============================] - 0s 75us/step - loss: 0.0409 - acc: 0.9963 - val_loss: 0.1302 - val_acc: 0.9511\n",
      "Epoch 88/100\n",
      "1347/1347 [==============================] - 0s 86us/step - loss: 0.0403 - acc: 0.9963 - val_loss: 0.1298 - val_acc: 0.9511\n",
      "Epoch 89/100\n",
      "1347/1347 [==============================] - 0s 77us/step - loss: 0.0397 - acc: 0.9970 - val_loss: 0.1294 - val_acc: 0.9511\n",
      "Epoch 90/100\n",
      "1347/1347 [==============================] - 0s 83us/step - loss: 0.0392 - acc: 0.9970 - val_loss: 0.1289 - val_acc: 0.9511\n",
      "Epoch 91/100\n",
      "1347/1347 [==============================] - 0s 100us/step - loss: 0.0386 - acc: 0.9970 - val_loss: 0.1289 - val_acc: 0.9511\n",
      "Epoch 92/100\n",
      "1347/1347 [==============================] - 0s 88us/step - loss: 0.0381 - acc: 0.9970 - val_loss: 0.1281 - val_acc: 0.9511\n",
      "Epoch 93/100\n",
      "1347/1347 [==============================] - 0s 81us/step - loss: 0.0375 - acc: 0.9970 - val_loss: 0.1276 - val_acc: 0.9511\n",
      "Epoch 94/100\n",
      "1347/1347 [==============================] - 0s 84us/step - loss: 0.0370 - acc: 0.9970 - val_loss: 0.1274 - val_acc: 0.9511\n",
      "Epoch 95/100\n",
      "1347/1347 [==============================] - 0s 73us/step - loss: 0.0365 - acc: 0.9970 - val_loss: 0.1270 - val_acc: 0.9511\n",
      "Epoch 96/100\n",
      "1347/1347 [==============================] - 0s 72us/step - loss: 0.0361 - acc: 0.9970 - val_loss: 0.1266 - val_acc: 0.9511\n",
      "Epoch 97/100\n",
      "1347/1347 [==============================] - 0s 95us/step - loss: 0.0355 - acc: 0.9970 - val_loss: 0.1261 - val_acc: 0.9511\n",
      "Epoch 98/100\n",
      "1347/1347 [==============================] - 0s 79us/step - loss: 0.0350 - acc: 0.9970 - val_loss: 0.1268 - val_acc: 0.9533\n",
      "Epoch 99/100\n",
      "1347/1347 [==============================] - 0s 88us/step - loss: 0.0347 - acc: 0.9978 - val_loss: 0.1262 - val_acc: 0.9511\n",
      "Epoch 100/100\n",
      "1347/1347 [==============================] - 0s 105us/step - loss: 0.0342 - acc: 0.9978 - val_loss: 0.1257 - val_acc: 0.9511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22fec6a9da0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras import optimizers\n",
    "\n",
    "n_i = X_train.shape[1]\n",
    "n_h = 50\n",
    "n_o = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_h, input_dim=n_i))\n",
    "model.add(Activation(\"tanh\"))\n",
    "\n",
    "model.add(Dense(30))\n",
    "\n",
    "model.add(Dense(n_o))\n",
    "\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizers.SGD(lr=0.01),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, validation_data=(X_test,Y_test),epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 50)                3250      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 10)                310       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 5,090\n",
      "Trainable params: 5,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Training a neural network with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by implementing the softmax function, sigmoid and its derivative, as well as negative log-likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):   \n",
    "    return np.exp(x) / np.sum(np.exp(x),axis=0)\n",
    "\n",
    "def sigmoid(X):\n",
    "    res=1/(1+exp(-X))\n",
    "    return res\n",
    "\n",
    "def dsigmoid(X):\n",
    "    \n",
    "    return sigmoid(X)*sigmoid(1-X)\n",
    "\n",
    "def nll(Y_true, Y_pred):\n",
    "    \n",
    "    return -np.sum(Y_true*math.log(Y_pred),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can complete the following python class to get your numpy network. This class is designed to be \"scikit like\", meaning that once it has been correctly completed, you can just execute \n",
    "\n",
    "-model = NeuralNet(n_features, n_hidden, n_classes); model.fit(X_train,Y_train,lr=0.1, n_epochs=20)\n",
    "\n",
    "to get it to work. Hints about how to fill the methods can be found in the pdf subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer and a sigmoid activation\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # TODO\n",
    "        self.W_h = None\n",
    "        self.b_h = None\n",
    "        self.W_o = None\n",
    "        self.b_o = None\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # TODO\n",
    "        return y\n",
    "    \n",
    "    def forward_with_hidden(self, X):\n",
    "        # TODO\n",
    "        return y, h, z_h\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        # TODO\n",
    "        return L\n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        # TODO\n",
    "        return grads\n",
    "\n",
    "    def train_sample(self, x, y, lr):\n",
    "        # TODO\n",
    "\n",
    "    def fit(self, X_train, Y_train , lr, n_epochs):\n",
    "        # TODO\n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO\n",
    "        return \n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
