{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob: https://github.com/sloria/TextBlob\n",
    "Polyglot: https://pypi.org/project/polyglot/\n",
    "Pattern: https://github.com/clips/pattern\n",
    "specturl cluster:https://github.com/PP8818/Python-Projects/tree/master/py2/data-mining/clustering/spectral_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5a7c334d-3afa-4508-874f-3366da0a9c2c",
    "_uuid": "1139f2793448fc6a647e9a0c8e8679963d9f206d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "init_notebook_mode(connected=True) #do not miss this line\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7cdec987-42d7-495d-8aed-d904f430707a",
    "_uuid": "2ab794b9bdd2400c1eb349493361f325fef51eee",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = '../input/data_elonmusk.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "52abb44e-881a-4fa5-aaad-cca178c148b4",
    "_uuid": "3247c5f08df034b7afe4be36e52a99baa625a5ba",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets = pd.read_csv(datafile, encoding='latin1')\n",
    "tweets = tweets.assign(Time=pd.to_datetime(tweets.Time)).drop('row ID', axis='columns')\n",
    "\n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fdadead9-8dea-4c34-8bf2-1c4652418f36",
    "_uuid": "a9bbf206e9d8b2b17c15da0df8c67f1365f03dab",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "range(len(tweets['Tweet']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "556c0eec-994f-4f72-82af-9da59e590a1b",
    "_uuid": "940eb7b9c8715792165ad73a40400e8c06c7b775"
   },
   "source": [
    "***Tweet Activity Over Years***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f11bc329-639a-4498-bfc2-389318602350",
    "_uuid": "1f37860fb232b0cca3b9dbd7924cc51e20556178",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "'''\n",
    "tweets['Time'] = pd.to_datetime(tweets['Time'], format='%y-%m-%d %H:%M:%S')\n",
    "tweetsT = tweets['Time']\n",
    "\n",
    "trace = go.Histogram(\n",
    "    x=tweetsT,\n",
    "    marker=dict(\n",
    "        color='blue'\n",
    "    ),\n",
    "    opacity=0.75\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Tweet Activity Over Years',\n",
    "    height=450,\n",
    "    width=1200,\n",
    "    xaxis=dict(\n",
    "        title='Month and year'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Tweet Quantity'\n",
    "    ),\n",
    "    bargap=0.2,\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b06b4ca0-f1ab-45a1-9550-1492a30c3c74",
    "_uuid": "ffb58f64bee55b852c07d0bc87be430d1258d976",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "a=[]\n",
    "for i in range(len(tweets['Tweet'])):\n",
    "        a=tweets['Tweet'][i]\n",
    "        corpus.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e943c950-1007-46ff-a868-412100b6830f",
    "_uuid": "eeb90a8ffa5442ee9e32a456b227df1f61be706f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d117ab5e-3395-4430-8dad-0750a520c313",
    "_uuid": "e6f1dd9faefafd8045c94fc71a033c201c5443ff",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import tempfile\n",
    "\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n",
    "\n",
    "from gensim import corpora\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "068cfe59-dbc2-4481-a20f-6cd321cbc4bb",
    "_uuid": "df0a72a7a17efcf0da96a51753855ec2039cffb8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# remove common words and tokenize\n",
    "list1 = ['RT','rt']\n",
    "stoplist = stopwords.words('english') + list(punctuation) + list1\n",
    "\n",
    "texts = [[word for word in str(document).lower().split() if word not in stoplist] for document in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "41eea25d-97f6-478c-97bb-8073cecb3101",
    "_uuid": "c9824ceb1efd0895923b23ff8d058a86a43b9c54",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save(os.path.join(TEMP_FOLDER, 'elon.dict'))  # store the dictionary, for future reference\n",
    "#print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "38f1a502-a6fe-4c3d-b8bf-27c9272c3753",
    "_uuid": "3944206c06ca0af2822f81c3ef0dadf994f5e023",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b80d4a62-6126-48e5-aba8-8de557c5d00d",
    "_uuid": "040914ec58061f9f66a39df81c8c1ae4cb9dbeb5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'elon.mm'), corpus)  # store to disk, for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "30731758-2dd0-482c-8ad8-995ebf37b686",
    "_uuid": "547b528b1af3eac74eed647c6013c67304a44f42"
   },
   "source": [
    "In the previous cells, we created a corpus of documents represented as a stream of vectors. To continue, let’s fire up gensim and use that corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c654c601-bf99-4ca3-8fcd-c035dbe8af8f",
    "_uuid": "b446ef66b9fb10675b0b8c0216527a35cddb5fa4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8c2fae3e-3c57-4a1b-b302-7f9bfe2fb5a0",
    "_uuid": "516974a573c993c655cec2ea8ccda4a3e4e3d899"
   },
   "source": [
    "### Creating a transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c8cc6eba-dfde-4078-93aa-49ec66b79483",
    "_uuid": "95d9a030b86689a097ff84cf988ce6bfee42b6d6"
   },
   "source": [
    "\n",
    "The transformations are standard Python objects, typically initialized by means of a training corpus:\n",
    "\n",
    "Different transformations may require different initialization parameters; in case of TfIdf, the “training” consists simply of\n",
    "going through the supplied corpus once and computing document frequencies of all its features.\n",
    "Training other models, such as Latent Semantic Analysis or Latent Dirichlet Allocation, is much more involved and,\n",
    "consequently, takes much more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a7966606-dc6a-44a8-8fe5-d1a79929e203",
    "_uuid": "91cd194bf43c84b564c693c6e0ea109d28724abc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "47a7ad12-9917-4a74-b190-b6f039e87baf",
    "_uuid": "902cd861d68967c9d6d78f7d074fe6e4d31e4547"
   },
   "source": [
    "### Note\n",
    "Transformations always convert between two specific vector spaces. The same vector space (= the same set of feature ids) must be used for training as well as for subsequent vector transformations. Failure to use the same input feature space, such as applying a different string preprocessing, using different feature ids, or using bag-of-words input vectors where TfIdf vectors are expected, will result in feature mismatch during transformation calls and consequently in either garbage output and/or runtime exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "31035482-743c-4143-a016-af6e7aafa8b2",
    "_uuid": "12084cb081ffae38fd076b271f748faa6e3ff5af"
   },
   "source": [
    "From now on, tfidf is treated as a read-only object that can be used to apply a transformation to a whole corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8d3211bd-a12d-4518-b822-eb92df9a564d",
    "_uuid": "6ccc9986f35d02916c62139410d0cbc6455e3149",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus]  # step 2 -- use the model to transform vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6096b0ef-bb2b-40d1-aac4-f538dae0f1e3",
    "_uuid": "39f769bd5e8fea11c952a7c33bf32b329618f837"
   },
   "source": [
    "### LDA:\n",
    "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7c793905-48bb-4526-9e78-1d5d1a9ed70b",
    "_uuid": "8aca0810b66308f947046dd9689409fc56067b1a"
   },
   "source": [
    "Latent Dirichlet Allocation, LDA is yet another transformation from bag-of-words counts into a topic space of lower dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA), so LDA’s topics can be interpreted as probability distributions over words. These distributions are, just like with LSA, inferred automatically from a training corpus. Documents are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cc49ccb3-bd68-4359-8fec-7f6cbff2a182",
    "_uuid": "5771a4cc1b5f20878d52409e11f3fecaa59a9ac6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_topics = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8fb9df90-fdc2-491b-a2aa-02c0eca4c29a",
    "_uuid": "db447043098a7546237bbaf4c7d1145103209160",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\n",
    "corpus_lda = lda[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dfbb2723-ec7e-490e-8abd-fb646335bc55",
    "_uuid": "53e59fb13b5f55abb2e8158cdd0b244bcb055b3f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Show first n important word in the topics:\n",
    "lda.show_topics(total_topics,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5e31a267-03c5-4a4e-8a73-c72f7abd4644",
    "_uuid": "25d66c5df87a0c365dcd0725e779dc8724ab63af",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "data_lda = {i: OrderedDict(lda.show_topic(i,25)) for i in range(total_topics)}\n",
    "#data_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "409fed91-48dd-4355-9587-a6299b63f181",
    "_uuid": "edeb3eedc9a10cc135c7bbef50fda042f330482d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_lda = pd.DataFrame(data_lda)\n",
    "print(df_lda.shape)\n",
    "df_lda = df_lda.fillna(0).T\n",
    "print(df_lda.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6410b413-a0d4-49be-a2c4-6844cfc8cb0c",
    "_uuid": "f81dd0ed3ef908b520c2416cf626473a4dd8acd6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "19a51407-5488-44d3-bdd2-9a0ec19ecf35",
    "_uuid": "5206e6ec92bc2678c20b427efd788c0be67adcbd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "g=sns.clustermap(df_lda.corr(), center=0, cmap=\"RdBu\", metric='cosine', linewidths=.75, figsize=(12, 12))\n",
    "plt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
    "plt.show()\n",
    "#plt.setp(ax_heatmap.get_yticklabels(), rotation=0)  # For y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bc653426-6a63-4b41-887d-92d270dc1168",
    "_uuid": "415d0d14427e94f9900776f3126d5bf8c2391beb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.gensim.prepare(lda, corpus_lda, dictionary, mds='tsne')\n",
    "panel"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
