{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Lab: Nonnegative Matrix Factorization\n",
    "## 1、 Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a9ac397898>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "im=plt.imread(\".\\\\orl_faces\\\\s1\\\\1.pgm\")\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sieze of image is: (112, 92)\n"
     ]
    }
   ],
   "source": [
    "print(\"The sieze of image is:\",im.shape)\n",
    "im_h=np.ravel(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 400 images in the database, each imamge has 10304 pixels with size (112,92)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、 Presentation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](2.1.jpg)\n",
    "![title](2.2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    So we have proved that the objective function is not convex. Its gradient is (-(M-WH)H'/np),-W'(M-WH)/np) and the gradient doesn't satisfy Lipschitz continuous\n",
    "   \n",
    "## 3 Find W when H0 is fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 3.1\n",
    "#### We get one possible solution H0 and W0 through this way. And we have choosed k bases who include the biggest singular values.  We could use the random way to initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 3.2\n",
    "![title](3.2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10304)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M=[]\n",
    "for i in range(1,11):\n",
    "    im=plt.imread(\".\\\\orl_faces\\\\s1\\\\\"+str(i)+\".pgm\")\n",
    "    M.append(np.ravel(im))\n",
    "M=np.array(M,dtype=float)\n",
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "k=5\n",
    "[n,p]=M.shape\n",
    "M=np.array(M,dtype=float)\n",
    "W0, S, H0 = svds(M, k)\n",
    "W0 = np.maximum(0, W0 * np.sqrt(S))\n",
    "H0 = np.maximum(0,(H0.T * np.sqrt(S)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_w(w,h=H0):\n",
    "    [n,p]=M.shape\n",
    "    w=w.reshape(n,k)\n",
    "    m_t=np.dot(w,h)\n",
    "    g=np.linalg.norm(m_t-M,ord='fro')**2\n",
    "    g=g/(2*n*p)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_gradient(w,h=H0):\n",
    "    [n,p]=M.shape\n",
    "    w=w.reshape(n,k)\n",
    "    m_t=np.dot(w,h)\n",
    "    g=-1*np.dot((M-m_t),h.T)/(n*p)\n",
    "    return np.ravel(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of function scipy.optimize.check grad is: 3.6764329094003976e-05\n",
      "It seems accpetable\n"
     ]
    }
   ],
   "source": [
    "t=g_gradient(W0)\n",
    "t.shape\n",
    "from scipy.optimize import check_grad\n",
    "print(\"The result of function scipy.optimize.check grad is:\",check_grad(g_w,g_gradient,np.ravel(W0)))\n",
    "print(\"It seems accpetable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.4\n",
    "\n",
    "We know:\n",
    "  \n",
    "$$prox_{\\gamma lR+}=\\underset{\\gamma }{argmin} (l_{R_+}(\\gamma)+\\frac{||\\gamma-x||^2}{2})$$\n",
    "\n",
    "To get the minimize result, the $\\gamma$ must bigger than 0, otherwise the $l_{R_+}(\\gamma)=+\\infty$\n",
    "\n",
    "So we get the  $\\gamma\\in R_+$\n",
    "\n",
    "When we  $\\gamma> 0$, the problem becomes to \n",
    "\n",
    "$$\\underset{\\gamma }{argmin} ||\\gamma-x|| ,\\quad  where \\quad \\gamma> 0  $$\n",
    "\n",
    "It equal to: $$proj_{R_+}(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_method(val_g,grad_g,W0,h,gamma,N):\n",
    "    w=W0\n",
    "    for i in range(N):\n",
    "        w=w-gamma*grad_g(W0,h).reshape(n,k)\n",
    "        w=np.maximum(0,w)\n",
    "        W0=w\n",
    "    return(val_g(w,h),w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before the iterations, the value of g(W0) is : 414.39665223126826\n",
      "After N=100 times iterations with the learning rate=1,we get new value of g(w) is: 360.95642813130837\n",
      "The time it costs is : 0.06699252128601074\n"
     ]
    }
   ],
   "source": [
    "print(\"Before the iterations, the value of g(W0) is :\",g_w(W0))\n",
    "gamma=1\n",
    "N=100\n",
    "start=time.time()\n",
    "g_new,w_new=projected_gradient_method(g_w,g_gradient,W0,H0,gamma,N)\n",
    "end=time.time()\n",
    "print(\"After N=100 times iterations with the learning rate=1,we get new value of g(w) is:\",g_new)\n",
    "print(\"The time it costs is :\",end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Algorithmic re\f",
    "nement for the problem with H0 fixed\n",
    "### Question 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_method_line_search(val_g,grad_g,W0,alpha,gamma,N):\n",
    "    w=W0\n",
    "    for i in range(N):\n",
    "        w=w-(gamma*grad_g(W0)).reshape(n,k)\n",
    "        w=np.maximum(0,w)\n",
    "        w=W0+alpha*(w-W0)\n",
    "        W0=w\n",
    "    return(val_g(w),w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before the iterations, the value of g(W0) is : 414.39665223126826\n",
      "After N=100 times iterations with the learning rate=1,alpha=0.5,we get new value of g(w) is: 363.8822461774789\n",
      "The time it costs is : 0.07741212844848633\n"
     ]
    }
   ],
   "source": [
    "print(\"Before the iterations, the value of g(W0) is :\",g_w(W0))\n",
    "gamma=1\n",
    "N=100\n",
    "alpha=0.5\n",
    "start=time.time()\n",
    "g_new,w_new=projected_gradient_method_line_search(g_w,g_gradient,W0,alpha,gamma,N)\n",
    "end=time.time()\n",
    "print(\"After N=100 times iterations with the learning rate=1,alpha=0.5,we get new value of g(w) is:\",g_new)\n",
    "print(\"The time it costs is :\",end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2\n",
    "When we apply a line search,we set the $\\alpha=0.5$ and the speed for iterations becomes slower. After N=100 times the g(w) becomes 363.89 compared the original methode of 360.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Resolution of the full problem\n",
    "### Question 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_h(h,w=W0):\n",
    "    [n,p]=M.shape\n",
    "    h=h.reshape(k,p)\n",
    "    m_t=np.dot(w,h)\n",
    "    g=np.linalg.norm(m_t-M,ord='fro')**2\n",
    "    g=g/(2*n*p)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_gradient_h(h,w=W0):\n",
    "    [n,p]=M.shape\n",
    "    h=h.reshape(k,p)\n",
    "    m_t=np.dot(w,h)\n",
    "    g=-1*np.dot(w.T,(M-m_t))/(n*p)\n",
    "    return np.ravel(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_method_line_search_wh(val_g,grad_g,grad_h,w,h,alpha,gamma,N):\n",
    "    for i in range(N):\n",
    "        W0,H0=w,h\n",
    "        w=w-(gamma*grad_g(W0)).reshape(n,k)\n",
    "        w=np.maximum(0,w)\n",
    "        w=W0+alpha*(w-W0)\n",
    "        h=h-(gamma*grad_h(H0)).reshape(k,p)\n",
    "        h=np.maximum(0,h)\n",
    "        h=H0+alpha*(h-H0)\n",
    "    return(val_g(w,h),w,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before the iterations, the value of g(W0) is : 414.39665223126826\n",
      "After N=1000 times iterations with the learning rate=1,alpha=0.5,we get new value of g(w) is: 304.8092383649963\n",
      "The time it costs is : 3.825888156890869\n"
     ]
    }
   ],
   "source": [
    "print(\"Before the iterations, the value of g(W0) is :\",g_w(W0))\n",
    "gamma=1\n",
    "N=1000\n",
    "alpha=0.5\n",
    "start=time.time()\n",
    "g_new,w_new,h_new=projected_gradient_method_line_search_wh(g_w,g_gradient,g_gradient_h,W0,H0,alpha,gamma,N)\n",
    "end=time.time()\n",
    "print(\"After N=1000 times iterations with the learning rate=1,alpha=0.5,we get new value of g(w) is:\",g_new)\n",
    "print(\"The time it costs is :\",end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "For the $W_t$ and $H_t$, we get the $W_{t+1}$ by searching the minimized value for the objective value with $H_t$, so the objective value for $W_{t+1}$ and $H_t$ decreases in this process. The same reason, we could also get the smaller objective value for searching $H_{t+1}$ when $W_{t+1}$ is not changed. So the final objective value for $W_{t+1}$ and $H_{t+1}$ decreases in every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_method_H(val_h,grad_h,H0,w,gamma,N):\n",
    "    h=H0\n",
    "    for i in range(N):\n",
    "        h=h-gamma*grad_h(h,w).reshape(k,p)\n",
    "        h=np.maximum(0,h)\n",
    "        H0=h\n",
    "    return(val_h(h,w),h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternate_minimization_method(W0,H0,t):\n",
    "    w,h=W0,H0\n",
    "    gamma=1\n",
    "    N=100\n",
    "    for i in range(t):\n",
    "        value,w=projected_gradient_method(g_w,g_gradient,w,h,gamma,N)\n",
    "        value,h=projected_gradient_method_H(g_h,g_gradient_h,h,w,gamma,N)      \n",
    "        print(\"After the\",i+1,\" times iterations the value of the objective is:\",value)\n",
    "    return(value,w,h)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the 1  times iterations the value of the objective is: 244.29925392619512\n",
      "After the 2  times iterations the value of the objective is: 182.04171373838338\n",
      "After the 3  times iterations the value of the objective is: 162.28036141287564\n",
      "After the 4  times iterations the value of the objective is: 153.00596446942504\n",
      "After the 5  times iterations the value of the objective is: 147.6585039065165\n",
      "After the 6  times iterations the value of the objective is: 144.01616494608174\n",
      "After the 7  times iterations the value of the objective is: 141.35705914316492\n",
      "After the 8  times iterations the value of the objective is: 139.31406992573352\n",
      "After the 9  times iterations the value of the objective is: 137.6779678619709\n",
      "After the 10  times iterations the value of the objective is: 136.32265195437034\n",
      "The final value of objective is 136.32265195437034\n",
      "The time it costs is : 4.228513956069946\n"
     ]
    }
   ],
   "source": [
    "t=10\n",
    "start=time.time()\n",
    "value,w_new,h_new=alternate_minimization_method(W0,H0,t)\n",
    "end=time.time()\n",
    "print(\"The final value of objective is\",value)\n",
    "print(\"The time it costs is :\",end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.4\n",
    "   The alternate minimizations method gets the better result which reduce the value of objective to 136.3, while the projected gradient mothod only reduce the value of objective to 304.8. So from the aspect of the objective value,the alternate minimizations method is better.\n",
    "   Comparing the computing time,the alternate minimizations method costs about 4.2s and the projected gradient mothod costs about 3.8s. There are not big differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If the $||g(W_t)-g(W_{t+1})||<\\epsilon$,with the $\\epsilon$ is a selected small value like 0.01, we could stop the iteration. We could use this as the stopping criterion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
