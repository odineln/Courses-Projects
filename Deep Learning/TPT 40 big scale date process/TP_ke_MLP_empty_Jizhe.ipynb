{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PW2E5Sn8qsYR"
   },
   "source": [
    "# ATHENS 2018\n",
    "# Practice of Large Scale Machine Learning \n",
    "\n",
    "## TP MLP in Keras (2018/11/22)\n",
    "\n",
    "For any remark or suggestion, please feel free to contact us at:\n",
    "geoffroy.peeters@telecom-paristech.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KDkdbdDQqsYT"
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "llMfT-9qqsYU",
    "outputId": "91cc27c0-4e5e-4d56-8207-b903309b3129"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from https://github.com/keras-team/keras/tree/master/examples\n",
    "# keras generator: https://medium.com/@fromtheast/implement-fit-generator-in-keras-61aa2786ce98\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Ojv4FpOqsYe"
   },
   "source": [
    "# 1. Simple MLP model with 2 output classes (binary classification):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdORur3MqsYf"
   },
   "source": [
    "## Generate dummy classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "Vw0Di4ywqsYg",
    "outputId": "1f4baa63-c53b-4e22-c3f1-446177fcbed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 20)\n",
      "(200, 20)\n",
      "(800,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# Generate dummy data\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = datasets.make_classification(n_samples=1000, n_features=20, n_classes=2)\n",
    "x = data[0]\n",
    "y = data[1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQbo79vCqsYn"
   },
   "source": [
    "## Create model\n",
    "\n",
    "Create a two-layers MLP (1 hidden layer) with \n",
    "- 32 hidden units\n",
    "- the first activation function as Sigmoid \n",
    "- the output activation function as Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "D9bqqjctqsYo",
    "outputId": "dfbf2b5d-7d61-4500-f154-3446393e8b9c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                672       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 705\n",
      "Trainable params: 705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# START CODE HERE\n",
    "keras.backend.clear_session()\n",
    "model=Sequential()\n",
    "model.add(Dense(32,activation='sigmoid',input_dim=20))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "# END CODE HERE\n",
    "\n",
    "# --- Display the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6OG9Pd6LqsYv"
   },
   "source": [
    "## Define loss (to minimize) and optimizer (how to update the weights)\n",
    "\n",
    "Configures the model for training: \n",
    "- define an ```optimizer``` (we will use ```sgd``` with a learn_rate (lr) or 0.01), \n",
    "- define the ```loss```to be minimized (which loss should be used for a binary classification problem ?)\n",
    "- define a list of ```metrics```to be displayed after each epoch (here we will use ```accuracy```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60ZZye1sqsYw"
   },
   "outputs": [],
   "source": [
    "# START CODE HERE\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SMHI76NqsY0"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Use \n",
    "- batches of size 32 \n",
    "- iterate for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7235
    },
    "colab_type": "code",
    "id": "BVDoDu0XqsY1",
    "outputId": "c9a756fa-065a-4c92-c763-a6286a33390b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "800/800 [==============================] - 0s 197us/step - loss: 0.7567 - acc: 0.5100\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.6987 - acc: 0.5375\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.6689 - acc: 0.6013\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.6494 - acc: 0.6512\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 48us/step - loss: 0.6339 - acc: 0.6725\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 48us/step - loss: 0.6199 - acc: 0.7063\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.6069 - acc: 0.7350\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.5943 - acc: 0.7575\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 54us/step - loss: 0.5823 - acc: 0.7750\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.5708 - acc: 0.7850\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.5598 - acc: 0.8013\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.5488 - acc: 0.8163\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 53us/step - loss: 0.5383 - acc: 0.8250\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.5281 - acc: 0.8425\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 51us/step - loss: 0.5181 - acc: 0.8513\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.5086 - acc: 0.8600\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.4990 - acc: 0.8650\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.4898 - acc: 0.8725\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.4809 - acc: 0.8788\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.4721 - acc: 0.8825\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.4636 - acc: 0.8837\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.4553 - acc: 0.8900\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.4472 - acc: 0.8938\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.4394 - acc: 0.9012\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.4318 - acc: 0.9037\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.4243 - acc: 0.9075\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.4170 - acc: 0.9050\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.4098 - acc: 0.9062\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.4029 - acc: 0.9075\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.3963 - acc: 0.9075\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.3897 - acc: 0.9075\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.3833 - acc: 0.9100\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.3772 - acc: 0.9088\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.3711 - acc: 0.9088\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.3653 - acc: 0.9100\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.3595 - acc: 0.9125\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.3539 - acc: 0.9138\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.3486 - acc: 0.9100\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.3433 - acc: 0.9150\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.3381 - acc: 0.9163\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.3332 - acc: 0.9163\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.3283 - acc: 0.9200\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.3235 - acc: 0.9213\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.3190 - acc: 0.9225\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.3145 - acc: 0.9250\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.3102 - acc: 0.9263\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.3060 - acc: 0.9263\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.3018 - acc: 0.9288\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.2979 - acc: 0.9312\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2939 - acc: 0.9300\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.2902 - acc: 0.9300\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.2865 - acc: 0.9312\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.2829 - acc: 0.9312\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.2795 - acc: 0.9300\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.2760 - acc: 0.9300\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.2728 - acc: 0.9325\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.2695 - acc: 0.9325\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.2664 - acc: 0.9325\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.2634 - acc: 0.9337\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.2604 - acc: 0.9337\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2575 - acc: 0.9350\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.2547 - acc: 0.9350\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.2520 - acc: 0.9363\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2493 - acc: 0.9375\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.2467 - acc: 0.9387\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.2442 - acc: 0.9363\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.2417 - acc: 0.9400\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.2393 - acc: 0.9413\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.2371 - acc: 0.9413\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.2347 - acc: 0.9413\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.2325 - acc: 0.9413\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.2303 - acc: 0.9413\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.2282 - acc: 0.9413\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.2262 - acc: 0.9425\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.2242 - acc: 0.9413\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.2222 - acc: 0.9450\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.2203 - acc: 0.9475\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.2184 - acc: 0.9487\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.2167 - acc: 0.9487\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.2149 - acc: 0.9487\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.2131 - acc: 0.9487\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.2115 - acc: 0.9525\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.2098 - acc: 0.9525\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.2082 - acc: 0.9513\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.2066 - acc: 0.9513\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.2051 - acc: 0.9513\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.2036 - acc: 0.9525\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.2022 - acc: 0.9525\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.2007 - acc: 0.9525\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.1994 - acc: 0.9525\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1980 - acc: 0.9525\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1967 - acc: 0.9537\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1954 - acc: 0.9550\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1941 - acc: 0.9537\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1929 - acc: 0.9537\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1917 - acc: 0.9537\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.1905 - acc: 0.9537\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1894 - acc: 0.9537\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1882 - acc: 0.9537\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.1871 - acc: 0.9537\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1861 - acc: 0.9537\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1850 - acc: 0.9537\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1840 - acc: 0.9550\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1830 - acc: 0.9550\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.1820 - acc: 0.9550\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1810 - acc: 0.9550\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1801 - acc: 0.9537\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1792 - acc: 0.9537\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.1782 - acc: 0.9550\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1774 - acc: 0.9537\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1765 - acc: 0.9563\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1757 - acc: 0.9550\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.1748 - acc: 0.9563\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1740 - acc: 0.9563\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1732 - acc: 0.9563\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1724 - acc: 0.9563\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1717 - acc: 0.9563\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.1709 - acc: 0.9563\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1702 - acc: 0.9563\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1695 - acc: 0.9563\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1688 - acc: 0.9563\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1681 - acc: 0.9575\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1674 - acc: 0.9575\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1667 - acc: 0.9575\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1661 - acc: 0.9575\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.1654 - acc: 0.9575\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1648 - acc: 0.9575\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.1642 - acc: 0.9575\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1636 - acc: 0.9575\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1630 - acc: 0.9575\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1624 - acc: 0.9587\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1619 - acc: 0.9587\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.1613 - acc: 0.9600\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.1608 - acc: 0.9600\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1602 - acc: 0.9600\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.1597 - acc: 0.9600\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1592 - acc: 0.9600\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1586 - acc: 0.9612\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1582 - acc: 0.9612\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1576 - acc: 0.9612\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1572 - acc: 0.9612\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1567 - acc: 0.9612\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1562 - acc: 0.9612\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1558 - acc: 0.9625\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1553 - acc: 0.9625\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1549 - acc: 0.9625\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1545 - acc: 0.9625\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1540 - acc: 0.9625\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1536 - acc: 0.9625\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1532 - acc: 0.9625\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1528 - acc: 0.9625\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.1524 - acc: 0.9625\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1520 - acc: 0.9625\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 50us/step - loss: 0.1516 - acc: 0.9625\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1513 - acc: 0.9625\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1509 - acc: 0.9625\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1505 - acc: 0.9625\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1502 - acc: 0.9625\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1498 - acc: 0.9625\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1495 - acc: 0.9637\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1491 - acc: 0.9637\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1488 - acc: 0.9625\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1485 - acc: 0.9637\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1481 - acc: 0.9637\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1478 - acc: 0.9637\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.1475 - acc: 0.9637\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1472 - acc: 0.9650\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1469 - acc: 0.9650\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1466 - acc: 0.9650\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1463 - acc: 0.9650\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1460 - acc: 0.9650\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1457 - acc: 0.9650\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1454 - acc: 0.9650\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.1451 - acc: 0.9650\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1449 - acc: 0.9650\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1446 - acc: 0.9650\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1443 - acc: 0.9650\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1441 - acc: 0.9650\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1438 - acc: 0.9662\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1436 - acc: 0.9662\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 48us/step - loss: 0.1433 - acc: 0.9662\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1430 - acc: 0.9662\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1428 - acc: 0.9662\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1425 - acc: 0.9662\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1423 - acc: 0.9662\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1421 - acc: 0.9662\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1418 - acc: 0.9662\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1416 - acc: 0.9662\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1414 - acc: 0.9662\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1412 - acc: 0.9662\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1410 - acc: 0.9662\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1407 - acc: 0.9662\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1405 - acc: 0.9662\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1403 - acc: 0.9662\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1401 - acc: 0.9662\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1399 - acc: 0.9662\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1397 - acc: 0.9662\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1395 - acc: 0.9662\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1393 - acc: 0.9662\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1391 - acc: 0.9662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa97e6dda90>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# START CODE HERE\n",
    "model.fit(x_train,y_train,epochs=200,batch_size=32)\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GxuryivoqsY8"
   },
   "source": [
    "## Evaluate the performance of your model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ABUgw_LzqsY-",
    "outputId": "86b15c55-43c2-4f3b-8af9-ec67a53831a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 208us/step\n",
      "[0.09590598165988923, 0.985]\n"
     ]
    }
   ],
   "source": [
    "# START CODE HERE\n",
    "score=model.evaluate(x_test,y_test,batch_size=32)\n",
    "print(score)\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfR_ZJDCqsZG"
   },
   "source": [
    "## Evaluate the performance of your model on the test data during training time\n",
    "\n",
    "Reset your model (by running again its definition code) and measure the performance of your model on test data during training time.\n",
    "- How does the loss on the testing data behaves over time ?\n",
    "- When should we stop training the model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7253
    },
    "colab_type": "code",
    "id": "etyxpLN0qsZH",
    "outputId": "82101d06-e95d-40ee-cdaa-03c8349e9500"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 0s 208us/step - loss: 0.6929 - acc: 0.5288 - val_loss: 0.6871 - val_acc: 0.5400\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.6750 - acc: 0.5725 - val_loss: 0.6710 - val_acc: 0.5950\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.6589 - acc: 0.6550 - val_loss: 0.6563 - val_acc: 0.6600\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.6439 - acc: 0.7025 - val_loss: 0.6422 - val_acc: 0.7000\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.6298 - acc: 0.7538 - val_loss: 0.6286 - val_acc: 0.7200\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 48us/step - loss: 0.6162 - acc: 0.7888 - val_loss: 0.6155 - val_acc: 0.7700\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 52us/step - loss: 0.6032 - acc: 0.7987 - val_loss: 0.6028 - val_acc: 0.8150\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.5903 - acc: 0.8188 - val_loss: 0.5906 - val_acc: 0.8300\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.5782 - acc: 0.8375 - val_loss: 0.5787 - val_acc: 0.8300\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.5664 - acc: 0.8463 - val_loss: 0.5671 - val_acc: 0.8450\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 50us/step - loss: 0.5546 - acc: 0.8612 - val_loss: 0.5558 - val_acc: 0.8700\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.5434 - acc: 0.8700 - val_loss: 0.5448 - val_acc: 0.8750\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.5324 - acc: 0.8775 - val_loss: 0.5341 - val_acc: 0.8750\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 54us/step - loss: 0.5217 - acc: 0.8825 - val_loss: 0.5237 - val_acc: 0.8750\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.5113 - acc: 0.8912 - val_loss: 0.5135 - val_acc: 0.8850\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.5013 - acc: 0.8950 - val_loss: 0.5036 - val_acc: 0.8950\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.4915 - acc: 0.8938 - val_loss: 0.4939 - val_acc: 0.8950\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.4818 - acc: 0.8987 - val_loss: 0.4844 - val_acc: 0.8950\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.4727 - acc: 0.8962 - val_loss: 0.4752 - val_acc: 0.8950\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.4634 - acc: 0.9062 - val_loss: 0.4661 - val_acc: 0.8900\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.4546 - acc: 0.9050 - val_loss: 0.4573 - val_acc: 0.8950\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.4461 - acc: 0.9075 - val_loss: 0.4488 - val_acc: 0.9000\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 48us/step - loss: 0.4377 - acc: 0.9150 - val_loss: 0.4404 - val_acc: 0.9050\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 48us/step - loss: 0.4296 - acc: 0.9150 - val_loss: 0.4322 - val_acc: 0.9200\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.4216 - acc: 0.9163 - val_loss: 0.4242 - val_acc: 0.9250\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 48us/step - loss: 0.4140 - acc: 0.9200 - val_loss: 0.4164 - val_acc: 0.9300\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.4064 - acc: 0.9200 - val_loss: 0.4089 - val_acc: 0.9300\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.3993 - acc: 0.9237 - val_loss: 0.4015 - val_acc: 0.9300\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.3922 - acc: 0.9237 - val_loss: 0.3943 - val_acc: 0.9300\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.3853 - acc: 0.9263 - val_loss: 0.3873 - val_acc: 0.9300\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 48us/step - loss: 0.3787 - acc: 0.9263 - val_loss: 0.3805 - val_acc: 0.9300\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 52us/step - loss: 0.3723 - acc: 0.9275 - val_loss: 0.3739 - val_acc: 0.9300\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.3660 - acc: 0.9300 - val_loss: 0.3674 - val_acc: 0.9300\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 48us/step - loss: 0.3600 - acc: 0.9275 - val_loss: 0.3611 - val_acc: 0.9350\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.3541 - acc: 0.9300 - val_loss: 0.3549 - val_acc: 0.9350\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.3484 - acc: 0.9300 - val_loss: 0.3490 - val_acc: 0.9350\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 52us/step - loss: 0.3428 - acc: 0.9275 - val_loss: 0.3431 - val_acc: 0.9350\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.3375 - acc: 0.9288 - val_loss: 0.3374 - val_acc: 0.9350\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.3322 - acc: 0.9325 - val_loss: 0.3319 - val_acc: 0.9350\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.3272 - acc: 0.9337 - val_loss: 0.3265 - val_acc: 0.9350\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 50us/step - loss: 0.3223 - acc: 0.9337 - val_loss: 0.3213 - val_acc: 0.9350\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.3175 - acc: 0.9337 - val_loss: 0.3162 - val_acc: 0.9350\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.3129 - acc: 0.9350 - val_loss: 0.3112 - val_acc: 0.9350\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.3085 - acc: 0.9375 - val_loss: 0.3063 - val_acc: 0.9350\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.3041 - acc: 0.9375 - val_loss: 0.3016 - val_acc: 0.9350\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.2999 - acc: 0.9375 - val_loss: 0.2970 - val_acc: 0.9350\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 48us/step - loss: 0.2958 - acc: 0.9375 - val_loss: 0.2925 - val_acc: 0.9350\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.2918 - acc: 0.9400 - val_loss: 0.2882 - val_acc: 0.9400\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 48us/step - loss: 0.2880 - acc: 0.9413 - val_loss: 0.2839 - val_acc: 0.9400\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2843 - acc: 0.9438 - val_loss: 0.2798 - val_acc: 0.9400\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.2806 - acc: 0.9413 - val_loss: 0.2757 - val_acc: 0.9450\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.2771 - acc: 0.9438 - val_loss: 0.2717 - val_acc: 0.9450\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.2737 - acc: 0.9450 - val_loss: 0.2679 - val_acc: 0.9450\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2703 - acc: 0.9450 - val_loss: 0.2641 - val_acc: 0.9450\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.2671 - acc: 0.9462 - val_loss: 0.2605 - val_acc: 0.9450\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.2640 - acc: 0.9462 - val_loss: 0.2569 - val_acc: 0.9500\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.2610 - acc: 0.9475 - val_loss: 0.2534 - val_acc: 0.9500\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.2580 - acc: 0.9475 - val_loss: 0.2500 - val_acc: 0.9550\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2552 - acc: 0.9475 - val_loss: 0.2467 - val_acc: 0.9550\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.2524 - acc: 0.9475 - val_loss: 0.2435 - val_acc: 0.9550\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2497 - acc: 0.9475 - val_loss: 0.2403 - val_acc: 0.9550\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.2470 - acc: 0.9475 - val_loss: 0.2372 - val_acc: 0.9550\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.2445 - acc: 0.9475 - val_loss: 0.2342 - val_acc: 0.9550\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2420 - acc: 0.9475 - val_loss: 0.2313 - val_acc: 0.9550\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.2395 - acc: 0.9475 - val_loss: 0.2284 - val_acc: 0.9550\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.2372 - acc: 0.9487 - val_loss: 0.2257 - val_acc: 0.9550\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2349 - acc: 0.9487 - val_loss: 0.2229 - val_acc: 0.9550\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2326 - acc: 0.9487 - val_loss: 0.2202 - val_acc: 0.9550\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.2305 - acc: 0.9487 - val_loss: 0.2176 - val_acc: 0.9550\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 48us/step - loss: 0.2283 - acc: 0.9487 - val_loss: 0.2151 - val_acc: 0.9550\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.2263 - acc: 0.9500 - val_loss: 0.2126 - val_acc: 0.9550\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.2243 - acc: 0.9487 - val_loss: 0.2102 - val_acc: 0.9550\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2224 - acc: 0.9500 - val_loss: 0.2078 - val_acc: 0.9550\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2204 - acc: 0.9500 - val_loss: 0.2055 - val_acc: 0.9600\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2186 - acc: 0.9500 - val_loss: 0.2032 - val_acc: 0.9600\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.2168 - acc: 0.9513 - val_loss: 0.2010 - val_acc: 0.9600\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2150 - acc: 0.9513 - val_loss: 0.1988 - val_acc: 0.9600\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.2133 - acc: 0.9500 - val_loss: 0.1967 - val_acc: 0.9650\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.2116 - acc: 0.9500 - val_loss: 0.1946 - val_acc: 0.9650\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2100 - acc: 0.9513 - val_loss: 0.1925 - val_acc: 0.9650\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.2084 - acc: 0.9513 - val_loss: 0.1906 - val_acc: 0.9650\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.2068 - acc: 0.9513 - val_loss: 0.1886 - val_acc: 0.9650\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.2053 - acc: 0.9525 - val_loss: 0.1867 - val_acc: 0.9650\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.2038 - acc: 0.9525 - val_loss: 0.1849 - val_acc: 0.9650\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.2024 - acc: 0.9525 - val_loss: 0.1831 - val_acc: 0.9700\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.2010 - acc: 0.9525 - val_loss: 0.1813 - val_acc: 0.9700\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1996 - acc: 0.9525 - val_loss: 0.1795 - val_acc: 0.9700\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1982 - acc: 0.9550 - val_loss: 0.1778 - val_acc: 0.9700\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1969 - acc: 0.9550 - val_loss: 0.1762 - val_acc: 0.9700\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1957 - acc: 0.9550 - val_loss: 0.1745 - val_acc: 0.9700\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 52us/step - loss: 0.1944 - acc: 0.9550 - val_loss: 0.1729 - val_acc: 0.9700\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1932 - acc: 0.9550 - val_loss: 0.1714 - val_acc: 0.9800\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1920 - acc: 0.9550 - val_loss: 0.1698 - val_acc: 0.9800\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1909 - acc: 0.9550 - val_loss: 0.1683 - val_acc: 0.9800\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1897 - acc: 0.9550 - val_loss: 0.1668 - val_acc: 0.9800\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1886 - acc: 0.9563 - val_loss: 0.1654 - val_acc: 0.9800\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.1875 - acc: 0.9575 - val_loss: 0.1640 - val_acc: 0.9800\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1864 - acc: 0.9563 - val_loss: 0.1626 - val_acc: 0.9800\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1854 - acc: 0.9575 - val_loss: 0.1612 - val_acc: 0.9800\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1844 - acc: 0.9575 - val_loss: 0.1599 - val_acc: 0.9800\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1834 - acc: 0.9587 - val_loss: 0.1586 - val_acc: 0.9800\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1824 - acc: 0.9587 - val_loss: 0.1573 - val_acc: 0.9800\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.1814 - acc: 0.9587 - val_loss: 0.1560 - val_acc: 0.9800\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1805 - acc: 0.9587 - val_loss: 0.1548 - val_acc: 0.9800\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1796 - acc: 0.9600 - val_loss: 0.1535 - val_acc: 0.9800\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1787 - acc: 0.9600 - val_loss: 0.1524 - val_acc: 0.9800\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1778 - acc: 0.9600 - val_loss: 0.1512 - val_acc: 0.9800\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1769 - acc: 0.9600 - val_loss: 0.1500 - val_acc: 0.9800\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1761 - acc: 0.9600 - val_loss: 0.1489 - val_acc: 0.9800\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1753 - acc: 0.9600 - val_loss: 0.1478 - val_acc: 0.9800\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1745 - acc: 0.9600 - val_loss: 0.1467 - val_acc: 0.9800\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1737 - acc: 0.9600 - val_loss: 0.1456 - val_acc: 0.9800\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1729 - acc: 0.9612 - val_loss: 0.1446 - val_acc: 0.9800\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1722 - acc: 0.9625 - val_loss: 0.1436 - val_acc: 0.9800\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1714 - acc: 0.9625 - val_loss: 0.1426 - val_acc: 0.9800\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.1707 - acc: 0.9625 - val_loss: 0.1416 - val_acc: 0.9800\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.1700 - acc: 0.9625 - val_loss: 0.1406 - val_acc: 0.9800\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1692 - acc: 0.9625 - val_loss: 0.1396 - val_acc: 0.9850\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1686 - acc: 0.9625 - val_loss: 0.1386 - val_acc: 0.9850\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1679 - acc: 0.9625 - val_loss: 0.1377 - val_acc: 0.9850\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.1673 - acc: 0.9625 - val_loss: 0.1368 - val_acc: 0.9850\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1666 - acc: 0.9625 - val_loss: 0.1359 - val_acc: 0.9850\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.1660 - acc: 0.9625 - val_loss: 0.1350 - val_acc: 0.9850\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1654 - acc: 0.9625 - val_loss: 0.1342 - val_acc: 0.9850\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.1647 - acc: 0.9625 - val_loss: 0.1333 - val_acc: 0.9850\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1641 - acc: 0.9625 - val_loss: 0.1325 - val_acc: 0.9900\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1635 - acc: 0.9625 - val_loss: 0.1317 - val_acc: 0.9900\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1630 - acc: 0.9625 - val_loss: 0.1309 - val_acc: 0.9900\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1624 - acc: 0.9625 - val_loss: 0.1301 - val_acc: 0.9900\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1619 - acc: 0.9625 - val_loss: 0.1293 - val_acc: 0.9900\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.1613 - acc: 0.9625 - val_loss: 0.1285 - val_acc: 0.9900\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1607 - acc: 0.9625 - val_loss: 0.1278 - val_acc: 0.9900\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1602 - acc: 0.9625 - val_loss: 0.1270 - val_acc: 0.9900\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.1597 - acc: 0.9625 - val_loss: 0.1263 - val_acc: 0.9900\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1592 - acc: 0.9625 - val_loss: 0.1256 - val_acc: 0.9900\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1587 - acc: 0.9625 - val_loss: 0.1249 - val_acc: 0.9900\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1582 - acc: 0.9625 - val_loss: 0.1242 - val_acc: 0.9900\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1577 - acc: 0.9625 - val_loss: 0.1235 - val_acc: 0.9900\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1573 - acc: 0.9625 - val_loss: 0.1228 - val_acc: 0.9900\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1568 - acc: 0.9625 - val_loss: 0.1222 - val_acc: 0.9900\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1563 - acc: 0.9625 - val_loss: 0.1215 - val_acc: 0.9900\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1558 - acc: 0.9625 - val_loss: 0.1208 - val_acc: 0.9900\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1554 - acc: 0.9625 - val_loss: 0.1202 - val_acc: 0.9900\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1550 - acc: 0.9625 - val_loss: 0.1196 - val_acc: 0.9900\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1545 - acc: 0.9637 - val_loss: 0.1190 - val_acc: 0.9900\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1541 - acc: 0.9637 - val_loss: 0.1184 - val_acc: 0.9900\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1537 - acc: 0.9637 - val_loss: 0.1178 - val_acc: 0.9900\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1533 - acc: 0.9637 - val_loss: 0.1172 - val_acc: 0.9900\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1529 - acc: 0.9637 - val_loss: 0.1166 - val_acc: 0.9900\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1525 - acc: 0.9637 - val_loss: 0.1160 - val_acc: 0.9900\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1521 - acc: 0.9637 - val_loss: 0.1155 - val_acc: 0.9900\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1517 - acc: 0.9637 - val_loss: 0.1149 - val_acc: 0.9900\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1514 - acc: 0.9637 - val_loss: 0.1144 - val_acc: 0.9900\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1510 - acc: 0.9650 - val_loss: 0.1138 - val_acc: 0.9900\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.1507 - acc: 0.9650 - val_loss: 0.1133 - val_acc: 0.9900\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1503 - acc: 0.9650 - val_loss: 0.1128 - val_acc: 0.9900\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1499 - acc: 0.9637 - val_loss: 0.1123 - val_acc: 0.9900\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1496 - acc: 0.9650 - val_loss: 0.1118 - val_acc: 0.9900\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1492 - acc: 0.9637 - val_loss: 0.1112 - val_acc: 0.9900\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1489 - acc: 0.9637 - val_loss: 0.1107 - val_acc: 0.9900\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1486 - acc: 0.9650 - val_loss: 0.1103 - val_acc: 0.9900\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1482 - acc: 0.9650 - val_loss: 0.1098 - val_acc: 0.9900\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1479 - acc: 0.9637 - val_loss: 0.1093 - val_acc: 0.9900\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1476 - acc: 0.9637 - val_loss: 0.1088 - val_acc: 0.9900\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1473 - acc: 0.9650 - val_loss: 0.1084 - val_acc: 0.9900\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1470 - acc: 0.9637 - val_loss: 0.1079 - val_acc: 0.9900\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1467 - acc: 0.9637 - val_loss: 0.1074 - val_acc: 0.9900\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1464 - acc: 0.9637 - val_loss: 0.1070 - val_acc: 0.9900\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1461 - acc: 0.9637 - val_loss: 0.1065 - val_acc: 0.9900\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.1458 - acc: 0.9650 - val_loss: 0.1061 - val_acc: 0.9900\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1455 - acc: 0.9637 - val_loss: 0.1057 - val_acc: 0.9900\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1452 - acc: 0.9650 - val_loss: 0.1053 - val_acc: 0.9900\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1449 - acc: 0.9637 - val_loss: 0.1049 - val_acc: 0.9900\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.1446 - acc: 0.9637 - val_loss: 0.1045 - val_acc: 0.9900\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1444 - acc: 0.9637 - val_loss: 0.1041 - val_acc: 0.9900\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1441 - acc: 0.9637 - val_loss: 0.1037 - val_acc: 0.9900\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1439 - acc: 0.9637 - val_loss: 0.1033 - val_acc: 0.9900\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.1436 - acc: 0.9637 - val_loss: 0.1029 - val_acc: 0.9900\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1433 - acc: 0.9637 - val_loss: 0.1025 - val_acc: 0.9900\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1431 - acc: 0.9637 - val_loss: 0.1021 - val_acc: 0.9900\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1428 - acc: 0.9637 - val_loss: 0.1017 - val_acc: 0.9900\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1426 - acc: 0.9637 - val_loss: 0.1013 - val_acc: 0.9900\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.1423 - acc: 0.9637 - val_loss: 0.1010 - val_acc: 0.9900\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1421 - acc: 0.9637 - val_loss: 0.1006 - val_acc: 0.9900\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1419 - acc: 0.9637 - val_loss: 0.1002 - val_acc: 0.9900\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1417 - acc: 0.9637 - val_loss: 0.0999 - val_acc: 0.9900\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1414 - acc: 0.9637 - val_loss: 0.0995 - val_acc: 0.9900\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1412 - acc: 0.9637 - val_loss: 0.0992 - val_acc: 0.9900\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1409 - acc: 0.9637 - val_loss: 0.0989 - val_acc: 0.9900\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1407 - acc: 0.9637 - val_loss: 0.0985 - val_acc: 0.9900\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1405 - acc: 0.9637 - val_loss: 0.0982 - val_acc: 0.9900\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.1403 - acc: 0.9637 - val_loss: 0.0979 - val_acc: 0.9900\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1401 - acc: 0.9637 - val_loss: 0.0975 - val_acc: 0.9900\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.1399 - acc: 0.9637 - val_loss: 0.0972 - val_acc: 0.9900\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1397 - acc: 0.9637 - val_loss: 0.0969 - val_acc: 0.9900\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1395 - acc: 0.9637 - val_loss: 0.0966 - val_acc: 0.9900\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1393 - acc: 0.9637 - val_loss: 0.0963 - val_acc: 0.9900\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1391 - acc: 0.9637 - val_loss: 0.0960 - val_acc: 0.9900\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.1389 - acc: 0.9637 - val_loss: 0.0957 - val_acc: 0.9900\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.1387 - acc: 0.9637 - val_loss: 0.0954 - val_acc: 0.9900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa969e8d2e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# START CODE HERE\n",
    "keras.backend.clear_session()\n",
    "model=Sequential()\n",
    "model.add(Dense(32,activation='sigmoid',input_dim=20))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,batch_size=32,epochs=200,verbose=1,validation_data=(x_test,y_test))\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xktmHJYEqsZP"
   },
   "source": [
    "### Questions:\n",
    "\n",
    "What happens if:\n",
    "- we replace the activation function of the first layer by a ReLu ?\n",
    "- you change the learning_rate to 0.0001, to 0.1 ?\n",
    "- we increase the number of hidden units in the hidden layer ?\n",
    "- we increase the number two two hidden layers ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7253
    },
    "colab_type": "code",
    "id": "hQOfB6-k6cJf",
    "outputId": "3bf2a660-4673-42af-bbdb-4a525c4d9f7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 0s 419us/step - loss: 0.7804 - acc: 0.4563 - val_loss: 0.7566 - val_acc: 0.4800\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 122us/step - loss: 0.7040 - acc: 0.5388 - val_loss: 0.6885 - val_acc: 0.5450\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.6437 - acc: 0.6350 - val_loss: 0.6335 - val_acc: 0.6150\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.5945 - acc: 0.6925 - val_loss: 0.5875 - val_acc: 0.6750\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.5529 - acc: 0.7375 - val_loss: 0.5486 - val_acc: 0.7400\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.5176 - acc: 0.7825 - val_loss: 0.5150 - val_acc: 0.7650\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.4869 - acc: 0.8150 - val_loss: 0.4855 - val_acc: 0.8050\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.4600 - acc: 0.8425 - val_loss: 0.4593 - val_acc: 0.8250\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.4364 - acc: 0.8525 - val_loss: 0.4361 - val_acc: 0.8550\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.4153 - acc: 0.8625 - val_loss: 0.4152 - val_acc: 0.8550\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.3964 - acc: 0.8725 - val_loss: 0.3966 - val_acc: 0.8650\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 124us/step - loss: 0.3796 - acc: 0.8750 - val_loss: 0.3797 - val_acc: 0.8650\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.3645 - acc: 0.8875 - val_loss: 0.3645 - val_acc: 0.8800\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 128us/step - loss: 0.3508 - acc: 0.8938 - val_loss: 0.3506 - val_acc: 0.8800\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.3384 - acc: 0.9012 - val_loss: 0.3380 - val_acc: 0.8800\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.3272 - acc: 0.9025 - val_loss: 0.3266 - val_acc: 0.8850\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.3169 - acc: 0.9050 - val_loss: 0.3161 - val_acc: 0.8850\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.3077 - acc: 0.9050 - val_loss: 0.3064 - val_acc: 0.8900\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.2993 - acc: 0.9113 - val_loss: 0.2976 - val_acc: 0.8950\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.2915 - acc: 0.9113 - val_loss: 0.2894 - val_acc: 0.8950\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.2844 - acc: 0.9150 - val_loss: 0.2819 - val_acc: 0.8950\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.2780 - acc: 0.9175 - val_loss: 0.2749 - val_acc: 0.9000\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.2720 - acc: 0.9213 - val_loss: 0.2685 - val_acc: 0.9000\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.2664 - acc: 0.9200 - val_loss: 0.2625 - val_acc: 0.9050\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.2613 - acc: 0.9213 - val_loss: 0.2569 - val_acc: 0.9100\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.2565 - acc: 0.9200 - val_loss: 0.2516 - val_acc: 0.9100\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.2521 - acc: 0.9225 - val_loss: 0.2467 - val_acc: 0.9100\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.2480 - acc: 0.9225 - val_loss: 0.2421 - val_acc: 0.9150\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.2442 - acc: 0.9237 - val_loss: 0.2378 - val_acc: 0.9150\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.2406 - acc: 0.9237 - val_loss: 0.2338 - val_acc: 0.9150\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.2373 - acc: 0.9237 - val_loss: 0.2299 - val_acc: 0.9150\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.2341 - acc: 0.9225 - val_loss: 0.2263 - val_acc: 0.9200\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.2312 - acc: 0.9237 - val_loss: 0.2229 - val_acc: 0.9250\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.2283 - acc: 0.9263 - val_loss: 0.2197 - val_acc: 0.9250\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 158us/step - loss: 0.2257 - acc: 0.9275 - val_loss: 0.2167 - val_acc: 0.9250\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.2233 - acc: 0.9300 - val_loss: 0.2139 - val_acc: 0.9250\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.2209 - acc: 0.9288 - val_loss: 0.2112 - val_acc: 0.9250\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.2187 - acc: 0.9288 - val_loss: 0.2087 - val_acc: 0.9250\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.2166 - acc: 0.9325 - val_loss: 0.2062 - val_acc: 0.9250\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.2146 - acc: 0.9300 - val_loss: 0.2040 - val_acc: 0.9250\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.2128 - acc: 0.9300 - val_loss: 0.2018 - val_acc: 0.9300\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.2111 - acc: 0.9312 - val_loss: 0.1998 - val_acc: 0.9350\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.2093 - acc: 0.9312 - val_loss: 0.1978 - val_acc: 0.9350\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.2077 - acc: 0.9325 - val_loss: 0.1960 - val_acc: 0.9350\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.2062 - acc: 0.9325 - val_loss: 0.1942 - val_acc: 0.9350\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.2048 - acc: 0.9325 - val_loss: 0.1926 - val_acc: 0.9350\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.2034 - acc: 0.9337 - val_loss: 0.1910 - val_acc: 0.9350\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.2021 - acc: 0.9363 - val_loss: 0.1895 - val_acc: 0.9350\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.2008 - acc: 0.9350 - val_loss: 0.1881 - val_acc: 0.9350\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1995 - acc: 0.9350 - val_loss: 0.1868 - val_acc: 0.9350\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1984 - acc: 0.9350 - val_loss: 0.1855 - val_acc: 0.9350\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1973 - acc: 0.9350 - val_loss: 0.1843 - val_acc: 0.9350\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1962 - acc: 0.9350 - val_loss: 0.1831 - val_acc: 0.9350\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.1952 - acc: 0.9363 - val_loss: 0.1819 - val_acc: 0.9350\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1942 - acc: 0.9363 - val_loss: 0.1809 - val_acc: 0.9350\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1933 - acc: 0.9363 - val_loss: 0.1799 - val_acc: 0.9350\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.1924 - acc: 0.9363 - val_loss: 0.1789 - val_acc: 0.9350\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1915 - acc: 0.9363 - val_loss: 0.1779 - val_acc: 0.9350\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1906 - acc: 0.9363 - val_loss: 0.1770 - val_acc: 0.9350\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1898 - acc: 0.9363 - val_loss: 0.1762 - val_acc: 0.9350\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1890 - acc: 0.9363 - val_loss: 0.1753 - val_acc: 0.9350\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1882 - acc: 0.9363 - val_loss: 0.1745 - val_acc: 0.9350\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1874 - acc: 0.9363 - val_loss: 0.1737 - val_acc: 0.9350\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1867 - acc: 0.9363 - val_loss: 0.1730 - val_acc: 0.9350\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1859 - acc: 0.9387 - val_loss: 0.1723 - val_acc: 0.9350\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1852 - acc: 0.9387 - val_loss: 0.1716 - val_acc: 0.9350\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1845 - acc: 0.9387 - val_loss: 0.1709 - val_acc: 0.9350\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1839 - acc: 0.9387 - val_loss: 0.1703 - val_acc: 0.9350\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1832 - acc: 0.9387 - val_loss: 0.1697 - val_acc: 0.9350\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1827 - acc: 0.9387 - val_loss: 0.1691 - val_acc: 0.9350\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1821 - acc: 0.9387 - val_loss: 0.1685 - val_acc: 0.9350\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1815 - acc: 0.9400 - val_loss: 0.1679 - val_acc: 0.9350\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 160us/step - loss: 0.1808 - acc: 0.9387 - val_loss: 0.1674 - val_acc: 0.9350\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1803 - acc: 0.9387 - val_loss: 0.1668 - val_acc: 0.9350\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1797 - acc: 0.9375 - val_loss: 0.1663 - val_acc: 0.9350\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1790 - acc: 0.9375 - val_loss: 0.1658 - val_acc: 0.9350\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1785 - acc: 0.9400 - val_loss: 0.1653 - val_acc: 0.9350\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1780 - acc: 0.9400 - val_loss: 0.1648 - val_acc: 0.9350\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1775 - acc: 0.9413 - val_loss: 0.1644 - val_acc: 0.9350\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1770 - acc: 0.9425 - val_loss: 0.1640 - val_acc: 0.9350\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1764 - acc: 0.9400 - val_loss: 0.1636 - val_acc: 0.9350\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.1759 - acc: 0.9425 - val_loss: 0.1631 - val_acc: 0.9350\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1754 - acc: 0.9413 - val_loss: 0.1627 - val_acc: 0.9350\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1749 - acc: 0.9450 - val_loss: 0.1623 - val_acc: 0.9350\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1744 - acc: 0.9438 - val_loss: 0.1619 - val_acc: 0.9350\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.1739 - acc: 0.9450 - val_loss: 0.1616 - val_acc: 0.9350\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1734 - acc: 0.9438 - val_loss: 0.1612 - val_acc: 0.9350\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.1730 - acc: 0.9462 - val_loss: 0.1608 - val_acc: 0.9350\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.1724 - acc: 0.9450 - val_loss: 0.1605 - val_acc: 0.9350\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1721 - acc: 0.9462 - val_loss: 0.1602 - val_acc: 0.9350\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.1715 - acc: 0.9475 - val_loss: 0.1598 - val_acc: 0.9350\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.1711 - acc: 0.9487 - val_loss: 0.1595 - val_acc: 0.9350\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1707 - acc: 0.9487 - val_loss: 0.1592 - val_acc: 0.9400\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1703 - acc: 0.9487 - val_loss: 0.1589 - val_acc: 0.9400\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1699 - acc: 0.9487 - val_loss: 0.1586 - val_acc: 0.9400\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1695 - acc: 0.9487 - val_loss: 0.1583 - val_acc: 0.9400\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1692 - acc: 0.9487 - val_loss: 0.1580 - val_acc: 0.9400\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1686 - acc: 0.9487 - val_loss: 0.1577 - val_acc: 0.9400\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1684 - acc: 0.9487 - val_loss: 0.1574 - val_acc: 0.9400\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1679 - acc: 0.9487 - val_loss: 0.1571 - val_acc: 0.9400\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 161us/step - loss: 0.1676 - acc: 0.9487 - val_loss: 0.1568 - val_acc: 0.9400\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1672 - acc: 0.9475 - val_loss: 0.1566 - val_acc: 0.9400\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1669 - acc: 0.9525 - val_loss: 0.1563 - val_acc: 0.9400\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.1665 - acc: 0.9487 - val_loss: 0.1560 - val_acc: 0.9400\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1661 - acc: 0.9513 - val_loss: 0.1558 - val_acc: 0.9400\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1658 - acc: 0.9513 - val_loss: 0.1555 - val_acc: 0.9400\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1654 - acc: 0.9525 - val_loss: 0.1553 - val_acc: 0.9400\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1652 - acc: 0.9525 - val_loss: 0.1551 - val_acc: 0.9400\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1648 - acc: 0.9525 - val_loss: 0.1549 - val_acc: 0.9400\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 158us/step - loss: 0.1645 - acc: 0.9525 - val_loss: 0.1547 - val_acc: 0.9400\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1642 - acc: 0.9525 - val_loss: 0.1545 - val_acc: 0.9400\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1639 - acc: 0.9525 - val_loss: 0.1543 - val_acc: 0.9400\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1636 - acc: 0.9525 - val_loss: 0.1541 - val_acc: 0.9400\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.1633 - acc: 0.9525 - val_loss: 0.1539 - val_acc: 0.9400\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1630 - acc: 0.9525 - val_loss: 0.1537 - val_acc: 0.9400\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.1626 - acc: 0.9525 - val_loss: 0.1535 - val_acc: 0.9400\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1623 - acc: 0.9537 - val_loss: 0.1533 - val_acc: 0.9400\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1621 - acc: 0.9525 - val_loss: 0.1531 - val_acc: 0.9400\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.1618 - acc: 0.9525 - val_loss: 0.1530 - val_acc: 0.9400\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1614 - acc: 0.9537 - val_loss: 0.1528 - val_acc: 0.9400\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1612 - acc: 0.9537 - val_loss: 0.1526 - val_acc: 0.9400\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1609 - acc: 0.9537 - val_loss: 0.1525 - val_acc: 0.9400\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1606 - acc: 0.9537 - val_loss: 0.1523 - val_acc: 0.9400\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1603 - acc: 0.9537 - val_loss: 0.1521 - val_acc: 0.9400\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.1600 - acc: 0.9537 - val_loss: 0.1519 - val_acc: 0.9400\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1598 - acc: 0.9537 - val_loss: 0.1518 - val_acc: 0.9400\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1594 - acc: 0.9537 - val_loss: 0.1516 - val_acc: 0.9400\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1592 - acc: 0.9537 - val_loss: 0.1515 - val_acc: 0.9400\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 156us/step - loss: 0.1589 - acc: 0.9550 - val_loss: 0.1514 - val_acc: 0.9400\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1586 - acc: 0.9550 - val_loss: 0.1512 - val_acc: 0.9400\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1583 - acc: 0.9537 - val_loss: 0.1511 - val_acc: 0.9400\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1581 - acc: 0.9563 - val_loss: 0.1510 - val_acc: 0.9400\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1578 - acc: 0.9563 - val_loss: 0.1509 - val_acc: 0.9400\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1576 - acc: 0.9563 - val_loss: 0.1507 - val_acc: 0.9400\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1572 - acc: 0.9563 - val_loss: 0.1506 - val_acc: 0.9400\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1570 - acc: 0.9563 - val_loss: 0.1505 - val_acc: 0.9400\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1568 - acc: 0.9575 - val_loss: 0.1504 - val_acc: 0.9400\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.1565 - acc: 0.9563 - val_loss: 0.1502 - val_acc: 0.9400\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1563 - acc: 0.9563 - val_loss: 0.1502 - val_acc: 0.9400\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.1561 - acc: 0.9575 - val_loss: 0.1501 - val_acc: 0.9400\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1558 - acc: 0.9563 - val_loss: 0.1500 - val_acc: 0.9400\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1555 - acc: 0.9563 - val_loss: 0.1499 - val_acc: 0.9400\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1554 - acc: 0.9563 - val_loss: 0.1498 - val_acc: 0.9400\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1550 - acc: 0.9563 - val_loss: 0.1496 - val_acc: 0.9400\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1548 - acc: 0.9563 - val_loss: 0.1495 - val_acc: 0.9400\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1546 - acc: 0.9575 - val_loss: 0.1495 - val_acc: 0.9400\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 153us/step - loss: 0.1543 - acc: 0.9563 - val_loss: 0.1494 - val_acc: 0.9400\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1541 - acc: 0.9563 - val_loss: 0.1493 - val_acc: 0.9400\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.1539 - acc: 0.9575 - val_loss: 0.1492 - val_acc: 0.9400\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1536 - acc: 0.9563 - val_loss: 0.1491 - val_acc: 0.9400\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1535 - acc: 0.9575 - val_loss: 0.1490 - val_acc: 0.9400\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1532 - acc: 0.9575 - val_loss: 0.1488 - val_acc: 0.9400\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1529 - acc: 0.9563 - val_loss: 0.1487 - val_acc: 0.9400\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1528 - acc: 0.9575 - val_loss: 0.1487 - val_acc: 0.9400\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1525 - acc: 0.9563 - val_loss: 0.1486 - val_acc: 0.9400\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1523 - acc: 0.9587 - val_loss: 0.1485 - val_acc: 0.9400\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 159us/step - loss: 0.1521 - acc: 0.9575 - val_loss: 0.1484 - val_acc: 0.9400\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1519 - acc: 0.9587 - val_loss: 0.1483 - val_acc: 0.9400\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1517 - acc: 0.9587 - val_loss: 0.1482 - val_acc: 0.9400\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1514 - acc: 0.9587 - val_loss: 0.1481 - val_acc: 0.9400\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1512 - acc: 0.9575 - val_loss: 0.1480 - val_acc: 0.9400\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1510 - acc: 0.9575 - val_loss: 0.1479 - val_acc: 0.9400\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1508 - acc: 0.9575 - val_loss: 0.1478 - val_acc: 0.9400\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1506 - acc: 0.9575 - val_loss: 0.1477 - val_acc: 0.9400\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1504 - acc: 0.9563 - val_loss: 0.1477 - val_acc: 0.9400\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.1501 - acc: 0.9575 - val_loss: 0.1476 - val_acc: 0.9400\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1499 - acc: 0.9575 - val_loss: 0.1475 - val_acc: 0.9400\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1497 - acc: 0.9575 - val_loss: 0.1474 - val_acc: 0.9400\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1496 - acc: 0.9575 - val_loss: 0.1474 - val_acc: 0.9400\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1493 - acc: 0.9575 - val_loss: 0.1473 - val_acc: 0.9400\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1492 - acc: 0.9563 - val_loss: 0.1472 - val_acc: 0.9400\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1488 - acc: 0.9575 - val_loss: 0.1471 - val_acc: 0.9400\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1487 - acc: 0.9575 - val_loss: 0.1471 - val_acc: 0.9400\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.1486 - acc: 0.9575 - val_loss: 0.1470 - val_acc: 0.9400\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.1483 - acc: 0.9575 - val_loss: 0.1470 - val_acc: 0.9400\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.1481 - acc: 0.9575 - val_loss: 0.1469 - val_acc: 0.9400\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1479 - acc: 0.9587 - val_loss: 0.1468 - val_acc: 0.9400\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1477 - acc: 0.9575 - val_loss: 0.1467 - val_acc: 0.9400\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1475 - acc: 0.9563 - val_loss: 0.1466 - val_acc: 0.9400\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1473 - acc: 0.9575 - val_loss: 0.1466 - val_acc: 0.9400\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.1471 - acc: 0.9575 - val_loss: 0.1464 - val_acc: 0.9400\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.1469 - acc: 0.9563 - val_loss: 0.1464 - val_acc: 0.9400\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1467 - acc: 0.9575 - val_loss: 0.1463 - val_acc: 0.9400\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1465 - acc: 0.9575 - val_loss: 0.1462 - val_acc: 0.9400\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.1463 - acc: 0.9563 - val_loss: 0.1462 - val_acc: 0.9400\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1461 - acc: 0.9575 - val_loss: 0.1461 - val_acc: 0.9400\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1459 - acc: 0.9587 - val_loss: 0.1460 - val_acc: 0.9400\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1458 - acc: 0.9600 - val_loss: 0.1460 - val_acc: 0.9400\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1456 - acc: 0.9575 - val_loss: 0.1459 - val_acc: 0.9400\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1454 - acc: 0.9575 - val_loss: 0.1458 - val_acc: 0.9400\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.1452 - acc: 0.9587 - val_loss: 0.1457 - val_acc: 0.9400\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.1450 - acc: 0.9587 - val_loss: 0.1457 - val_acc: 0.9400\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1448 - acc: 0.9600 - val_loss: 0.1456 - val_acc: 0.9400\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 154us/step - loss: 0.1447 - acc: 0.9575 - val_loss: 0.1455 - val_acc: 0.9400\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1444 - acc: 0.9587 - val_loss: 0.1455 - val_acc: 0.9450\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1443 - acc: 0.9587 - val_loss: 0.1454 - val_acc: 0.9450\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1442 - acc: 0.9575 - val_loss: 0.1454 - val_acc: 0.9450\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1439 - acc: 0.9575 - val_loss: 0.1453 - val_acc: 0.9450\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1437 - acc: 0.9575 - val_loss: 0.1452 - val_acc: 0.9450\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1435 - acc: 0.9587 - val_loss: 0.1452 - val_acc: 0.9450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe62ba46080>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "keras.backend.clear_session()\n",
    "model=Sequential()\n",
    "model.add(Dense(32,activation='relu',input_dim=20))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,batch_size=32,epochs=200,verbose=1,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLkyFB2G7xoJ"
   },
   "source": [
    "If we ues the activation fonction \"ReLu\", the speed of training is faster, we only need 15 epochs to raise the accuracy over 90% while the former methode needs 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7253
    },
    "colab_type": "code",
    "id": "OsCgy5sG8VKY",
    "outputId": "74648062-8d3b-4ee3-8a0b-91a6e776c176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 0s 326us/step - loss: 0.7476 - acc: 0.5337 - val_loss: 0.7692 - val_acc: 0.5250\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.7465 - acc: 0.5363 - val_loss: 0.7682 - val_acc: 0.5250\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.7453 - acc: 0.5375 - val_loss: 0.7671 - val_acc: 0.5250\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.7442 - acc: 0.5388 - val_loss: 0.7660 - val_acc: 0.5250\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.7431 - acc: 0.5388 - val_loss: 0.7650 - val_acc: 0.5300\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 124us/step - loss: 0.7419 - acc: 0.5388 - val_loss: 0.7639 - val_acc: 0.5300\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 124us/step - loss: 0.7408 - acc: 0.5400 - val_loss: 0.7629 - val_acc: 0.5300\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.7397 - acc: 0.5400 - val_loss: 0.7618 - val_acc: 0.5300\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.7386 - acc: 0.5400 - val_loss: 0.7608 - val_acc: 0.5300\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.7375 - acc: 0.5400 - val_loss: 0.7598 - val_acc: 0.5300\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 124us/step - loss: 0.7364 - acc: 0.5400 - val_loss: 0.7587 - val_acc: 0.5300\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.7353 - acc: 0.5400 - val_loss: 0.7577 - val_acc: 0.5300\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.7342 - acc: 0.5413 - val_loss: 0.7567 - val_acc: 0.5300\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.7331 - acc: 0.5413 - val_loss: 0.7557 - val_acc: 0.5300\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 124us/step - loss: 0.7320 - acc: 0.5413 - val_loss: 0.7547 - val_acc: 0.5300\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 121us/step - loss: 0.7310 - acc: 0.5413 - val_loss: 0.7537 - val_acc: 0.5300\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 128us/step - loss: 0.7299 - acc: 0.5425 - val_loss: 0.7527 - val_acc: 0.5300\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 128us/step - loss: 0.7288 - acc: 0.5425 - val_loss: 0.7517 - val_acc: 0.5350\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.7278 - acc: 0.5463 - val_loss: 0.7507 - val_acc: 0.5350\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.7267 - acc: 0.5487 - val_loss: 0.7497 - val_acc: 0.5350\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.7256 - acc: 0.5500 - val_loss: 0.7487 - val_acc: 0.5350\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.7246 - acc: 0.5500 - val_loss: 0.7477 - val_acc: 0.5350\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.7235 - acc: 0.5513 - val_loss: 0.7467 - val_acc: 0.5350\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 162us/step - loss: 0.7225 - acc: 0.5525 - val_loss: 0.7457 - val_acc: 0.5350\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.7215 - acc: 0.5550 - val_loss: 0.7448 - val_acc: 0.5350\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.7204 - acc: 0.5550 - val_loss: 0.7438 - val_acc: 0.5350\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.7194 - acc: 0.5550 - val_loss: 0.7428 - val_acc: 0.5350\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.7184 - acc: 0.5550 - val_loss: 0.7419 - val_acc: 0.5350\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.7173 - acc: 0.5575 - val_loss: 0.7409 - val_acc: 0.5350\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.7163 - acc: 0.5600 - val_loss: 0.7400 - val_acc: 0.5350\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.7153 - acc: 0.5613 - val_loss: 0.7390 - val_acc: 0.5350\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.7143 - acc: 0.5613 - val_loss: 0.7381 - val_acc: 0.5350\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.7133 - acc: 0.5613 - val_loss: 0.7371 - val_acc: 0.5400\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.7123 - acc: 0.5613 - val_loss: 0.7362 - val_acc: 0.5400\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.7113 - acc: 0.5613 - val_loss: 0.7353 - val_acc: 0.5400\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.7103 - acc: 0.5613 - val_loss: 0.7343 - val_acc: 0.5400\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.7093 - acc: 0.5625 - val_loss: 0.7334 - val_acc: 0.5400\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.7084 - acc: 0.5613 - val_loss: 0.7325 - val_acc: 0.5400\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.7074 - acc: 0.5613 - val_loss: 0.7316 - val_acc: 0.5400\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.7064 - acc: 0.5600 - val_loss: 0.7306 - val_acc: 0.5400\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.7054 - acc: 0.5600 - val_loss: 0.7297 - val_acc: 0.5400\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.7045 - acc: 0.5625 - val_loss: 0.7288 - val_acc: 0.5400\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.7035 - acc: 0.5625 - val_loss: 0.7279 - val_acc: 0.5400\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.7025 - acc: 0.5625 - val_loss: 0.7270 - val_acc: 0.5400\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.7016 - acc: 0.5625 - val_loss: 0.7261 - val_acc: 0.5400\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.7006 - acc: 0.5650 - val_loss: 0.7252 - val_acc: 0.5400\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.6997 - acc: 0.5663 - val_loss: 0.7243 - val_acc: 0.5400\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6987 - acc: 0.5687 - val_loss: 0.7234 - val_acc: 0.5400\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6978 - acc: 0.5700 - val_loss: 0.7226 - val_acc: 0.5400\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.6969 - acc: 0.5725 - val_loss: 0.7217 - val_acc: 0.5400\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.6959 - acc: 0.5725 - val_loss: 0.7208 - val_acc: 0.5400\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.6950 - acc: 0.5737 - val_loss: 0.7199 - val_acc: 0.5400\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.6941 - acc: 0.5737 - val_loss: 0.7191 - val_acc: 0.5400\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6931 - acc: 0.5737 - val_loss: 0.7182 - val_acc: 0.5400\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6922 - acc: 0.5737 - val_loss: 0.7173 - val_acc: 0.5400\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6913 - acc: 0.5737 - val_loss: 0.7165 - val_acc: 0.5400\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6904 - acc: 0.5737 - val_loss: 0.7156 - val_acc: 0.5450\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6895 - acc: 0.5737 - val_loss: 0.7148 - val_acc: 0.5450\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6886 - acc: 0.5737 - val_loss: 0.7139 - val_acc: 0.5450\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6877 - acc: 0.5737 - val_loss: 0.7131 - val_acc: 0.5450\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.6868 - acc: 0.5737 - val_loss: 0.7122 - val_acc: 0.5450\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.6859 - acc: 0.5763 - val_loss: 0.7114 - val_acc: 0.5500\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6850 - acc: 0.5775 - val_loss: 0.7106 - val_acc: 0.5550\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6841 - acc: 0.5787 - val_loss: 0.7097 - val_acc: 0.5550\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6832 - acc: 0.5787 - val_loss: 0.7089 - val_acc: 0.5550\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.6823 - acc: 0.5787 - val_loss: 0.7081 - val_acc: 0.5550\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.6815 - acc: 0.5787 - val_loss: 0.7072 - val_acc: 0.5550\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.6806 - acc: 0.5800 - val_loss: 0.7064 - val_acc: 0.5600\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6797 - acc: 0.5862 - val_loss: 0.7056 - val_acc: 0.5600\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.6789 - acc: 0.5862 - val_loss: 0.7048 - val_acc: 0.5600\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.6780 - acc: 0.5862 - val_loss: 0.7040 - val_acc: 0.5600\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6771 - acc: 0.5875 - val_loss: 0.7032 - val_acc: 0.5600\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.7024 - val_acc: 0.5600\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6754 - acc: 0.5887 - val_loss: 0.7016 - val_acc: 0.5600\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6746 - acc: 0.5900 - val_loss: 0.7008 - val_acc: 0.5600\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6737 - acc: 0.5887 - val_loss: 0.7000 - val_acc: 0.5600\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6729 - acc: 0.5887 - val_loss: 0.6992 - val_acc: 0.5600\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.6720 - acc: 0.5913 - val_loss: 0.6984 - val_acc: 0.5650\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.6712 - acc: 0.5938 - val_loss: 0.6976 - val_acc: 0.5650\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 161us/step - loss: 0.6704 - acc: 0.5950 - val_loss: 0.6968 - val_acc: 0.5650\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.6695 - acc: 0.5950 - val_loss: 0.6960 - val_acc: 0.5650\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6687 - acc: 0.5975 - val_loss: 0.6952 - val_acc: 0.5650\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.6679 - acc: 0.5975 - val_loss: 0.6945 - val_acc: 0.5650\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6671 - acc: 0.5988 - val_loss: 0.6937 - val_acc: 0.5700\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.6662 - acc: 0.6000 - val_loss: 0.6929 - val_acc: 0.5700\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.6654 - acc: 0.6000 - val_loss: 0.6921 - val_acc: 0.5700\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.6646 - acc: 0.6000 - val_loss: 0.6914 - val_acc: 0.5700\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6638 - acc: 0.6013 - val_loss: 0.6906 - val_acc: 0.5700\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 162us/step - loss: 0.6630 - acc: 0.6013 - val_loss: 0.6898 - val_acc: 0.5700\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6622 - acc: 0.6025 - val_loss: 0.6891 - val_acc: 0.5750\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.6614 - acc: 0.6038 - val_loss: 0.6883 - val_acc: 0.5750\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.6606 - acc: 0.6038 - val_loss: 0.6876 - val_acc: 0.5750\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6598 - acc: 0.6038 - val_loss: 0.6868 - val_acc: 0.5750\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6590 - acc: 0.6025 - val_loss: 0.6861 - val_acc: 0.5750\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6582 - acc: 0.6038 - val_loss: 0.6853 - val_acc: 0.5750\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6574 - acc: 0.6050 - val_loss: 0.6846 - val_acc: 0.5800\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6567 - acc: 0.6050 - val_loss: 0.6839 - val_acc: 0.5850\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 156us/step - loss: 0.6559 - acc: 0.6050 - val_loss: 0.6831 - val_acc: 0.5850\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6551 - acc: 0.6050 - val_loss: 0.6824 - val_acc: 0.5850\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.6543 - acc: 0.6062 - val_loss: 0.6816 - val_acc: 0.5850\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6536 - acc: 0.6088 - val_loss: 0.6809 - val_acc: 0.5950\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.6528 - acc: 0.6088 - val_loss: 0.6802 - val_acc: 0.5950\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6520 - acc: 0.6088 - val_loss: 0.6795 - val_acc: 0.5950\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.6513 - acc: 0.6088 - val_loss: 0.6787 - val_acc: 0.5950\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6505 - acc: 0.6075 - val_loss: 0.6780 - val_acc: 0.5950\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.6498 - acc: 0.6088 - val_loss: 0.6773 - val_acc: 0.5950\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.6490 - acc: 0.6088 - val_loss: 0.6766 - val_acc: 0.5950\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.6483 - acc: 0.6088 - val_loss: 0.6759 - val_acc: 0.5950\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.6475 - acc: 0.6088 - val_loss: 0.6752 - val_acc: 0.5950\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.6468 - acc: 0.6075 - val_loss: 0.6745 - val_acc: 0.6000\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.6460 - acc: 0.6075 - val_loss: 0.6738 - val_acc: 0.6000\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6453 - acc: 0.6100 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6445 - acc: 0.6112 - val_loss: 0.6724 - val_acc: 0.6000\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6438 - acc: 0.6112 - val_loss: 0.6717 - val_acc: 0.6000\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.6431 - acc: 0.6112 - val_loss: 0.6710 - val_acc: 0.6000\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6423 - acc: 0.6163 - val_loss: 0.6703 - val_acc: 0.6000\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.6416 - acc: 0.6175 - val_loss: 0.6696 - val_acc: 0.6000\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.6409 - acc: 0.6187 - val_loss: 0.6689 - val_acc: 0.6050\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6402 - acc: 0.6200 - val_loss: 0.6682 - val_acc: 0.6050\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.6394 - acc: 0.6225 - val_loss: 0.6675 - val_acc: 0.6050\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.6387 - acc: 0.6225 - val_loss: 0.6668 - val_acc: 0.6050\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.6380 - acc: 0.6238 - val_loss: 0.6662 - val_acc: 0.6050\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.6373 - acc: 0.6250 - val_loss: 0.6655 - val_acc: 0.6050\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.6366 - acc: 0.6262 - val_loss: 0.6648 - val_acc: 0.6050\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6359 - acc: 0.6262 - val_loss: 0.6641 - val_acc: 0.6050\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.6352 - acc: 0.6262 - val_loss: 0.6635 - val_acc: 0.6100\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6345 - acc: 0.6250 - val_loss: 0.6628 - val_acc: 0.6100\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6338 - acc: 0.6275 - val_loss: 0.6621 - val_acc: 0.6100\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6331 - acc: 0.6288 - val_loss: 0.6615 - val_acc: 0.6100\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6324 - acc: 0.6288 - val_loss: 0.6608 - val_acc: 0.6100\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6317 - acc: 0.6288 - val_loss: 0.6602 - val_acc: 0.6100\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6310 - acc: 0.6288 - val_loss: 0.6595 - val_acc: 0.6150\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6303 - acc: 0.6300 - val_loss: 0.6588 - val_acc: 0.6150\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.6296 - acc: 0.6313 - val_loss: 0.6582 - val_acc: 0.6150\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.6289 - acc: 0.6313 - val_loss: 0.6575 - val_acc: 0.6150\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 153us/step - loss: 0.6283 - acc: 0.6313 - val_loss: 0.6569 - val_acc: 0.6150\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6276 - acc: 0.6313 - val_loss: 0.6563 - val_acc: 0.6150\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.6269 - acc: 0.6313 - val_loss: 0.6556 - val_acc: 0.6150\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6262 - acc: 0.6313 - val_loss: 0.6550 - val_acc: 0.6150\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6256 - acc: 0.6337 - val_loss: 0.6543 - val_acc: 0.6150\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6249 - acc: 0.6337 - val_loss: 0.6537 - val_acc: 0.6150\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 128us/step - loss: 0.6242 - acc: 0.6337 - val_loss: 0.6531 - val_acc: 0.6150\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6236 - acc: 0.6337 - val_loss: 0.6524 - val_acc: 0.6150\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6229 - acc: 0.6337 - val_loss: 0.6518 - val_acc: 0.6150\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.6222 - acc: 0.6350 - val_loss: 0.6512 - val_acc: 0.6150\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.6216 - acc: 0.6375 - val_loss: 0.6505 - val_acc: 0.6150\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6209 - acc: 0.6375 - val_loss: 0.6499 - val_acc: 0.6200\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.6203 - acc: 0.6375 - val_loss: 0.6493 - val_acc: 0.6150\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6196 - acc: 0.6375 - val_loss: 0.6487 - val_acc: 0.6100\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.6190 - acc: 0.6362 - val_loss: 0.6481 - val_acc: 0.6100\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.6183 - acc: 0.6362 - val_loss: 0.6474 - val_acc: 0.6100\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.6177 - acc: 0.6400 - val_loss: 0.6468 - val_acc: 0.6100\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6170 - acc: 0.6400 - val_loss: 0.6462 - val_acc: 0.6100\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.6164 - acc: 0.6400 - val_loss: 0.6456 - val_acc: 0.6100\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6158 - acc: 0.6425 - val_loss: 0.6450 - val_acc: 0.6100\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.6151 - acc: 0.6425 - val_loss: 0.6444 - val_acc: 0.6100\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.6145 - acc: 0.6475 - val_loss: 0.6438 - val_acc: 0.6100\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6139 - acc: 0.6500 - val_loss: 0.6432 - val_acc: 0.6150\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.6132 - acc: 0.6500 - val_loss: 0.6426 - val_acc: 0.6150\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.6126 - acc: 0.6512 - val_loss: 0.6420 - val_acc: 0.6150\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.6120 - acc: 0.6537 - val_loss: 0.6414 - val_acc: 0.6150\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6113 - acc: 0.6550 - val_loss: 0.6408 - val_acc: 0.6150\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6107 - acc: 0.6550 - val_loss: 0.6402 - val_acc: 0.6200\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.6101 - acc: 0.6550 - val_loss: 0.6396 - val_acc: 0.6200\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6095 - acc: 0.6550 - val_loss: 0.6390 - val_acc: 0.6200\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.6089 - acc: 0.6562 - val_loss: 0.6384 - val_acc: 0.6200\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.6082 - acc: 0.6588 - val_loss: 0.6378 - val_acc: 0.6200\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6076 - acc: 0.6613 - val_loss: 0.6372 - val_acc: 0.6200\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.6070 - acc: 0.6613 - val_loss: 0.6367 - val_acc: 0.6200\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6064 - acc: 0.6625 - val_loss: 0.6361 - val_acc: 0.6200\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.6058 - acc: 0.6625 - val_loss: 0.6355 - val_acc: 0.6200\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.6052 - acc: 0.6625 - val_loss: 0.6349 - val_acc: 0.6250\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 163us/step - loss: 0.6046 - acc: 0.6638 - val_loss: 0.6343 - val_acc: 0.6300\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6040 - acc: 0.6638 - val_loss: 0.6338 - val_acc: 0.6300\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6034 - acc: 0.6650 - val_loss: 0.6332 - val_acc: 0.6300\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.6028 - acc: 0.6675 - val_loss: 0.6326 - val_acc: 0.6300\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.6022 - acc: 0.6675 - val_loss: 0.6320 - val_acc: 0.6300\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.6016 - acc: 0.6700 - val_loss: 0.6315 - val_acc: 0.6300\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.6010 - acc: 0.6700 - val_loss: 0.6309 - val_acc: 0.6400\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.6004 - acc: 0.6700 - val_loss: 0.6303 - val_acc: 0.6400\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 156us/step - loss: 0.5998 - acc: 0.6687 - val_loss: 0.6298 - val_acc: 0.6400\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.5992 - acc: 0.6687 - val_loss: 0.6292 - val_acc: 0.6400\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.5986 - acc: 0.6700 - val_loss: 0.6286 - val_acc: 0.6400\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.5981 - acc: 0.6700 - val_loss: 0.6281 - val_acc: 0.6400\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.5975 - acc: 0.6713 - val_loss: 0.6275 - val_acc: 0.6400\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.5969 - acc: 0.6700 - val_loss: 0.6270 - val_acc: 0.6400\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.5963 - acc: 0.6700 - val_loss: 0.6264 - val_acc: 0.6400\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.5957 - acc: 0.6700 - val_loss: 0.6259 - val_acc: 0.6400\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.5952 - acc: 0.6713 - val_loss: 0.6253 - val_acc: 0.6450\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.5946 - acc: 0.6713 - val_loss: 0.6248 - val_acc: 0.6450\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 161us/step - loss: 0.5940 - acc: 0.6725 - val_loss: 0.6242 - val_acc: 0.6450\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.5934 - acc: 0.6725 - val_loss: 0.6237 - val_acc: 0.6450\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.5929 - acc: 0.6737 - val_loss: 0.6231 - val_acc: 0.6450\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.5923 - acc: 0.6775 - val_loss: 0.6226 - val_acc: 0.6400\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.5917 - acc: 0.6775 - val_loss: 0.6220 - val_acc: 0.6400\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.5912 - acc: 0.6775 - val_loss: 0.6215 - val_acc: 0.6450\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.5906 - acc: 0.6788 - val_loss: 0.6209 - val_acc: 0.6450\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.5901 - acc: 0.6788 - val_loss: 0.6204 - val_acc: 0.6450\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.5895 - acc: 0.6800 - val_loss: 0.6199 - val_acc: 0.6500\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.5889 - acc: 0.6800 - val_loss: 0.6193 - val_acc: 0.6500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe62bf6e7f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "keras.backend.clear_session()\n",
    "model=Sequential()\n",
    "model.add(Dense(32,activation='relu',input_dim=20))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "sgd = optimizers.SGD(lr=0.0001)\n",
    "model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,batch_size=32,epochs=200,verbose=1,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-C1talq8sIX"
   },
   "source": [
    "When we set the learning rate as 0.0001, Its speech become very slow where we just reach 68% accuracy after 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7253
    },
    "colab_type": "code",
    "id": "UtZoe7yG9DdW",
    "outputId": "fbe8b6c1-65a4-40b0-c0b2-c362364c9459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 0s 325us/step - loss: 0.5015 - acc: 0.7762 - val_loss: 0.3454 - val_acc: 0.9050\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 123us/step - loss: 0.3019 - acc: 0.9125 - val_loss: 0.2530 - val_acc: 0.9250\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.2469 - acc: 0.9300 - val_loss: 0.2173 - val_acc: 0.9400\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 153us/step - loss: 0.2223 - acc: 0.9363 - val_loss: 0.1991 - val_acc: 0.9400\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.2095 - acc: 0.9337 - val_loss: 0.1872 - val_acc: 0.9450\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.1999 - acc: 0.9337 - val_loss: 0.1810 - val_acc: 0.9450\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1934 - acc: 0.9413 - val_loss: 0.1777 - val_acc: 0.9400\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.1874 - acc: 0.9425 - val_loss: 0.1752 - val_acc: 0.9450\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1836 - acc: 0.9413 - val_loss: 0.1724 - val_acc: 0.9400\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.1802 - acc: 0.9413 - val_loss: 0.1689 - val_acc: 0.9400\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1755 - acc: 0.9475 - val_loss: 0.1673 - val_acc: 0.9400\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1732 - acc: 0.9450 - val_loss: 0.1655 - val_acc: 0.9450\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1703 - acc: 0.9487 - val_loss: 0.1648 - val_acc: 0.9400\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1663 - acc: 0.9475 - val_loss: 0.1638 - val_acc: 0.9350\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.1641 - acc: 0.9513 - val_loss: 0.1639 - val_acc: 0.9400\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 127us/step - loss: 0.1617 - acc: 0.9513 - val_loss: 0.1634 - val_acc: 0.9400\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.1589 - acc: 0.9525 - val_loss: 0.1631 - val_acc: 0.9400\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.1562 - acc: 0.9450 - val_loss: 0.1611 - val_acc: 0.9400\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 127us/step - loss: 0.1538 - acc: 0.9550 - val_loss: 0.1593 - val_acc: 0.9400\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1514 - acc: 0.9587 - val_loss: 0.1622 - val_acc: 0.9250\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1501 - acc: 0.9513 - val_loss: 0.1587 - val_acc: 0.9350\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1481 - acc: 0.9600 - val_loss: 0.1625 - val_acc: 0.9300\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.1458 - acc: 0.9587 - val_loss: 0.1597 - val_acc: 0.9250\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1450 - acc: 0.9575 - val_loss: 0.1607 - val_acc: 0.9250\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1424 - acc: 0.9575 - val_loss: 0.1620 - val_acc: 0.9350\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1409 - acc: 0.9587 - val_loss: 0.1664 - val_acc: 0.9250\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1394 - acc: 0.9625 - val_loss: 0.1623 - val_acc: 0.9250\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1383 - acc: 0.9587 - val_loss: 0.1633 - val_acc: 0.9250\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1357 - acc: 0.9587 - val_loss: 0.1617 - val_acc: 0.9300\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1346 - acc: 0.9612 - val_loss: 0.1648 - val_acc: 0.9250\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1319 - acc: 0.9625 - val_loss: 0.1642 - val_acc: 0.9200\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.1309 - acc: 0.9637 - val_loss: 0.1650 - val_acc: 0.9300\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1296 - acc: 0.9600 - val_loss: 0.1699 - val_acc: 0.9250\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.1278 - acc: 0.9650 - val_loss: 0.1654 - val_acc: 0.9300\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1264 - acc: 0.9612 - val_loss: 0.1649 - val_acc: 0.9300\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1249 - acc: 0.9662 - val_loss: 0.1659 - val_acc: 0.9350\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1241 - acc: 0.9637 - val_loss: 0.1722 - val_acc: 0.9250\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1229 - acc: 0.9637 - val_loss: 0.1720 - val_acc: 0.9250\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1214 - acc: 0.9637 - val_loss: 0.1708 - val_acc: 0.9250\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1205 - acc: 0.9625 - val_loss: 0.1707 - val_acc: 0.9250\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1186 - acc: 0.9713 - val_loss: 0.1671 - val_acc: 0.9300\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 161us/step - loss: 0.1180 - acc: 0.9637 - val_loss: 0.1700 - val_acc: 0.9250\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1157 - acc: 0.9688 - val_loss: 0.1685 - val_acc: 0.9300\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.1142 - acc: 0.9688 - val_loss: 0.1693 - val_acc: 0.9300\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.1135 - acc: 0.9675 - val_loss: 0.1709 - val_acc: 0.9250\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1118 - acc: 0.9700 - val_loss: 0.1709 - val_acc: 0.9250\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.1114 - acc: 0.9675 - val_loss: 0.1711 - val_acc: 0.9250\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1099 - acc: 0.9700 - val_loss: 0.1753 - val_acc: 0.9200\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1084 - acc: 0.9700 - val_loss: 0.1757 - val_acc: 0.9250\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1076 - acc: 0.9700 - val_loss: 0.1736 - val_acc: 0.9250\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 153us/step - loss: 0.1060 - acc: 0.9688 - val_loss: 0.1775 - val_acc: 0.9200\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1057 - acc: 0.9675 - val_loss: 0.1734 - val_acc: 0.9250\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1032 - acc: 0.9725 - val_loss: 0.1730 - val_acc: 0.9250\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1037 - acc: 0.9700 - val_loss: 0.1771 - val_acc: 0.9200\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.1020 - acc: 0.9738 - val_loss: 0.1794 - val_acc: 0.9200\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1009 - acc: 0.9725 - val_loss: 0.1782 - val_acc: 0.9200\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1001 - acc: 0.9763 - val_loss: 0.1802 - val_acc: 0.9150\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0984 - acc: 0.9713 - val_loss: 0.1794 - val_acc: 0.9150\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0977 - acc: 0.9725 - val_loss: 0.1814 - val_acc: 0.9150\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.0969 - acc: 0.9738 - val_loss: 0.1860 - val_acc: 0.9100\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0957 - acc: 0.9725 - val_loss: 0.1842 - val_acc: 0.9100\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0952 - acc: 0.9738 - val_loss: 0.1854 - val_acc: 0.9100\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.0937 - acc: 0.9775 - val_loss: 0.1841 - val_acc: 0.9100\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.0927 - acc: 0.9725 - val_loss: 0.1885 - val_acc: 0.9100\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0930 - acc: 0.9725 - val_loss: 0.1896 - val_acc: 0.9100\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.0913 - acc: 0.9750 - val_loss: 0.1852 - val_acc: 0.9100\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0894 - acc: 0.9763 - val_loss: 0.1873 - val_acc: 0.9100\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0898 - acc: 0.9788 - val_loss: 0.1889 - val_acc: 0.9100\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0890 - acc: 0.9775 - val_loss: 0.1879 - val_acc: 0.9100\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 157us/step - loss: 0.0877 - acc: 0.9775 - val_loss: 0.1886 - val_acc: 0.9100\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.0858 - acc: 0.9825 - val_loss: 0.1904 - val_acc: 0.9150\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0860 - acc: 0.9788 - val_loss: 0.1941 - val_acc: 0.9050\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0845 - acc: 0.9775 - val_loss: 0.1964 - val_acc: 0.9050\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0843 - acc: 0.9800 - val_loss: 0.1969 - val_acc: 0.9050\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0833 - acc: 0.9788 - val_loss: 0.2001 - val_acc: 0.9050\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.0823 - acc: 0.9788 - val_loss: 0.1974 - val_acc: 0.9150\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.0810 - acc: 0.9788 - val_loss: 0.1989 - val_acc: 0.9050\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0803 - acc: 0.9800 - val_loss: 0.1999 - val_acc: 0.9150\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0797 - acc: 0.9812 - val_loss: 0.2009 - val_acc: 0.9100\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.0788 - acc: 0.9812 - val_loss: 0.2024 - val_acc: 0.9150\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.0781 - acc: 0.9838 - val_loss: 0.2037 - val_acc: 0.9150\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0775 - acc: 0.9825 - val_loss: 0.2059 - val_acc: 0.9100\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0764 - acc: 0.9812 - val_loss: 0.2085 - val_acc: 0.9100\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.0757 - acc: 0.9838 - val_loss: 0.2084 - val_acc: 0.9100\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0745 - acc: 0.9850 - val_loss: 0.2099 - val_acc: 0.9100\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0743 - acc: 0.9862 - val_loss: 0.2101 - val_acc: 0.9100\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0727 - acc: 0.9850 - val_loss: 0.2129 - val_acc: 0.9100\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.0726 - acc: 0.9850 - val_loss: 0.2081 - val_acc: 0.9150\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.0719 - acc: 0.9850 - val_loss: 0.2088 - val_acc: 0.9150\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0703 - acc: 0.9862 - val_loss: 0.2084 - val_acc: 0.9150\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0701 - acc: 0.9850 - val_loss: 0.2125 - val_acc: 0.9100\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0688 - acc: 0.9850 - val_loss: 0.2158 - val_acc: 0.9100\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.0684 - acc: 0.9862 - val_loss: 0.2118 - val_acc: 0.9100\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0682 - acc: 0.9875 - val_loss: 0.2129 - val_acc: 0.9150\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.0667 - acc: 0.9888 - val_loss: 0.2180 - val_acc: 0.9100\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0660 - acc: 0.9888 - val_loss: 0.2174 - val_acc: 0.9100\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.0657 - acc: 0.9875 - val_loss: 0.2231 - val_acc: 0.9100\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.0651 - acc: 0.9875 - val_loss: 0.2185 - val_acc: 0.9100\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0646 - acc: 0.9862 - val_loss: 0.2206 - val_acc: 0.9100\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0629 - acc: 0.9888 - val_loss: 0.2208 - val_acc: 0.9100\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0625 - acc: 0.9875 - val_loss: 0.2253 - val_acc: 0.9100\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.0620 - acc: 0.9888 - val_loss: 0.2258 - val_acc: 0.9100\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0608 - acc: 0.9888 - val_loss: 0.2300 - val_acc: 0.9100\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0607 - acc: 0.9888 - val_loss: 0.2306 - val_acc: 0.9100\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.0600 - acc: 0.9888 - val_loss: 0.2291 - val_acc: 0.9100\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.0592 - acc: 0.9875 - val_loss: 0.2338 - val_acc: 0.9100\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.0581 - acc: 0.9900 - val_loss: 0.2344 - val_acc: 0.9050\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.0578 - acc: 0.9888 - val_loss: 0.2341 - val_acc: 0.9000\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.0572 - acc: 0.9888 - val_loss: 0.2350 - val_acc: 0.9100\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.0564 - acc: 0.9888 - val_loss: 0.2347 - val_acc: 0.9050\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0558 - acc: 0.9913 - val_loss: 0.2394 - val_acc: 0.9100\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.0556 - acc: 0.9900 - val_loss: 0.2367 - val_acc: 0.9100\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.0542 - acc: 0.9900 - val_loss: 0.2337 - val_acc: 0.9100\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0538 - acc: 0.9913 - val_loss: 0.2388 - val_acc: 0.9100\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.0536 - acc: 0.9913 - val_loss: 0.2391 - val_acc: 0.9100\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0534 - acc: 0.9925 - val_loss: 0.2450 - val_acc: 0.9100\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 159us/step - loss: 0.0519 - acc: 0.9900 - val_loss: 0.2469 - val_acc: 0.9050\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0514 - acc: 0.9900 - val_loss: 0.2470 - val_acc: 0.9100\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.0515 - acc: 0.9937 - val_loss: 0.2500 - val_acc: 0.9050\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0506 - acc: 0.9925 - val_loss: 0.2501 - val_acc: 0.9000\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.0492 - acc: 0.9913 - val_loss: 0.2595 - val_acc: 0.9100\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0494 - acc: 0.9925 - val_loss: 0.2540 - val_acc: 0.9100\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0493 - acc: 0.9925 - val_loss: 0.2565 - val_acc: 0.9100\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0478 - acc: 0.9913 - val_loss: 0.2570 - val_acc: 0.9050\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.0476 - acc: 0.9950 - val_loss: 0.2540 - val_acc: 0.9100\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 153us/step - loss: 0.0469 - acc: 0.9925 - val_loss: 0.2565 - val_acc: 0.9100\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0460 - acc: 0.9950 - val_loss: 0.2670 - val_acc: 0.8950\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0454 - acc: 0.9950 - val_loss: 0.2642 - val_acc: 0.9000\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.0449 - acc: 0.9925 - val_loss: 0.2631 - val_acc: 0.9050\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.0446 - acc: 0.9950 - val_loss: 0.2658 - val_acc: 0.9000\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.0436 - acc: 0.9950 - val_loss: 0.2700 - val_acc: 0.8950\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0437 - acc: 0.9937 - val_loss: 0.2698 - val_acc: 0.9000\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.0431 - acc: 0.9950 - val_loss: 0.2752 - val_acc: 0.9000\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0424 - acc: 0.9950 - val_loss: 0.2707 - val_acc: 0.9000\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.0421 - acc: 0.9950 - val_loss: 0.2747 - val_acc: 0.8950\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.0414 - acc: 0.9937 - val_loss: 0.2796 - val_acc: 0.8950\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.0406 - acc: 0.9937 - val_loss: 0.2787 - val_acc: 0.9000\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0407 - acc: 0.9925 - val_loss: 0.2836 - val_acc: 0.8950\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0391 - acc: 0.9950 - val_loss: 0.2842 - val_acc: 0.8850\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.0394 - acc: 0.9950 - val_loss: 0.2881 - val_acc: 0.8950\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.0386 - acc: 0.9950 - val_loss: 0.2835 - val_acc: 0.8950\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0375 - acc: 0.9950 - val_loss: 0.2824 - val_acc: 0.8900\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0376 - acc: 0.9950 - val_loss: 0.2906 - val_acc: 0.8950\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0371 - acc: 0.9950 - val_loss: 0.2850 - val_acc: 0.8900\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.0365 - acc: 0.9950 - val_loss: 0.2979 - val_acc: 0.8700\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0361 - acc: 0.9950 - val_loss: 0.2926 - val_acc: 0.8800\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.0355 - acc: 0.9950 - val_loss: 0.2992 - val_acc: 0.8850\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0346 - acc: 0.9950 - val_loss: 0.3006 - val_acc: 0.8850\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.0341 - acc: 0.9950 - val_loss: 0.2976 - val_acc: 0.8750\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.0344 - acc: 0.9950 - val_loss: 0.3017 - val_acc: 0.8800\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.0335 - acc: 0.9950 - val_loss: 0.3019 - val_acc: 0.8800\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0335 - acc: 0.9950 - val_loss: 0.3046 - val_acc: 0.8750\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0328 - acc: 0.9950 - val_loss: 0.3031 - val_acc: 0.8800\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.0325 - acc: 0.9950 - val_loss: 0.3097 - val_acc: 0.8750\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.0319 - acc: 0.9950 - val_loss: 0.3083 - val_acc: 0.8750\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.0313 - acc: 0.9950 - val_loss: 0.3090 - val_acc: 0.8750\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0311 - acc: 0.9950 - val_loss: 0.3207 - val_acc: 0.8800\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.0307 - acc: 0.9950 - val_loss: 0.3109 - val_acc: 0.8750\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.0304 - acc: 0.9950 - val_loss: 0.3149 - val_acc: 0.8750\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.0301 - acc: 0.9950 - val_loss: 0.3201 - val_acc: 0.8750\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.0297 - acc: 0.9950 - val_loss: 0.3200 - val_acc: 0.8750\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0290 - acc: 0.9950 - val_loss: 0.3206 - val_acc: 0.8750\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 156us/step - loss: 0.0288 - acc: 0.9950 - val_loss: 0.3233 - val_acc: 0.8750\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0285 - acc: 0.9950 - val_loss: 0.3283 - val_acc: 0.8800\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0284 - acc: 0.9950 - val_loss: 0.3252 - val_acc: 0.8800\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.0274 - acc: 0.9962 - val_loss: 0.3209 - val_acc: 0.8700\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.0277 - acc: 0.9950 - val_loss: 0.3264 - val_acc: 0.8700\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.0270 - acc: 0.9950 - val_loss: 0.3326 - val_acc: 0.8750\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.0270 - acc: 0.9950 - val_loss: 0.3322 - val_acc: 0.8700\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.0264 - acc: 0.9950 - val_loss: 0.3323 - val_acc: 0.8750\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.0263 - acc: 0.9950 - val_loss: 0.3389 - val_acc: 0.8750\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.0258 - acc: 0.9950 - val_loss: 0.3409 - val_acc: 0.8700\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.0251 - acc: 0.9950 - val_loss: 0.3411 - val_acc: 0.8700\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.0249 - acc: 0.9950 - val_loss: 0.3433 - val_acc: 0.8650\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.0247 - acc: 0.9975 - val_loss: 0.3390 - val_acc: 0.8650\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0251 - acc: 0.9962 - val_loss: 0.3398 - val_acc: 0.8700\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.0246 - acc: 0.9962 - val_loss: 0.3460 - val_acc: 0.8700\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0238 - acc: 0.9962 - val_loss: 0.3431 - val_acc: 0.8650\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.0237 - acc: 0.9962 - val_loss: 0.3447 - val_acc: 0.8750\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.0233 - acc: 0.9975 - val_loss: 0.3462 - val_acc: 0.8700\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0229 - acc: 0.9975 - val_loss: 0.3466 - val_acc: 0.8650\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.0230 - acc: 0.9975 - val_loss: 0.3509 - val_acc: 0.8650\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.0226 - acc: 0.9975 - val_loss: 0.3476 - val_acc: 0.8700\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.0223 - acc: 0.9962 - val_loss: 0.3529 - val_acc: 0.8650\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.0221 - acc: 0.9988 - val_loss: 0.3507 - val_acc: 0.8700\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.0216 - acc: 0.9988 - val_loss: 0.3530 - val_acc: 0.8650\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.0215 - acc: 0.9975 - val_loss: 0.3567 - val_acc: 0.8700\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0212 - acc: 0.9988 - val_loss: 0.3618 - val_acc: 0.8700\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.0206 - acc: 1.0000 - val_loss: 0.3550 - val_acc: 0.8700\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0209 - acc: 0.9975 - val_loss: 0.3605 - val_acc: 0.8750\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 164us/step - loss: 0.0203 - acc: 1.0000 - val_loss: 0.3571 - val_acc: 0.8750\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.0202 - acc: 0.9988 - val_loss: 0.3607 - val_acc: 0.8700\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.3609 - val_acc: 0.8700\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0199 - acc: 0.9988 - val_loss: 0.3617 - val_acc: 0.8700\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.0196 - acc: 0.9988 - val_loss: 0.3653 - val_acc: 0.8700\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.0195 - acc: 1.0000 - val_loss: 0.3651 - val_acc: 0.8700\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.3692 - val_acc: 0.8700\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.3657 - val_acc: 0.8700\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.0190 - acc: 0.9988 - val_loss: 0.3698 - val_acc: 0.8700\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 160us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.3673 - val_acc: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe62b7a7e80>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "keras.backend.clear_session()\n",
    "model=Sequential()\n",
    "model.add(Dense(32,activation='relu',input_dim=20))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,batch_size=32,epochs=200,verbose=1,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1EYkFMs9Mcr"
   },
   "source": [
    "When we set the learning rate as 0.1, we could find that we only cost 2 epochs to get the accuracy over 90%. It significantly improve the training speed, but in other case maybe it would lead to the unstable state and hard to converge to the best solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7253
    },
    "colab_type": "code",
    "id": "6cG8FjqJ9_41",
    "outputId": "13d99acb-d3f6-4053-8e10-5b4a40d44662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 0s 368us/step - loss: 0.6641 - acc: 0.6187 - val_loss: 0.6153 - val_acc: 0.7450\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 153us/step - loss: 0.5868 - acc: 0.7863 - val_loss: 0.5520 - val_acc: 0.8250\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.5272 - acc: 0.8675 - val_loss: 0.5023 - val_acc: 0.8750\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.4801 - acc: 0.8962 - val_loss: 0.4622 - val_acc: 0.9000\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.4421 - acc: 0.9150 - val_loss: 0.4294 - val_acc: 0.9150\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.4108 - acc: 0.9200 - val_loss: 0.4021 - val_acc: 0.9200\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.3848 - acc: 0.9225 - val_loss: 0.3790 - val_acc: 0.9200\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 174us/step - loss: 0.3629 - acc: 0.9275 - val_loss: 0.3592 - val_acc: 0.9200\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.3443 - acc: 0.9275 - val_loss: 0.3423 - val_acc: 0.9250\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 127us/step - loss: 0.3281 - acc: 0.9288 - val_loss: 0.3275 - val_acc: 0.9300\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.3143 - acc: 0.9312 - val_loss: 0.3146 - val_acc: 0.9300\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 125us/step - loss: 0.3022 - acc: 0.9325 - val_loss: 0.3032 - val_acc: 0.9350\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 126us/step - loss: 0.2916 - acc: 0.9375 - val_loss: 0.2932 - val_acc: 0.9300\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.2822 - acc: 0.9363 - val_loss: 0.2842 - val_acc: 0.9300\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.2739 - acc: 0.9350 - val_loss: 0.2761 - val_acc: 0.9300\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.2663 - acc: 0.9375 - val_loss: 0.2688 - val_acc: 0.9400\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 165us/step - loss: 0.2596 - acc: 0.9375 - val_loss: 0.2623 - val_acc: 0.9400\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.2537 - acc: 0.9375 - val_loss: 0.2563 - val_acc: 0.9400\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.2481 - acc: 0.9387 - val_loss: 0.2508 - val_acc: 0.9400\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.2431 - acc: 0.9387 - val_loss: 0.2457 - val_acc: 0.9400\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.2386 - acc: 0.9375 - val_loss: 0.2411 - val_acc: 0.9400\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.2344 - acc: 0.9400 - val_loss: 0.2368 - val_acc: 0.9400\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.2306 - acc: 0.9400 - val_loss: 0.2329 - val_acc: 0.9400\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.2271 - acc: 0.9400 - val_loss: 0.2293 - val_acc: 0.9400\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.2238 - acc: 0.9400 - val_loss: 0.2259 - val_acc: 0.9400\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.2208 - acc: 0.9400 - val_loss: 0.2227 - val_acc: 0.9400\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.2179 - acc: 0.9400 - val_loss: 0.2197 - val_acc: 0.9400\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.2153 - acc: 0.9425 - val_loss: 0.2170 - val_acc: 0.9400\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.2129 - acc: 0.9425 - val_loss: 0.2143 - val_acc: 0.9400\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.2104 - acc: 0.9450 - val_loss: 0.2119 - val_acc: 0.9400\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.2082 - acc: 0.9438 - val_loss: 0.2096 - val_acc: 0.9400\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.2061 - acc: 0.9438 - val_loss: 0.2074 - val_acc: 0.9400\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.2043 - acc: 0.9438 - val_loss: 0.2054 - val_acc: 0.9400\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.2024 - acc: 0.9438 - val_loss: 0.2034 - val_acc: 0.9400\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.2007 - acc: 0.9450 - val_loss: 0.2016 - val_acc: 0.9400\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 163us/step - loss: 0.1989 - acc: 0.9438 - val_loss: 0.1998 - val_acc: 0.9400\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1973 - acc: 0.9438 - val_loss: 0.1981 - val_acc: 0.9400\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1958 - acc: 0.9450 - val_loss: 0.1965 - val_acc: 0.9400\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1944 - acc: 0.9438 - val_loss: 0.1950 - val_acc: 0.9450\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.1931 - acc: 0.9462 - val_loss: 0.1935 - val_acc: 0.9450\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1917 - acc: 0.9462 - val_loss: 0.1922 - val_acc: 0.9450\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1904 - acc: 0.9462 - val_loss: 0.1909 - val_acc: 0.9450\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1892 - acc: 0.9462 - val_loss: 0.1896 - val_acc: 0.9450\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1880 - acc: 0.9462 - val_loss: 0.1884 - val_acc: 0.9450\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 153us/step - loss: 0.1869 - acc: 0.9475 - val_loss: 0.1873 - val_acc: 0.9450\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1859 - acc: 0.9475 - val_loss: 0.1862 - val_acc: 0.9450\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.1848 - acc: 0.9475 - val_loss: 0.1851 - val_acc: 0.9450\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1838 - acc: 0.9462 - val_loss: 0.1841 - val_acc: 0.9450\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1828 - acc: 0.9475 - val_loss: 0.1831 - val_acc: 0.9400\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1818 - acc: 0.9475 - val_loss: 0.1822 - val_acc: 0.9400\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1810 - acc: 0.9487 - val_loss: 0.1813 - val_acc: 0.9400\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.1801 - acc: 0.9475 - val_loss: 0.1804 - val_acc: 0.9400\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1791 - acc: 0.9487 - val_loss: 0.1796 - val_acc: 0.9400\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1783 - acc: 0.9500 - val_loss: 0.1788 - val_acc: 0.9400\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1776 - acc: 0.9500 - val_loss: 0.1780 - val_acc: 0.9400\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1768 - acc: 0.9500 - val_loss: 0.1773 - val_acc: 0.9400\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1759 - acc: 0.9525 - val_loss: 0.1766 - val_acc: 0.9400\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1752 - acc: 0.9513 - val_loss: 0.1759 - val_acc: 0.9400\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1745 - acc: 0.9513 - val_loss: 0.1752 - val_acc: 0.9400\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1738 - acc: 0.9513 - val_loss: 0.1745 - val_acc: 0.9400\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1731 - acc: 0.9537 - val_loss: 0.1739 - val_acc: 0.9400\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1724 - acc: 0.9525 - val_loss: 0.1732 - val_acc: 0.9400\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1717 - acc: 0.9525 - val_loss: 0.1726 - val_acc: 0.9400\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 157us/step - loss: 0.1711 - acc: 0.9537 - val_loss: 0.1720 - val_acc: 0.9400\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1704 - acc: 0.9550 - val_loss: 0.1715 - val_acc: 0.9400\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1698 - acc: 0.9537 - val_loss: 0.1710 - val_acc: 0.9400\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1691 - acc: 0.9550 - val_loss: 0.1704 - val_acc: 0.9400\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1686 - acc: 0.9550 - val_loss: 0.1700 - val_acc: 0.9400\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1679 - acc: 0.9550 - val_loss: 0.1695 - val_acc: 0.9400\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1674 - acc: 0.9550 - val_loss: 0.1690 - val_acc: 0.9400\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1668 - acc: 0.9550 - val_loss: 0.1685 - val_acc: 0.9400\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.1663 - acc: 0.9550 - val_loss: 0.1680 - val_acc: 0.9400\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.1657 - acc: 0.9537 - val_loss: 0.1676 - val_acc: 0.9400\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1651 - acc: 0.9550 - val_loss: 0.1672 - val_acc: 0.9400\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1646 - acc: 0.9550 - val_loss: 0.1668 - val_acc: 0.9400\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1641 - acc: 0.9563 - val_loss: 0.1664 - val_acc: 0.9400\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1637 - acc: 0.9537 - val_loss: 0.1660 - val_acc: 0.9400\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.1631 - acc: 0.9550 - val_loss: 0.1657 - val_acc: 0.9400\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1626 - acc: 0.9563 - val_loss: 0.1653 - val_acc: 0.9400\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1622 - acc: 0.9563 - val_loss: 0.1649 - val_acc: 0.9400\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1616 - acc: 0.9563 - val_loss: 0.1646 - val_acc: 0.9400\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 153us/step - loss: 0.1612 - acc: 0.9563 - val_loss: 0.1642 - val_acc: 0.9400\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1606 - acc: 0.9563 - val_loss: 0.1639 - val_acc: 0.9400\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1603 - acc: 0.9575 - val_loss: 0.1636 - val_acc: 0.9400\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1597 - acc: 0.9575 - val_loss: 0.1633 - val_acc: 0.9400\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1593 - acc: 0.9563 - val_loss: 0.1630 - val_acc: 0.9400\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1589 - acc: 0.9587 - val_loss: 0.1626 - val_acc: 0.9400\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1585 - acc: 0.9575 - val_loss: 0.1624 - val_acc: 0.9400\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1580 - acc: 0.9587 - val_loss: 0.1621 - val_acc: 0.9400\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1576 - acc: 0.9575 - val_loss: 0.1618 - val_acc: 0.9400\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 156us/step - loss: 0.1572 - acc: 0.9587 - val_loss: 0.1615 - val_acc: 0.9400\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1568 - acc: 0.9575 - val_loss: 0.1613 - val_acc: 0.9400\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.1563 - acc: 0.9612 - val_loss: 0.1611 - val_acc: 0.9400\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1559 - acc: 0.9612 - val_loss: 0.1608 - val_acc: 0.9400\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1555 - acc: 0.9600 - val_loss: 0.1605 - val_acc: 0.9400\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.1551 - acc: 0.9600 - val_loss: 0.1603 - val_acc: 0.9400\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1548 - acc: 0.9612 - val_loss: 0.1601 - val_acc: 0.9400\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1544 - acc: 0.9612 - val_loss: 0.1598 - val_acc: 0.9400\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1540 - acc: 0.9612 - val_loss: 0.1595 - val_acc: 0.9400\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1536 - acc: 0.9612 - val_loss: 0.1593 - val_acc: 0.9400\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.1532 - acc: 0.9612 - val_loss: 0.1591 - val_acc: 0.9350\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1528 - acc: 0.9612 - val_loss: 0.1589 - val_acc: 0.9350\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1525 - acc: 0.9600 - val_loss: 0.1588 - val_acc: 0.9350\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1521 - acc: 0.9612 - val_loss: 0.1585 - val_acc: 0.9350\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1517 - acc: 0.9612 - val_loss: 0.1583 - val_acc: 0.9350\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1514 - acc: 0.9625 - val_loss: 0.1582 - val_acc: 0.9350\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1510 - acc: 0.9612 - val_loss: 0.1580 - val_acc: 0.9350\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1506 - acc: 0.9625 - val_loss: 0.1578 - val_acc: 0.9350\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1504 - acc: 0.9625 - val_loss: 0.1577 - val_acc: 0.9350\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 160us/step - loss: 0.1499 - acc: 0.9625 - val_loss: 0.1575 - val_acc: 0.9350\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.1496 - acc: 0.9625 - val_loss: 0.1573 - val_acc: 0.9350\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1493 - acc: 0.9625 - val_loss: 0.1571 - val_acc: 0.9350\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1490 - acc: 0.9625 - val_loss: 0.1570 - val_acc: 0.9350\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1486 - acc: 0.9625 - val_loss: 0.1569 - val_acc: 0.9350\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1483 - acc: 0.9625 - val_loss: 0.1567 - val_acc: 0.9350\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1479 - acc: 0.9625 - val_loss: 0.1565 - val_acc: 0.9350\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.1476 - acc: 0.9625 - val_loss: 0.1564 - val_acc: 0.9350\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1473 - acc: 0.9625 - val_loss: 0.1563 - val_acc: 0.9350\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1470 - acc: 0.9625 - val_loss: 0.1561 - val_acc: 0.9350\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.1466 - acc: 0.9625 - val_loss: 0.1560 - val_acc: 0.9350\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1463 - acc: 0.9625 - val_loss: 0.1559 - val_acc: 0.9350\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1460 - acc: 0.9625 - val_loss: 0.1558 - val_acc: 0.9350\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.1457 - acc: 0.9625 - val_loss: 0.1556 - val_acc: 0.9350\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1453 - acc: 0.9625 - val_loss: 0.1555 - val_acc: 0.9350\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1451 - acc: 0.9625 - val_loss: 0.1554 - val_acc: 0.9350\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1447 - acc: 0.9637 - val_loss: 0.1553 - val_acc: 0.9350\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1444 - acc: 0.9625 - val_loss: 0.1552 - val_acc: 0.9350\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1442 - acc: 0.9625 - val_loss: 0.1552 - val_acc: 0.9350\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.1438 - acc: 0.9625 - val_loss: 0.1550 - val_acc: 0.9350\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1435 - acc: 0.9625 - val_loss: 0.1549 - val_acc: 0.9350\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1432 - acc: 0.9637 - val_loss: 0.1548 - val_acc: 0.9350\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1429 - acc: 0.9637 - val_loss: 0.1548 - val_acc: 0.9350\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1426 - acc: 0.9637 - val_loss: 0.1547 - val_acc: 0.9350\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1423 - acc: 0.9637 - val_loss: 0.1546 - val_acc: 0.9350\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 140us/step - loss: 0.1420 - acc: 0.9650 - val_loss: 0.1545 - val_acc: 0.9350\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1418 - acc: 0.9637 - val_loss: 0.1544 - val_acc: 0.9350\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.1414 - acc: 0.9650 - val_loss: 0.1543 - val_acc: 0.9350\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.1412 - acc: 0.9650 - val_loss: 0.1543 - val_acc: 0.9350\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1409 - acc: 0.9650 - val_loss: 0.1542 - val_acc: 0.9350\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1406 - acc: 0.9650 - val_loss: 0.1541 - val_acc: 0.9350\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.1403 - acc: 0.9650 - val_loss: 0.1540 - val_acc: 0.9350\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1400 - acc: 0.9650 - val_loss: 0.1539 - val_acc: 0.9350\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1398 - acc: 0.9650 - val_loss: 0.1539 - val_acc: 0.9350\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1395 - acc: 0.9650 - val_loss: 0.1538 - val_acc: 0.9350\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1392 - acc: 0.9650 - val_loss: 0.1537 - val_acc: 0.9350\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1389 - acc: 0.9650 - val_loss: 0.1536 - val_acc: 0.9350\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.1386 - acc: 0.9650 - val_loss: 0.1536 - val_acc: 0.9350\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1384 - acc: 0.9650 - val_loss: 0.1535 - val_acc: 0.9350\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1381 - acc: 0.9650 - val_loss: 0.1535 - val_acc: 0.9350\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1378 - acc: 0.9650 - val_loss: 0.1534 - val_acc: 0.9350\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.1376 - acc: 0.9650 - val_loss: 0.1534 - val_acc: 0.9350\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.1374 - acc: 0.9650 - val_loss: 0.1534 - val_acc: 0.9350\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1370 - acc: 0.9650 - val_loss: 0.1533 - val_acc: 0.9350\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1368 - acc: 0.9650 - val_loss: 0.1532 - val_acc: 0.9350\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1365 - acc: 0.9650 - val_loss: 0.1532 - val_acc: 0.9350\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.1362 - acc: 0.9650 - val_loss: 0.1532 - val_acc: 0.9350\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1360 - acc: 0.9650 - val_loss: 0.1531 - val_acc: 0.9350\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1357 - acc: 0.9650 - val_loss: 0.1531 - val_acc: 0.9350\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1355 - acc: 0.9650 - val_loss: 0.1531 - val_acc: 0.9350\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1353 - acc: 0.9650 - val_loss: 0.1530 - val_acc: 0.9350\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1349 - acc: 0.9650 - val_loss: 0.1530 - val_acc: 0.9350\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.1348 - acc: 0.9662 - val_loss: 0.1529 - val_acc: 0.9350\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1344 - acc: 0.9662 - val_loss: 0.1529 - val_acc: 0.9350\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1342 - acc: 0.9675 - val_loss: 0.1528 - val_acc: 0.9350\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 160us/step - loss: 0.1339 - acc: 0.9675 - val_loss: 0.1528 - val_acc: 0.9350\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1337 - acc: 0.9675 - val_loss: 0.1528 - val_acc: 0.9350\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1335 - acc: 0.9662 - val_loss: 0.1528 - val_acc: 0.9350\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1332 - acc: 0.9675 - val_loss: 0.1528 - val_acc: 0.9350\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1330 - acc: 0.9675 - val_loss: 0.1528 - val_acc: 0.9350\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 134us/step - loss: 0.1327 - acc: 0.9675 - val_loss: 0.1528 - val_acc: 0.9350\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 129us/step - loss: 0.1325 - acc: 0.9675 - val_loss: 0.1527 - val_acc: 0.9350\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1322 - acc: 0.9675 - val_loss: 0.1527 - val_acc: 0.9350\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1319 - acc: 0.9675 - val_loss: 0.1526 - val_acc: 0.9350\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1317 - acc: 0.9675 - val_loss: 0.1526 - val_acc: 0.9350\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1314 - acc: 0.9675 - val_loss: 0.1526 - val_acc: 0.9350\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.1312 - acc: 0.9675 - val_loss: 0.1526 - val_acc: 0.9350\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1310 - acc: 0.9675 - val_loss: 0.1526 - val_acc: 0.9350\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1307 - acc: 0.9675 - val_loss: 0.1526 - val_acc: 0.9350\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1305 - acc: 0.9675 - val_loss: 0.1526 - val_acc: 0.9350\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.1303 - acc: 0.9675 - val_loss: 0.1525 - val_acc: 0.9350\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1300 - acc: 0.9675 - val_loss: 0.1525 - val_acc: 0.9350\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1298 - acc: 0.9675 - val_loss: 0.1525 - val_acc: 0.9350\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1296 - acc: 0.9675 - val_loss: 0.1525 - val_acc: 0.9350\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.1294 - acc: 0.9675 - val_loss: 0.1525 - val_acc: 0.9350\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.1291 - acc: 0.9675 - val_loss: 0.1525 - val_acc: 0.9350\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1289 - acc: 0.9675 - val_loss: 0.1525 - val_acc: 0.9350\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1287 - acc: 0.9675 - val_loss: 0.1525 - val_acc: 0.9350\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 137us/step - loss: 0.1284 - acc: 0.9675 - val_loss: 0.1525 - val_acc: 0.9350\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1282 - acc: 0.9675 - val_loss: 0.1524 - val_acc: 0.9350\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 131us/step - loss: 0.1280 - acc: 0.9675 - val_loss: 0.1524 - val_acc: 0.9350\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1277 - acc: 0.9675 - val_loss: 0.1524 - val_acc: 0.9350\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1275 - acc: 0.9675 - val_loss: 0.1524 - val_acc: 0.9350\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 159us/step - loss: 0.1272 - acc: 0.9675 - val_loss: 0.1524 - val_acc: 0.9350\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.1270 - acc: 0.9675 - val_loss: 0.1525 - val_acc: 0.9350\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1268 - acc: 0.9675 - val_loss: 0.1524 - val_acc: 0.9350\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 132us/step - loss: 0.1266 - acc: 0.9675 - val_loss: 0.1525 - val_acc: 0.9350\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.1264 - acc: 0.9675 - val_loss: 0.1525 - val_acc: 0.9350\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1261 - acc: 0.9675 - val_loss: 0.1525 - val_acc: 0.9350\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.1259 - acc: 0.9675 - val_loss: 0.1525 - val_acc: 0.9350\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 130us/step - loss: 0.1257 - acc: 0.9675 - val_loss: 0.1525 - val_acc: 0.9350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe62b252e80>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "keras.backend.clear_session()\n",
    "model=Sequential()\n",
    "model.add(Dense(320,activation='relu',input_dim=20))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,batch_size=32,epochs=200,verbose=1,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3avzviU-Mmq"
   },
   "source": [
    "We find that when we increase the number in the units in hiden layer from 32 to 320, it takes less epochs to reach the same level,only cost 5 epochs. So it also could improve the speed of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7253
    },
    "colab_type": "code",
    "id": "x-ZBJiT0_Bi8",
    "outputId": "f4428540-3576-4e23-d25d-fac5bb04291c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 0s 366us/step - loss: 0.7218 - acc: 0.4725 - val_loss: 0.7010 - val_acc: 0.5400\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.6654 - acc: 0.5763 - val_loss: 0.6556 - val_acc: 0.6650\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 160us/step - loss: 0.6225 - acc: 0.6913 - val_loss: 0.6192 - val_acc: 0.7000\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.5858 - acc: 0.7600 - val_loss: 0.5870 - val_acc: 0.7650\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.5521 - acc: 0.8038 - val_loss: 0.5570 - val_acc: 0.8050\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.5208 - acc: 0.8250 - val_loss: 0.5285 - val_acc: 0.8250\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.4915 - acc: 0.8525 - val_loss: 0.5016 - val_acc: 0.8350\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 136us/step - loss: 0.4642 - acc: 0.8700 - val_loss: 0.4763 - val_acc: 0.8400\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.4385 - acc: 0.8825 - val_loss: 0.4525 - val_acc: 0.8500\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.4151 - acc: 0.8888 - val_loss: 0.4305 - val_acc: 0.8500\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.3936 - acc: 0.8925 - val_loss: 0.4102 - val_acc: 0.8650\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 156us/step - loss: 0.3743 - acc: 0.8975 - val_loss: 0.3916 - val_acc: 0.8650\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.3568 - acc: 0.9000 - val_loss: 0.3744 - val_acc: 0.8750\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 135us/step - loss: 0.3409 - acc: 0.9037 - val_loss: 0.3585 - val_acc: 0.8800\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 133us/step - loss: 0.3269 - acc: 0.9062 - val_loss: 0.3440 - val_acc: 0.8900\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 139us/step - loss: 0.3141 - acc: 0.9100 - val_loss: 0.3308 - val_acc: 0.8950\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 138us/step - loss: 0.3029 - acc: 0.9100 - val_loss: 0.3191 - val_acc: 0.8900\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.2927 - acc: 0.9125 - val_loss: 0.3084 - val_acc: 0.8850\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.2840 - acc: 0.9113 - val_loss: 0.2989 - val_acc: 0.8950\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.2757 - acc: 0.9125 - val_loss: 0.2901 - val_acc: 0.8950\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 171us/step - loss: 0.2684 - acc: 0.9138 - val_loss: 0.2820 - val_acc: 0.8950\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.2618 - acc: 0.9138 - val_loss: 0.2747 - val_acc: 0.8950\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.2556 - acc: 0.9163 - val_loss: 0.2679 - val_acc: 0.9050\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.2503 - acc: 0.9163 - val_loss: 0.2616 - val_acc: 0.9100\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.2454 - acc: 0.9163 - val_loss: 0.2557 - val_acc: 0.9100\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.2405 - acc: 0.9187 - val_loss: 0.2504 - val_acc: 0.9100\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.2363 - acc: 0.9225 - val_loss: 0.2455 - val_acc: 0.9050\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.2324 - acc: 0.9275 - val_loss: 0.2408 - val_acc: 0.9050\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.2286 - acc: 0.9275 - val_loss: 0.2366 - val_acc: 0.9050\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 167us/step - loss: 0.2251 - acc: 0.9312 - val_loss: 0.2325 - val_acc: 0.9050\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.2219 - acc: 0.9300 - val_loss: 0.2290 - val_acc: 0.9100\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.2189 - acc: 0.9300 - val_loss: 0.2254 - val_acc: 0.9150\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.2159 - acc: 0.9300 - val_loss: 0.2223 - val_acc: 0.9150\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.2132 - acc: 0.9325 - val_loss: 0.2194 - val_acc: 0.9200\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.2106 - acc: 0.9312 - val_loss: 0.2168 - val_acc: 0.9200\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.2084 - acc: 0.9312 - val_loss: 0.2141 - val_acc: 0.9200\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.2062 - acc: 0.9325 - val_loss: 0.2118 - val_acc: 0.9200\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 159us/step - loss: 0.2039 - acc: 0.9325 - val_loss: 0.2094 - val_acc: 0.9200\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 170us/step - loss: 0.2020 - acc: 0.9325 - val_loss: 0.2072 - val_acc: 0.9200\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.2000 - acc: 0.9337 - val_loss: 0.2052 - val_acc: 0.9250\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1983 - acc: 0.9325 - val_loss: 0.2035 - val_acc: 0.9250\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1965 - acc: 0.9337 - val_loss: 0.2017 - val_acc: 0.9250\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1947 - acc: 0.9337 - val_loss: 0.2000 - val_acc: 0.9250\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1932 - acc: 0.9337 - val_loss: 0.1985 - val_acc: 0.9250\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1916 - acc: 0.9363 - val_loss: 0.1972 - val_acc: 0.9250\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.1901 - acc: 0.9363 - val_loss: 0.1958 - val_acc: 0.9300\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 169us/step - loss: 0.1886 - acc: 0.9375 - val_loss: 0.1943 - val_acc: 0.9300\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1875 - acc: 0.9375 - val_loss: 0.1931 - val_acc: 0.9300\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1861 - acc: 0.9375 - val_loss: 0.1920 - val_acc: 0.9300\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1848 - acc: 0.9375 - val_loss: 0.1909 - val_acc: 0.9300\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1835 - acc: 0.9375 - val_loss: 0.1900 - val_acc: 0.9300\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1826 - acc: 0.9400 - val_loss: 0.1889 - val_acc: 0.9300\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.1813 - acc: 0.9413 - val_loss: 0.1877 - val_acc: 0.9300\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.1802 - acc: 0.9400 - val_loss: 0.1868 - val_acc: 0.9300\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1792 - acc: 0.9425 - val_loss: 0.1862 - val_acc: 0.9300\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 171us/step - loss: 0.1782 - acc: 0.9438 - val_loss: 0.1853 - val_acc: 0.9300\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.1772 - acc: 0.9438 - val_loss: 0.1848 - val_acc: 0.9300\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1762 - acc: 0.9438 - val_loss: 0.1842 - val_acc: 0.9300\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.1752 - acc: 0.9438 - val_loss: 0.1837 - val_acc: 0.9300\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1744 - acc: 0.9438 - val_loss: 0.1829 - val_acc: 0.9300\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1734 - acc: 0.9450 - val_loss: 0.1823 - val_acc: 0.9300\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1725 - acc: 0.9450 - val_loss: 0.1820 - val_acc: 0.9300\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1717 - acc: 0.9450 - val_loss: 0.1814 - val_acc: 0.9300\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 168us/step - loss: 0.1707 - acc: 0.9450 - val_loss: 0.1810 - val_acc: 0.9350\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.1699 - acc: 0.9475 - val_loss: 0.1807 - val_acc: 0.9350\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.1692 - acc: 0.9475 - val_loss: 0.1804 - val_acc: 0.9350\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1685 - acc: 0.9487 - val_loss: 0.1800 - val_acc: 0.9350\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.1678 - acc: 0.9475 - val_loss: 0.1797 - val_acc: 0.9350\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1670 - acc: 0.9500 - val_loss: 0.1789 - val_acc: 0.9350\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1664 - acc: 0.9500 - val_loss: 0.1787 - val_acc: 0.9350\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.1655 - acc: 0.9487 - val_loss: 0.1784 - val_acc: 0.9350\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1649 - acc: 0.9513 - val_loss: 0.1780 - val_acc: 0.9350\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 168us/step - loss: 0.1642 - acc: 0.9513 - val_loss: 0.1779 - val_acc: 0.9350\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.1635 - acc: 0.9525 - val_loss: 0.1776 - val_acc: 0.9350\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 159us/step - loss: 0.1629 - acc: 0.9513 - val_loss: 0.1773 - val_acc: 0.9350\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1622 - acc: 0.9513 - val_loss: 0.1773 - val_acc: 0.9350\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1615 - acc: 0.9513 - val_loss: 0.1772 - val_acc: 0.9350\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.1609 - acc: 0.9525 - val_loss: 0.1770 - val_acc: 0.9350\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1604 - acc: 0.9525 - val_loss: 0.1766 - val_acc: 0.9350\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1596 - acc: 0.9525 - val_loss: 0.1763 - val_acc: 0.9350\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 171us/step - loss: 0.1590 - acc: 0.9537 - val_loss: 0.1762 - val_acc: 0.9350\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.1584 - acc: 0.9525 - val_loss: 0.1760 - val_acc: 0.9350\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1578 - acc: 0.9525 - val_loss: 0.1757 - val_acc: 0.9350\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.1572 - acc: 0.9550 - val_loss: 0.1754 - val_acc: 0.9350\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1568 - acc: 0.9550 - val_loss: 0.1754 - val_acc: 0.9350\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1562 - acc: 0.9575 - val_loss: 0.1753 - val_acc: 0.9350\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.1557 - acc: 0.9563 - val_loss: 0.1752 - val_acc: 0.9350\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1552 - acc: 0.9575 - val_loss: 0.1752 - val_acc: 0.9350\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1546 - acc: 0.9575 - val_loss: 0.1752 - val_acc: 0.9350\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 168us/step - loss: 0.1541 - acc: 0.9575 - val_loss: 0.1750 - val_acc: 0.9350\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 155us/step - loss: 0.1536 - acc: 0.9575 - val_loss: 0.1747 - val_acc: 0.9350\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.1531 - acc: 0.9575 - val_loss: 0.1747 - val_acc: 0.9350\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.1526 - acc: 0.9575 - val_loss: 0.1744 - val_acc: 0.9350\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1520 - acc: 0.9612 - val_loss: 0.1744 - val_acc: 0.9350\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1516 - acc: 0.9587 - val_loss: 0.1743 - val_acc: 0.9350\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1512 - acc: 0.9600 - val_loss: 0.1744 - val_acc: 0.9350\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1505 - acc: 0.9600 - val_loss: 0.1746 - val_acc: 0.9350\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 172us/step - loss: 0.1500 - acc: 0.9612 - val_loss: 0.1747 - val_acc: 0.9350\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.1498 - acc: 0.9600 - val_loss: 0.1746 - val_acc: 0.9350\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 153us/step - loss: 0.1492 - acc: 0.9587 - val_loss: 0.1746 - val_acc: 0.9350\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1487 - acc: 0.9587 - val_loss: 0.1746 - val_acc: 0.9350\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1482 - acc: 0.9600 - val_loss: 0.1746 - val_acc: 0.9350\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.1478 - acc: 0.9612 - val_loss: 0.1745 - val_acc: 0.9350\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1473 - acc: 0.9612 - val_loss: 0.1745 - val_acc: 0.9350\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1468 - acc: 0.9612 - val_loss: 0.1744 - val_acc: 0.9350\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.1462 - acc: 0.9625 - val_loss: 0.1745 - val_acc: 0.9350\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 167us/step - loss: 0.1458 - acc: 0.9625 - val_loss: 0.1742 - val_acc: 0.9350\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1455 - acc: 0.9625 - val_loss: 0.1742 - val_acc: 0.9350\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.1449 - acc: 0.9625 - val_loss: 0.1745 - val_acc: 0.9350\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.1446 - acc: 0.9625 - val_loss: 0.1746 - val_acc: 0.9350\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1441 - acc: 0.9625 - val_loss: 0.1745 - val_acc: 0.9350\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.1436 - acc: 0.9625 - val_loss: 0.1745 - val_acc: 0.9350\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1433 - acc: 0.9637 - val_loss: 0.1748 - val_acc: 0.9350\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.1428 - acc: 0.9625 - val_loss: 0.1748 - val_acc: 0.9350\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 169us/step - loss: 0.1424 - acc: 0.9625 - val_loss: 0.1749 - val_acc: 0.9350\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1420 - acc: 0.9625 - val_loss: 0.1749 - val_acc: 0.9400\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.1415 - acc: 0.9625 - val_loss: 0.1748 - val_acc: 0.9400\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.1411 - acc: 0.9637 - val_loss: 0.1747 - val_acc: 0.9400\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1407 - acc: 0.9625 - val_loss: 0.1747 - val_acc: 0.9400\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1402 - acc: 0.9637 - val_loss: 0.1750 - val_acc: 0.9400\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1399 - acc: 0.9637 - val_loss: 0.1748 - val_acc: 0.9400\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1394 - acc: 0.9637 - val_loss: 0.1747 - val_acc: 0.9400\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.1391 - acc: 0.9637 - val_loss: 0.1749 - val_acc: 0.9400\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.1386 - acc: 0.9637 - val_loss: 0.1752 - val_acc: 0.9400\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1381 - acc: 0.9637 - val_loss: 0.1751 - val_acc: 0.9400\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.1378 - acc: 0.9637 - val_loss: 0.1750 - val_acc: 0.9400\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1373 - acc: 0.9637 - val_loss: 0.1753 - val_acc: 0.9400\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1369 - acc: 0.9637 - val_loss: 0.1752 - val_acc: 0.9400\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1365 - acc: 0.9637 - val_loss: 0.1753 - val_acc: 0.9400\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1361 - acc: 0.9637 - val_loss: 0.1754 - val_acc: 0.9400\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1357 - acc: 0.9637 - val_loss: 0.1754 - val_acc: 0.9400\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 159us/step - loss: 0.1354 - acc: 0.9637 - val_loss: 0.1753 - val_acc: 0.9400\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.1349 - acc: 0.9637 - val_loss: 0.1753 - val_acc: 0.9400\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1345 - acc: 0.9637 - val_loss: 0.1754 - val_acc: 0.9400\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1341 - acc: 0.9637 - val_loss: 0.1755 - val_acc: 0.9400\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1338 - acc: 0.9637 - val_loss: 0.1755 - val_acc: 0.9400\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1333 - acc: 0.9637 - val_loss: 0.1757 - val_acc: 0.9400\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1330 - acc: 0.9637 - val_loss: 0.1759 - val_acc: 0.9400\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1326 - acc: 0.9650 - val_loss: 0.1758 - val_acc: 0.9400\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1322 - acc: 0.9650 - val_loss: 0.1758 - val_acc: 0.9400\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 170us/step - loss: 0.1317 - acc: 0.9637 - val_loss: 0.1758 - val_acc: 0.9350\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.1315 - acc: 0.9662 - val_loss: 0.1756 - val_acc: 0.9400\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.1312 - acc: 0.9650 - val_loss: 0.1757 - val_acc: 0.9350\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1309 - acc: 0.9662 - val_loss: 0.1757 - val_acc: 0.9350\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1304 - acc: 0.9650 - val_loss: 0.1756 - val_acc: 0.9350\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.1301 - acc: 0.9650 - val_loss: 0.1755 - val_acc: 0.9350\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1296 - acc: 0.9662 - val_loss: 0.1755 - val_acc: 0.9350\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.1293 - acc: 0.9662 - val_loss: 0.1757 - val_acc: 0.9350\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.1290 - acc: 0.9675 - val_loss: 0.1756 - val_acc: 0.9350\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 166us/step - loss: 0.1285 - acc: 0.9675 - val_loss: 0.1759 - val_acc: 0.9350\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1282 - acc: 0.9662 - val_loss: 0.1758 - val_acc: 0.9350\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.1278 - acc: 0.9675 - val_loss: 0.1760 - val_acc: 0.9350\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 159us/step - loss: 0.1275 - acc: 0.9662 - val_loss: 0.1761 - val_acc: 0.9350\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 165us/step - loss: 0.1271 - acc: 0.9675 - val_loss: 0.1763 - val_acc: 0.9350\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1267 - acc: 0.9675 - val_loss: 0.1767 - val_acc: 0.9350\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.1265 - acc: 0.9675 - val_loss: 0.1764 - val_acc: 0.9350\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.1260 - acc: 0.9662 - val_loss: 0.1766 - val_acc: 0.9350\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 159us/step - loss: 0.1258 - acc: 0.9688 - val_loss: 0.1764 - val_acc: 0.9350\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1254 - acc: 0.9688 - val_loss: 0.1766 - val_acc: 0.9350\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 150us/step - loss: 0.1250 - acc: 0.9688 - val_loss: 0.1768 - val_acc: 0.9350\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1247 - acc: 0.9688 - val_loss: 0.1768 - val_acc: 0.9350\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.1243 - acc: 0.9688 - val_loss: 0.1768 - val_acc: 0.9350\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1240 - acc: 0.9688 - val_loss: 0.1768 - val_acc: 0.9350\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.1235 - acc: 0.9688 - val_loss: 0.1770 - val_acc: 0.9350\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1233 - acc: 0.9700 - val_loss: 0.1769 - val_acc: 0.9350\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 155us/step - loss: 0.1229 - acc: 0.9688 - val_loss: 0.1769 - val_acc: 0.9350\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 152us/step - loss: 0.1225 - acc: 0.9713 - val_loss: 0.1774 - val_acc: 0.9350\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1222 - acc: 0.9713 - val_loss: 0.1771 - val_acc: 0.9350\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1220 - acc: 0.9700 - val_loss: 0.1773 - val_acc: 0.9350\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 148us/step - loss: 0.1216 - acc: 0.9725 - val_loss: 0.1776 - val_acc: 0.9350\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 141us/step - loss: 0.1212 - acc: 0.9725 - val_loss: 0.1778 - val_acc: 0.9350\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1209 - acc: 0.9700 - val_loss: 0.1778 - val_acc: 0.9350\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1206 - acc: 0.9700 - val_loss: 0.1781 - val_acc: 0.9350\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1202 - acc: 0.9713 - val_loss: 0.1783 - val_acc: 0.9350\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 168us/step - loss: 0.1197 - acc: 0.9725 - val_loss: 0.1781 - val_acc: 0.9350\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1195 - acc: 0.9738 - val_loss: 0.1780 - val_acc: 0.9350\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1192 - acc: 0.9725 - val_loss: 0.1781 - val_acc: 0.9350\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1188 - acc: 0.9713 - val_loss: 0.1785 - val_acc: 0.9350\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1184 - acc: 0.9738 - val_loss: 0.1783 - val_acc: 0.9350\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1182 - acc: 0.9738 - val_loss: 0.1783 - val_acc: 0.9350\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.1179 - acc: 0.9738 - val_loss: 0.1784 - val_acc: 0.9350\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1176 - acc: 0.9738 - val_loss: 0.1784 - val_acc: 0.9350\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1173 - acc: 0.9713 - val_loss: 0.1785 - val_acc: 0.9350\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.1168 - acc: 0.9738 - val_loss: 0.1787 - val_acc: 0.9350\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 146us/step - loss: 0.1165 - acc: 0.9738 - val_loss: 0.1789 - val_acc: 0.9350\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 149us/step - loss: 0.1162 - acc: 0.9738 - val_loss: 0.1789 - val_acc: 0.9350\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1158 - acc: 0.9725 - val_loss: 0.1792 - val_acc: 0.9350\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1156 - acc: 0.9713 - val_loss: 0.1795 - val_acc: 0.9350\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1152 - acc: 0.9738 - val_loss: 0.1792 - val_acc: 0.9300\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 145us/step - loss: 0.1149 - acc: 0.9738 - val_loss: 0.1793 - val_acc: 0.9350\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1146 - acc: 0.9738 - val_loss: 0.1795 - val_acc: 0.9350\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 173us/step - loss: 0.1142 - acc: 0.9738 - val_loss: 0.1795 - val_acc: 0.9350\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1139 - acc: 0.9725 - val_loss: 0.1796 - val_acc: 0.9350\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1136 - acc: 0.9738 - val_loss: 0.1800 - val_acc: 0.9350\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1133 - acc: 0.9738 - val_loss: 0.1799 - val_acc: 0.9350\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 142us/step - loss: 0.1130 - acc: 0.9738 - val_loss: 0.1801 - val_acc: 0.9350\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 147us/step - loss: 0.1127 - acc: 0.9738 - val_loss: 0.1803 - val_acc: 0.9350\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 151us/step - loss: 0.1124 - acc: 0.9738 - val_loss: 0.1802 - val_acc: 0.9350\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 143us/step - loss: 0.1121 - acc: 0.9763 - val_loss: 0.1806 - val_acc: 0.9350\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 144us/step - loss: 0.1118 - acc: 0.9738 - val_loss: 0.1806 - val_acc: 0.9350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe62c2450b8>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "keras.backend.clear_session()\n",
    "model=Sequential()\n",
    "model.add(Dense(32,activation='relu',input_dim=20))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,batch_size=32,epochs=200,verbose=1,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_H6YH2d_ui_"
   },
   "source": [
    "Compared with the only one hiden layer, using two hiden layers couls imporve the speech in some degree and also improve the performence in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lM3EJl6HqsZR"
   },
   "source": [
    "# 2. Simple MLP with 10 output classes (multi-class problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6_7Fi7cqsZS"
   },
   "source": [
    "## Generate dummy classification data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "6ZWjuOVLqsZT",
    "outputId": "576d8f21-4519-419b-eb82-5a9fbdc2bded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 20)\n",
      "(200, 20)\n",
      "(800,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# Generate dummy data\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = datasets.make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=10)\n",
    "x = data[0]\n",
    "y = data[1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCxFIht1qsZg"
   },
   "source": [
    "## Create model\n",
    "\n",
    "Create a two-layers MLP (1 hidden layer) with \n",
    "- 32 hidden units\n",
    "- the first activation function as ReLu \n",
    "- which output activation should we use for multi-class classification ?\n",
    "- the network should have has many outputs as classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "73v5k1zaqsZi",
    "outputId": "54904b1d-6b70-4053-e703-a505d742267d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                672       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 1,002\n",
      "Trainable params: 1,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# START CODE HERE\n",
    "from keras.utils.np_utils import to_categorical\n",
    "keras.backend.clear_session()\n",
    "model=Sequential()\n",
    "model.add(Dense(32,activation='relu',input_dim=20))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "model.summary()\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZEm95ySCqsZm"
   },
   "source": [
    "## Define loss (to minimize) and optimizer (how to update the weights)\n",
    "\n",
    "Configures the model for training: \n",
    "- define an ```optimizer``` (we will use ```sgd``` with a learn_rate (lr) or 0.01), \n",
    "- define the ```loss```to be minimized (which loss should be used for a binary classification problem ?)\n",
    "- define a list of ```metrics```to be displayed after each epoch (here we will use ```accuracy```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NyLdmVpaqsZo"
   },
   "outputs": [],
   "source": [
    "# START CODE HERE\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOsmpZd1qsZu"
   },
   "source": [
    "## Converting ground-truth output values to one-hot-vector\n",
    "\n",
    "So far $y^{(i)}$ represent labels (values between 0 and 9); we need to convert it to one-hot-vector encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yc8s-d_CqsZw"
   },
   "outputs": [],
   "source": [
    "# START CODE HERE\n",
    "\n",
    "y_train_oh=to_categorical(y_train,num_classes=10)\n",
    "y_test_oh =to_categorical(y_test,num_classes=10)\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6_I2d1SqsZ_"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Use \n",
    "- batches of size 32 \n",
    "- iterate for 200 epochs.\n",
    "\n",
    "Also evaluate the performances of the model and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7253
    },
    "colab_type": "code",
    "id": "BCRjF7BQrku9",
    "outputId": "de9f04fa-08da-4a18-b018-051d4f93eeee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 0s 186us/step - loss: 2.9775 - acc: 0.1050 - val_loss: 2.5050 - val_acc: 0.1950\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 55us/step - loss: 2.5827 - acc: 0.1363 - val_loss: 2.3148 - val_acc: 0.2200\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 49us/step - loss: 2.3667 - acc: 0.1638 - val_loss: 2.2111 - val_acc: 0.2350\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 2.2287 - acc: 0.2012 - val_loss: 2.1454 - val_acc: 0.2700\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 2.1326 - acc: 0.2275 - val_loss: 2.0989 - val_acc: 0.2800\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 2.0599 - acc: 0.2550 - val_loss: 2.0615 - val_acc: 0.2900\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 2.0010 - acc: 0.2750 - val_loss: 2.0301 - val_acc: 0.3050\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 1.9506 - acc: 0.2812 - val_loss: 2.0042 - val_acc: 0.3100\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.9072 - acc: 0.3050 - val_loss: 1.9806 - val_acc: 0.3250\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 52us/step - loss: 1.8685 - acc: 0.3238 - val_loss: 1.9585 - val_acc: 0.3350\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.8339 - acc: 0.3375 - val_loss: 1.9382 - val_acc: 0.3500\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 1.8024 - acc: 0.3488 - val_loss: 1.9198 - val_acc: 0.3650\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 1.7742 - acc: 0.3600 - val_loss: 1.9035 - val_acc: 0.3800\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.7475 - acc: 0.3737 - val_loss: 1.8877 - val_acc: 0.3800\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 1.7242 - acc: 0.3900 - val_loss: 1.8740 - val_acc: 0.3850\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.7023 - acc: 0.4025 - val_loss: 1.8613 - val_acc: 0.3850\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.6817 - acc: 0.4175 - val_loss: 1.8492 - val_acc: 0.3900\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 1.6634 - acc: 0.4313 - val_loss: 1.8393 - val_acc: 0.3900\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 1.6454 - acc: 0.4400 - val_loss: 1.8296 - val_acc: 0.4000\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 1.6292 - acc: 0.4400 - val_loss: 1.8187 - val_acc: 0.3950\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 1.6139 - acc: 0.4512 - val_loss: 1.8104 - val_acc: 0.4000\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 1.5989 - acc: 0.4575 - val_loss: 1.8008 - val_acc: 0.4050\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 1.5846 - acc: 0.4550 - val_loss: 1.7940 - val_acc: 0.4050\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 1.5723 - acc: 0.4637 - val_loss: 1.7865 - val_acc: 0.4100\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 48us/step - loss: 1.5590 - acc: 0.4700 - val_loss: 1.7798 - val_acc: 0.4150\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 51us/step - loss: 1.5477 - acc: 0.4788 - val_loss: 1.7714 - val_acc: 0.4200\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.5358 - acc: 0.4812 - val_loss: 1.7670 - val_acc: 0.4150\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 50us/step - loss: 1.5255 - acc: 0.4812 - val_loss: 1.7616 - val_acc: 0.4250\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.5142 - acc: 0.4925 - val_loss: 1.7576 - val_acc: 0.4250\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 1.5034 - acc: 0.4950 - val_loss: 1.7523 - val_acc: 0.4300\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.4933 - acc: 0.5050 - val_loss: 1.7460 - val_acc: 0.4350\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 1.4841 - acc: 0.5062 - val_loss: 1.7406 - val_acc: 0.4350\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.4748 - acc: 0.5100 - val_loss: 1.7370 - val_acc: 0.4300\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 1.4654 - acc: 0.5162 - val_loss: 1.7323 - val_acc: 0.4350\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.4564 - acc: 0.5237 - val_loss: 1.7289 - val_acc: 0.4400\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.4477 - acc: 0.5275 - val_loss: 1.7262 - val_acc: 0.4400\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 1.4391 - acc: 0.5250 - val_loss: 1.7209 - val_acc: 0.4400\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.4306 - acc: 0.5300 - val_loss: 1.7174 - val_acc: 0.4400\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.4232 - acc: 0.5337 - val_loss: 1.7140 - val_acc: 0.4450\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 1.4151 - acc: 0.5312 - val_loss: 1.7115 - val_acc: 0.4450\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 1.4068 - acc: 0.5375 - val_loss: 1.7104 - val_acc: 0.4400\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 49us/step - loss: 1.4002 - acc: 0.5363 - val_loss: 1.7048 - val_acc: 0.4400\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.3916 - acc: 0.5388 - val_loss: 1.7022 - val_acc: 0.4450\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.3845 - acc: 0.5388 - val_loss: 1.6965 - val_acc: 0.4450\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.3769 - acc: 0.5463 - val_loss: 1.6934 - val_acc: 0.4450\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.3703 - acc: 0.5450 - val_loss: 1.6901 - val_acc: 0.4450\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 1.3628 - acc: 0.5550 - val_loss: 1.6867 - val_acc: 0.4450\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.3563 - acc: 0.5538 - val_loss: 1.6832 - val_acc: 0.4450\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 1.3492 - acc: 0.5600 - val_loss: 1.6819 - val_acc: 0.4450\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.3422 - acc: 0.5587 - val_loss: 1.6755 - val_acc: 0.4450\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 1.3368 - acc: 0.5650 - val_loss: 1.6736 - val_acc: 0.4450\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.3291 - acc: 0.5625 - val_loss: 1.6719 - val_acc: 0.4550\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.3229 - acc: 0.5687 - val_loss: 1.6687 - val_acc: 0.4550\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 1.3176 - acc: 0.5687 - val_loss: 1.6670 - val_acc: 0.4500\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 1.3112 - acc: 0.5687 - val_loss: 1.6638 - val_acc: 0.4550\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.3055 - acc: 0.5763 - val_loss: 1.6607 - val_acc: 0.4500\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.2995 - acc: 0.5687 - val_loss: 1.6587 - val_acc: 0.4500\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.2937 - acc: 0.5775 - val_loss: 1.6547 - val_acc: 0.4600\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.2878 - acc: 0.5813 - val_loss: 1.6534 - val_acc: 0.4550\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 1.2828 - acc: 0.5825 - val_loss: 1.6492 - val_acc: 0.4750\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 1.2759 - acc: 0.5850 - val_loss: 1.6470 - val_acc: 0.4750\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.2716 - acc: 0.5837 - val_loss: 1.6465 - val_acc: 0.4700\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.2658 - acc: 0.5813 - val_loss: 1.6454 - val_acc: 0.4700\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.2605 - acc: 0.5875 - val_loss: 1.6436 - val_acc: 0.4750\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.2545 - acc: 0.5900 - val_loss: 1.6413 - val_acc: 0.4750\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.2489 - acc: 0.5913 - val_loss: 1.6400 - val_acc: 0.4750\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.2443 - acc: 0.5988 - val_loss: 1.6380 - val_acc: 0.4800\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.2386 - acc: 0.6013 - val_loss: 1.6336 - val_acc: 0.4850\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.2334 - acc: 0.6025 - val_loss: 1.6331 - val_acc: 0.4850\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.2279 - acc: 0.6038 - val_loss: 1.6300 - val_acc: 0.4900\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.2228 - acc: 0.6100 - val_loss: 1.6284 - val_acc: 0.4850\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 1.2175 - acc: 0.6150 - val_loss: 1.6282 - val_acc: 0.4900\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.2126 - acc: 0.6150 - val_loss: 1.6270 - val_acc: 0.4950\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 1.2070 - acc: 0.6138 - val_loss: 1.6265 - val_acc: 0.4900\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 49us/step - loss: 1.2032 - acc: 0.6187 - val_loss: 1.6247 - val_acc: 0.4900\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 55us/step - loss: 1.1978 - acc: 0.6238 - val_loss: 1.6218 - val_acc: 0.4950\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.1931 - acc: 0.6288 - val_loss: 1.6230 - val_acc: 0.4850\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 48us/step - loss: 1.1882 - acc: 0.6313 - val_loss: 1.6208 - val_acc: 0.4800\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.1834 - acc: 0.6350 - val_loss: 1.6179 - val_acc: 0.4850\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 1.1787 - acc: 0.6350 - val_loss: 1.6170 - val_acc: 0.4850\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.1740 - acc: 0.6388 - val_loss: 1.6197 - val_acc: 0.4800\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 1.1696 - acc: 0.6362 - val_loss: 1.6173 - val_acc: 0.4800\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 1.1655 - acc: 0.6412 - val_loss: 1.6168 - val_acc: 0.4800\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.1609 - acc: 0.6388 - val_loss: 1.6147 - val_acc: 0.4850\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.1564 - acc: 0.6438 - val_loss: 1.6147 - val_acc: 0.4800\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 1.1516 - acc: 0.6375 - val_loss: 1.6150 - val_acc: 0.4850\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.1478 - acc: 0.6450 - val_loss: 1.6122 - val_acc: 0.4850\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.1438 - acc: 0.6450 - val_loss: 1.6131 - val_acc: 0.4800\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.1401 - acc: 0.6425 - val_loss: 1.6154 - val_acc: 0.4750\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.1355 - acc: 0.6487 - val_loss: 1.6131 - val_acc: 0.4750\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 1.1316 - acc: 0.6475 - val_loss: 1.6119 - val_acc: 0.4750\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.1273 - acc: 0.6537 - val_loss: 1.6127 - val_acc: 0.4750\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.1239 - acc: 0.6450 - val_loss: 1.6123 - val_acc: 0.4700\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 1.1202 - acc: 0.6500 - val_loss: 1.6103 - val_acc: 0.4700\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.1152 - acc: 0.6550 - val_loss: 1.6082 - val_acc: 0.4700\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.1117 - acc: 0.6525 - val_loss: 1.6097 - val_acc: 0.4650\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 1.1080 - acc: 0.6525 - val_loss: 1.6093 - val_acc: 0.4750\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.1043 - acc: 0.6512 - val_loss: 1.6090 - val_acc: 0.4700\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 1.1001 - acc: 0.6537 - val_loss: 1.6104 - val_acc: 0.4650\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0974 - acc: 0.6550 - val_loss: 1.6098 - val_acc: 0.4600\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.0932 - acc: 0.6550 - val_loss: 1.6066 - val_acc: 0.4700\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 1.0890 - acc: 0.6575 - val_loss: 1.6084 - val_acc: 0.4650\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0865 - acc: 0.6600 - val_loss: 1.6066 - val_acc: 0.4600\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.0828 - acc: 0.6562 - val_loss: 1.6092 - val_acc: 0.4550\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0790 - acc: 0.6638 - val_loss: 1.6084 - val_acc: 0.4600\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0755 - acc: 0.6550 - val_loss: 1.6079 - val_acc: 0.4600\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0721 - acc: 0.6613 - val_loss: 1.6062 - val_acc: 0.4600\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0689 - acc: 0.6575 - val_loss: 1.6064 - val_acc: 0.4550\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0660 - acc: 0.6600 - val_loss: 1.6024 - val_acc: 0.4600\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0622 - acc: 0.6625 - val_loss: 1.6021 - val_acc: 0.4650\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 1.0597 - acc: 0.6663 - val_loss: 1.6040 - val_acc: 0.4600\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.0563 - acc: 0.6625 - val_loss: 1.6052 - val_acc: 0.4700\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 1.0529 - acc: 0.6638 - val_loss: 1.6021 - val_acc: 0.4650\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0495 - acc: 0.6625 - val_loss: 1.6057 - val_acc: 0.4650\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.0462 - acc: 0.6638 - val_loss: 1.6078 - val_acc: 0.4650\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0432 - acc: 0.6663 - val_loss: 1.6093 - val_acc: 0.4650\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0401 - acc: 0.6675 - val_loss: 1.6071 - val_acc: 0.4750\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0370 - acc: 0.6700 - val_loss: 1.6059 - val_acc: 0.4700\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0347 - acc: 0.6650 - val_loss: 1.6097 - val_acc: 0.4700\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0307 - acc: 0.6687 - val_loss: 1.6076 - val_acc: 0.4700\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 1.0275 - acc: 0.6687 - val_loss: 1.6060 - val_acc: 0.4700\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 1.0252 - acc: 0.6713 - val_loss: 1.6054 - val_acc: 0.4700\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 1.0214 - acc: 0.6675 - val_loss: 1.6061 - val_acc: 0.4750\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0188 - acc: 0.6700 - val_loss: 1.6070 - val_acc: 0.4750\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 1.0159 - acc: 0.6687 - val_loss: 1.6105 - val_acc: 0.4800\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0126 - acc: 0.6737 - val_loss: 1.6115 - val_acc: 0.4700\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 52us/step - loss: 1.0102 - acc: 0.6737 - val_loss: 1.6106 - val_acc: 0.4700\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 1.0071 - acc: 0.6763 - val_loss: 1.6103 - val_acc: 0.4750\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 1.0044 - acc: 0.6788 - val_loss: 1.6093 - val_acc: 0.4750\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 1.0019 - acc: 0.6750 - val_loss: 1.6082 - val_acc: 0.4700\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.9986 - acc: 0.6837 - val_loss: 1.6094 - val_acc: 0.4750\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 54us/step - loss: 0.9957 - acc: 0.6788 - val_loss: 1.6057 - val_acc: 0.4700\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.9932 - acc: 0.6775 - val_loss: 1.6064 - val_acc: 0.4750\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.9909 - acc: 0.6788 - val_loss: 1.6076 - val_acc: 0.4750\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.9880 - acc: 0.6812 - val_loss: 1.6083 - val_acc: 0.4750\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.9848 - acc: 0.6825 - val_loss: 1.6093 - val_acc: 0.4800\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.9823 - acc: 0.6837 - val_loss: 1.6090 - val_acc: 0.4800\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.9795 - acc: 0.6887 - val_loss: 1.6112 - val_acc: 0.4800\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.9774 - acc: 0.6875 - val_loss: 1.6104 - val_acc: 0.4800\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.9740 - acc: 0.6850 - val_loss: 1.6126 - val_acc: 0.4800\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.9716 - acc: 0.6913 - val_loss: 1.6091 - val_acc: 0.4800\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.9690 - acc: 0.6925 - val_loss: 1.6105 - val_acc: 0.4800\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.9667 - acc: 0.6887 - val_loss: 1.6105 - val_acc: 0.4800\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.9640 - acc: 0.6925 - val_loss: 1.6134 - val_acc: 0.4850\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.9618 - acc: 0.6913 - val_loss: 1.6120 - val_acc: 0.4850\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.9591 - acc: 0.6913 - val_loss: 1.6105 - val_acc: 0.4850\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.9568 - acc: 0.6900 - val_loss: 1.6102 - val_acc: 0.4750\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.9547 - acc: 0.6950 - val_loss: 1.6132 - val_acc: 0.4850\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.9517 - acc: 0.6962 - val_loss: 1.6151 - val_acc: 0.4800\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.9500 - acc: 0.6938 - val_loss: 1.6164 - val_acc: 0.4700\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.9469 - acc: 0.6975 - val_loss: 1.6192 - val_acc: 0.4700\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.9441 - acc: 0.6975 - val_loss: 1.6166 - val_acc: 0.4800\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.9416 - acc: 0.7000 - val_loss: 1.6178 - val_acc: 0.4750\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.9391 - acc: 0.7000 - val_loss: 1.6164 - val_acc: 0.4800\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.9372 - acc: 0.7013 - val_loss: 1.6143 - val_acc: 0.4800\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.9344 - acc: 0.7013 - val_loss: 1.6157 - val_acc: 0.4800\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.9325 - acc: 0.6975 - val_loss: 1.6160 - val_acc: 0.4800\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.9296 - acc: 0.7050 - val_loss: 1.6176 - val_acc: 0.4750\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.9274 - acc: 0.7063 - val_loss: 1.6188 - val_acc: 0.4800\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.9249 - acc: 0.7050 - val_loss: 1.6191 - val_acc: 0.4800\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.9222 - acc: 0.7087 - val_loss: 1.6231 - val_acc: 0.4800\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.9201 - acc: 0.7050 - val_loss: 1.6182 - val_acc: 0.4800\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.9183 - acc: 0.7050 - val_loss: 1.6219 - val_acc: 0.4800\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.9159 - acc: 0.7075 - val_loss: 1.6217 - val_acc: 0.4850\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.9141 - acc: 0.7087 - val_loss: 1.6212 - val_acc: 0.4800\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.9108 - acc: 0.7063 - val_loss: 1.6238 - val_acc: 0.4850\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.9086 - acc: 0.7112 - val_loss: 1.6248 - val_acc: 0.4800\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.9066 - acc: 0.7063 - val_loss: 1.6264 - val_acc: 0.4800\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.9041 - acc: 0.7075 - val_loss: 1.6241 - val_acc: 0.4850\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.9023 - acc: 0.7100 - val_loss: 1.6253 - val_acc: 0.4850\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.9004 - acc: 0.7112 - val_loss: 1.6277 - val_acc: 0.4850\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.8978 - acc: 0.7125 - val_loss: 1.6267 - val_acc: 0.4850\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.8953 - acc: 0.7200 - val_loss: 1.6320 - val_acc: 0.4850\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.8926 - acc: 0.7162 - val_loss: 1.6333 - val_acc: 0.4900\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.8911 - acc: 0.7188 - val_loss: 1.6325 - val_acc: 0.4850\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.8890 - acc: 0.7162 - val_loss: 1.6325 - val_acc: 0.4800\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.8868 - acc: 0.7213 - val_loss: 1.6336 - val_acc: 0.4800\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.8843 - acc: 0.7250 - val_loss: 1.6318 - val_acc: 0.4850\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.8825 - acc: 0.7238 - val_loss: 1.6352 - val_acc: 0.4800\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.8802 - acc: 0.7213 - val_loss: 1.6365 - val_acc: 0.4850\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.8779 - acc: 0.7238 - val_loss: 1.6409 - val_acc: 0.4850\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 48us/step - loss: 0.8752 - acc: 0.7250 - val_loss: 1.6368 - val_acc: 0.4850\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.8737 - acc: 0.7225 - val_loss: 1.6397 - val_acc: 0.4850\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.8719 - acc: 0.7263 - val_loss: 1.6420 - val_acc: 0.4900\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.8698 - acc: 0.7300 - val_loss: 1.6436 - val_acc: 0.4900\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.8677 - acc: 0.7275 - val_loss: 1.6447 - val_acc: 0.4850\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.8655 - acc: 0.7300 - val_loss: 1.6426 - val_acc: 0.4950\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.8633 - acc: 0.7288 - val_loss: 1.6443 - val_acc: 0.5000\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.8611 - acc: 0.7338 - val_loss: 1.6452 - val_acc: 0.4950\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.8597 - acc: 0.7288 - val_loss: 1.6476 - val_acc: 0.4900\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.8574 - acc: 0.7325 - val_loss: 1.6485 - val_acc: 0.4950\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.8550 - acc: 0.7362 - val_loss: 1.6467 - val_acc: 0.4950\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.8533 - acc: 0.7375 - val_loss: 1.6481 - val_acc: 0.5050\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.8509 - acc: 0.7400 - val_loss: 1.6496 - val_acc: 0.5050\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.8489 - acc: 0.7375 - val_loss: 1.6504 - val_acc: 0.5000\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.8472 - acc: 0.7288 - val_loss: 1.6527 - val_acc: 0.4950\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.8447 - acc: 0.7413 - val_loss: 1.6541 - val_acc: 0.4950\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.8422 - acc: 0.7388 - val_loss: 1.6560 - val_acc: 0.5050\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.8402 - acc: 0.7388 - val_loss: 1.6544 - val_acc: 0.5100\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 43us/step - loss: 0.8390 - acc: 0.7338 - val_loss: 1.6547 - val_acc: 0.5050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa96a6dcbe0>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train_oh,batch_size=32,epochs=200,verbose=1,validation_data=(x_test,y_test_oh))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-1DfIX4qsaC",
    "outputId": "6b78027f-881d-4346-947f-893c1ea4d89b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 0s 214us/step - loss: 2.9195 - acc: 0.0788 - val_loss: 2.7708 - val_acc: 0.0900\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 2.6210 - acc: 0.1175 - val_loss: 2.5678 - val_acc: 0.1050\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 2.4529 - acc: 0.1363 - val_loss: 2.4415 - val_acc: 0.1250\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 2.3418 - acc: 0.1600 - val_loss: 2.3572 - val_acc: 0.1450\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 2.2604 - acc: 0.1800 - val_loss: 2.2937 - val_acc: 0.1750\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 2.1956 - acc: 0.1975 - val_loss: 2.2453 - val_acc: 0.1800\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 2.1428 - acc: 0.2225 - val_loss: 2.2045 - val_acc: 0.2050\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 2.0950 - acc: 0.2363 - val_loss: 2.1682 - val_acc: 0.2100\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 2.0542 - acc: 0.2525 - val_loss: 2.1369 - val_acc: 0.2200\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 2.0180 - acc: 0.2650 - val_loss: 2.1088 - val_acc: 0.2250\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.9853 - acc: 0.2812 - val_loss: 2.0851 - val_acc: 0.2350\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.9559 - acc: 0.2925 - val_loss: 2.0623 - val_acc: 0.2450\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.9283 - acc: 0.3087 - val_loss: 2.0446 - val_acc: 0.2500\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.9034 - acc: 0.3125 - val_loss: 2.0239 - val_acc: 0.2700\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.8800 - acc: 0.3225 - val_loss: 2.0086 - val_acc: 0.2700\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.8584 - acc: 0.3262 - val_loss: 1.9926 - val_acc: 0.2700\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.8381 - acc: 0.3313 - val_loss: 1.9775 - val_acc: 0.2700\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.8179 - acc: 0.3362 - val_loss: 1.9624 - val_acc: 0.2900\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.8003 - acc: 0.3475 - val_loss: 1.9497 - val_acc: 0.2900\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.7834 - acc: 0.3500 - val_loss: 1.9381 - val_acc: 0.3000\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.7666 - acc: 0.3538 - val_loss: 1.9264 - val_acc: 0.3050\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.7507 - acc: 0.3688 - val_loss: 1.9154 - val_acc: 0.3100\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.7359 - acc: 0.3788 - val_loss: 1.9054 - val_acc: 0.3100\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.7217 - acc: 0.3887 - val_loss: 1.8957 - val_acc: 0.3100\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.7079 - acc: 0.3962 - val_loss: 1.8846 - val_acc: 0.3100\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.6938 - acc: 0.4075 - val_loss: 1.8765 - val_acc: 0.3200\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 1.6814 - acc: 0.4075 - val_loss: 1.8674 - val_acc: 0.3200\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 1.6695 - acc: 0.4150 - val_loss: 1.8594 - val_acc: 0.3200\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 1.6567 - acc: 0.4225 - val_loss: 1.8502 - val_acc: 0.3250\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.6455 - acc: 0.4250 - val_loss: 1.8419 - val_acc: 0.3250\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.6338 - acc: 0.4313 - val_loss: 1.8327 - val_acc: 0.3350\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.6232 - acc: 0.4337 - val_loss: 1.8260 - val_acc: 0.3500\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 1.6119 - acc: 0.4375 - val_loss: 1.8198 - val_acc: 0.3650\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.6020 - acc: 0.4462 - val_loss: 1.8124 - val_acc: 0.3650\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 1.5915 - acc: 0.4475 - val_loss: 1.8060 - val_acc: 0.3750\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.5823 - acc: 0.4450 - val_loss: 1.7979 - val_acc: 0.3750\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 1.5726 - acc: 0.4525 - val_loss: 1.7913 - val_acc: 0.3850\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.5632 - acc: 0.4600 - val_loss: 1.7839 - val_acc: 0.3850\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.5536 - acc: 0.4587 - val_loss: 1.7779 - val_acc: 0.3750\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.5439 - acc: 0.4650 - val_loss: 1.7727 - val_acc: 0.3750\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.5355 - acc: 0.4700 - val_loss: 1.7660 - val_acc: 0.3900\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.5266 - acc: 0.4738 - val_loss: 1.7620 - val_acc: 0.3800\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.5188 - acc: 0.4738 - val_loss: 1.7563 - val_acc: 0.3750\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.5097 - acc: 0.4763 - val_loss: 1.7488 - val_acc: 0.3900\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - ETA: 0s - loss: 1.5699 - acc: 0.500 - 0s 27us/step - loss: 1.5019 - acc: 0.4825 - val_loss: 1.7443 - val_acc: 0.3900\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.4934 - acc: 0.4838 - val_loss: 1.7392 - val_acc: 0.3750\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.4863 - acc: 0.4900 - val_loss: 1.7347 - val_acc: 0.3850\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.4780 - acc: 0.4925 - val_loss: 1.7304 - val_acc: 0.3800\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.4703 - acc: 0.4925 - val_loss: 1.7245 - val_acc: 0.3900\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.4627 - acc: 0.5012 - val_loss: 1.7204 - val_acc: 0.3950\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.4560 - acc: 0.5000 - val_loss: 1.7165 - val_acc: 0.3950\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.4481 - acc: 0.5025 - val_loss: 1.7125 - val_acc: 0.3800\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.4411 - acc: 0.5075 - val_loss: 1.7073 - val_acc: 0.3750\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.4336 - acc: 0.5138 - val_loss: 1.7062 - val_acc: 0.3800\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.4265 - acc: 0.5237 - val_loss: 1.7030 - val_acc: 0.3950\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.4196 - acc: 0.5188 - val_loss: 1.6992 - val_acc: 0.4100\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.4135 - acc: 0.5188 - val_loss: 1.6949 - val_acc: 0.4100\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.4067 - acc: 0.5250 - val_loss: 1.6906 - val_acc: 0.4200\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.3993 - acc: 0.5250 - val_loss: 1.6884 - val_acc: 0.4100\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 1.3934 - acc: 0.5312 - val_loss: 1.6865 - val_acc: 0.4150\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.3855 - acc: 0.5363 - val_loss: 1.6843 - val_acc: 0.4150\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.3794 - acc: 0.5375 - val_loss: 1.6771 - val_acc: 0.4300\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.3734 - acc: 0.5312 - val_loss: 1.6738 - val_acc: 0.4200\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.3675 - acc: 0.5350 - val_loss: 1.6700 - val_acc: 0.4250\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.3607 - acc: 0.5400 - val_loss: 1.6670 - val_acc: 0.4350\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.3558 - acc: 0.5388 - val_loss: 1.6643 - val_acc: 0.4250\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.3486 - acc: 0.5437 - val_loss: 1.6637 - val_acc: 0.4300\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.3432 - acc: 0.5525 - val_loss: 1.6613 - val_acc: 0.4300\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.3372 - acc: 0.5450 - val_loss: 1.6589 - val_acc: 0.4300\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.3318 - acc: 0.5637 - val_loss: 1.6572 - val_acc: 0.4400\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.3267 - acc: 0.5525 - val_loss: 1.6551 - val_acc: 0.4450\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 1.3212 - acc: 0.5563 - val_loss: 1.6528 - val_acc: 0.4500\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 24us/step - loss: 1.3164 - acc: 0.5650 - val_loss: 1.6505 - val_acc: 0.4500\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 24us/step - loss: 1.3101 - acc: 0.5675 - val_loss: 1.6468 - val_acc: 0.4500\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.3057 - acc: 0.5687 - val_loss: 1.6439 - val_acc: 0.4400\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 1.3009 - acc: 0.5737 - val_loss: 1.6412 - val_acc: 0.4550\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 1.2958 - acc: 0.5663 - val_loss: 1.6402 - val_acc: 0.4600\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 1.2900 - acc: 0.5763 - val_loss: 1.6400 - val_acc: 0.4550\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.2854 - acc: 0.5725 - val_loss: 1.6370 - val_acc: 0.4600\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.2809 - acc: 0.5750 - val_loss: 1.6343 - val_acc: 0.4650\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.2754 - acc: 0.5725 - val_loss: 1.6326 - val_acc: 0.4600\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.2705 - acc: 0.5775 - val_loss: 1.6321 - val_acc: 0.4650\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.2660 - acc: 0.5737 - val_loss: 1.6311 - val_acc: 0.4650\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.2617 - acc: 0.5837 - val_loss: 1.6286 - val_acc: 0.4600\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.2568 - acc: 0.5837 - val_loss: 1.6255 - val_acc: 0.4700\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.2522 - acc: 0.5825 - val_loss: 1.6251 - val_acc: 0.4650\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.2475 - acc: 0.5787 - val_loss: 1.6236 - val_acc: 0.4650\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.2436 - acc: 0.5837 - val_loss: 1.6215 - val_acc: 0.4650\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.2382 - acc: 0.5925 - val_loss: 1.6210 - val_acc: 0.4650\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.2339 - acc: 0.5962 - val_loss: 1.6185 - val_acc: 0.4650\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.2298 - acc: 0.5900 - val_loss: 1.6168 - val_acc: 0.4700\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.2255 - acc: 0.5900 - val_loss: 1.6158 - val_acc: 0.4750\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.2212 - acc: 0.5938 - val_loss: 1.6159 - val_acc: 0.4850\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.2175 - acc: 0.5938 - val_loss: 1.6153 - val_acc: 0.4750\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.2121 - acc: 0.5900 - val_loss: 1.6140 - val_acc: 0.4850\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.2082 - acc: 0.5975 - val_loss: 1.6109 - val_acc: 0.5000\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.2038 - acc: 0.5988 - val_loss: 1.6076 - val_acc: 0.4850\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.2002 - acc: 0.6000 - val_loss: 1.6067 - val_acc: 0.4950\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1961 - acc: 0.6062 - val_loss: 1.6039 - val_acc: 0.4950\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1916 - acc: 0.6062 - val_loss: 1.6034 - val_acc: 0.5050\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1879 - acc: 0.6062 - val_loss: 1.6015 - val_acc: 0.5050\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1845 - acc: 0.6025 - val_loss: 1.5999 - val_acc: 0.5100\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1803 - acc: 0.6125 - val_loss: 1.5992 - val_acc: 0.5050\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.1764 - acc: 0.6088 - val_loss: 1.5965 - val_acc: 0.5050\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1730 - acc: 0.6062 - val_loss: 1.5968 - val_acc: 0.5050\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1692 - acc: 0.6150 - val_loss: 1.5973 - val_acc: 0.5050\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.1659 - acc: 0.6088 - val_loss: 1.5971 - val_acc: 0.5100\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1618 - acc: 0.6100 - val_loss: 1.5957 - val_acc: 0.5150\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.1586 - acc: 0.6088 - val_loss: 1.5947 - val_acc: 0.5050\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1549 - acc: 0.6100 - val_loss: 1.5935 - val_acc: 0.5050\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1510 - acc: 0.6212 - val_loss: 1.5936 - val_acc: 0.5000\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.1476 - acc: 0.6187 - val_loss: 1.5917 - val_acc: 0.5050\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.1443 - acc: 0.6238 - val_loss: 1.5907 - val_acc: 0.5050\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1404 - acc: 0.6238 - val_loss: 1.5878 - val_acc: 0.5000\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1372 - acc: 0.6238 - val_loss: 1.5874 - val_acc: 0.5050\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1334 - acc: 0.6288 - val_loss: 1.5834 - val_acc: 0.5000\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1303 - acc: 0.6288 - val_loss: 1.5832 - val_acc: 0.5000\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1269 - acc: 0.6275 - val_loss: 1.5835 - val_acc: 0.5050\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.1230 - acc: 0.6325 - val_loss: 1.5832 - val_acc: 0.5000\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.1196 - acc: 0.6362 - val_loss: 1.5828 - val_acc: 0.5000\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 25us/step - loss: 1.1167 - acc: 0.6400 - val_loss: 1.5815 - val_acc: 0.5050\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 1.1132 - acc: 0.6362 - val_loss: 1.5789 - val_acc: 0.5050\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1103 - acc: 0.6400 - val_loss: 1.5764 - val_acc: 0.5050\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1064 - acc: 0.6362 - val_loss: 1.5766 - val_acc: 0.5050\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1042 - acc: 0.6438 - val_loss: 1.5764 - val_acc: 0.5050\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 1.1011 - acc: 0.6337 - val_loss: 1.5773 - val_acc: 0.5050\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 1.0976 - acc: 0.6450 - val_loss: 1.5764 - val_acc: 0.5000\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 1.0947 - acc: 0.6475 - val_loss: 1.5769 - val_acc: 0.5050\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.0913 - acc: 0.6462 - val_loss: 1.5754 - val_acc: 0.5000\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0881 - acc: 0.6412 - val_loss: 1.5765 - val_acc: 0.5000\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0852 - acc: 0.6562 - val_loss: 1.5726 - val_acc: 0.4950\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.0817 - acc: 0.6462 - val_loss: 1.5729 - val_acc: 0.4950\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.0782 - acc: 0.6438 - val_loss: 1.5724 - val_acc: 0.5050\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.0751 - acc: 0.6475 - val_loss: 1.5744 - val_acc: 0.5000\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.0724 - acc: 0.6525 - val_loss: 1.5701 - val_acc: 0.5000\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0699 - acc: 0.6537 - val_loss: 1.5718 - val_acc: 0.4950\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0673 - acc: 0.6550 - val_loss: 1.5701 - val_acc: 0.4950\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0642 - acc: 0.6575 - val_loss: 1.5711 - val_acc: 0.4950\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0612 - acc: 0.6588 - val_loss: 1.5698 - val_acc: 0.4950\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.0586 - acc: 0.6525 - val_loss: 1.5698 - val_acc: 0.5000\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0555 - acc: 0.6613 - val_loss: 1.5688 - val_acc: 0.4950\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.0525 - acc: 0.6588 - val_loss: 1.5695 - val_acc: 0.4950\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0508 - acc: 0.6625 - val_loss: 1.5670 - val_acc: 0.4900\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0483 - acc: 0.6575 - val_loss: 1.5673 - val_acc: 0.4900\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0442 - acc: 0.6625 - val_loss: 1.5673 - val_acc: 0.4900\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.0422 - acc: 0.6600 - val_loss: 1.5647 - val_acc: 0.4950\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0395 - acc: 0.6575 - val_loss: 1.5656 - val_acc: 0.5000\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.0372 - acc: 0.6687 - val_loss: 1.5662 - val_acc: 0.4900\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0349 - acc: 0.6675 - val_loss: 1.5666 - val_acc: 0.4950\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0321 - acc: 0.6687 - val_loss: 1.5663 - val_acc: 0.5000\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 1.0290 - acc: 0.6687 - val_loss: 1.5658 - val_acc: 0.5000\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0259 - acc: 0.6700 - val_loss: 1.5650 - val_acc: 0.5000\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0231 - acc: 0.6750 - val_loss: 1.5645 - val_acc: 0.5000\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.0213 - acc: 0.6700 - val_loss: 1.5666 - val_acc: 0.4950\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.0190 - acc: 0.6825 - val_loss: 1.5653 - val_acc: 0.4900\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.0154 - acc: 0.6775 - val_loss: 1.5666 - val_acc: 0.4950\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0141 - acc: 0.6800 - val_loss: 1.5659 - val_acc: 0.4900\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0116 - acc: 0.6825 - val_loss: 1.5643 - val_acc: 0.4950\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.0096 - acc: 0.6850 - val_loss: 1.5632 - val_acc: 0.5000\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.0060 - acc: 0.6775 - val_loss: 1.5639 - val_acc: 0.5000\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - ETA: 0s - loss: 0.7667 - acc: 0.812 - 0s 26us/step - loss: 1.0041 - acc: 0.6863 - val_loss: 1.5628 - val_acc: 0.4900\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.0010 - acc: 0.6875 - val_loss: 1.5639 - val_acc: 0.5000\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9978 - acc: 0.6837 - val_loss: 1.5658 - val_acc: 0.4850\n",
      "Epoch 164/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 0.9959 - acc: 0.6875 - val_loss: 1.5641 - val_acc: 0.4850\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 24us/step - loss: 0.9944 - acc: 0.6938 - val_loss: 1.5640 - val_acc: 0.4800\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 24us/step - loss: 0.9914 - acc: 0.6887 - val_loss: 1.5628 - val_acc: 0.4850\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9895 - acc: 0.6900 - val_loss: 1.5658 - val_acc: 0.4800\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9869 - acc: 0.6975 - val_loss: 1.5656 - val_acc: 0.4850\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 0.9844 - acc: 0.6925 - val_loss: 1.5635 - val_acc: 0.4950\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 0.9824 - acc: 0.6913 - val_loss: 1.5622 - val_acc: 0.4950\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 0.9800 - acc: 0.6950 - val_loss: 1.5635 - val_acc: 0.4900\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 0.9779 - acc: 0.7000 - val_loss: 1.5649 - val_acc: 0.4950\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9768 - acc: 0.6950 - val_loss: 1.5660 - val_acc: 0.4900\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9735 - acc: 0.6925 - val_loss: 1.5685 - val_acc: 0.4950\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9720 - acc: 0.6938 - val_loss: 1.5673 - val_acc: 0.4950\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9706 - acc: 0.6975 - val_loss: 1.5655 - val_acc: 0.4950\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9685 - acc: 0.7100 - val_loss: 1.5645 - val_acc: 0.4950\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 0.9661 - acc: 0.6975 - val_loss: 1.5652 - val_acc: 0.4900\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 0.9637 - acc: 0.7000 - val_loss: 1.5660 - val_acc: 0.4900\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9615 - acc: 0.6950 - val_loss: 1.5663 - val_acc: 0.4850\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 27us/step - loss: 0.9598 - acc: 0.7025 - val_loss: 1.5629 - val_acc: 0.4900\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9584 - acc: 0.6987 - val_loss: 1.5646 - val_acc: 0.4900\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9552 - acc: 0.6962 - val_loss: 1.5649 - val_acc: 0.4950\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9542 - acc: 0.7063 - val_loss: 1.5666 - val_acc: 0.4950\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9523 - acc: 0.7100 - val_loss: 1.5710 - val_acc: 0.4950\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 0.9493 - acc: 0.7025 - val_loss: 1.5678 - val_acc: 0.4900\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 0.9481 - acc: 0.7112 - val_loss: 1.5666 - val_acc: 0.4900\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9470 - acc: 0.7112 - val_loss: 1.5706 - val_acc: 0.4900\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 0.9447 - acc: 0.7013 - val_loss: 1.5727 - val_acc: 0.4950\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 0.9424 - acc: 0.7075 - val_loss: 1.5707 - val_acc: 0.4850\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 0.9408 - acc: 0.7037 - val_loss: 1.5753 - val_acc: 0.4800\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 25us/step - loss: 0.9384 - acc: 0.7112 - val_loss: 1.5720 - val_acc: 0.4850\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9368 - acc: 0.7112 - val_loss: 1.5748 - val_acc: 0.4900\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9356 - acc: 0.7075 - val_loss: 1.5747 - val_acc: 0.4850\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 0.9334 - acc: 0.7112 - val_loss: 1.5761 - val_acc: 0.4900\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 0.9321 - acc: 0.7075 - val_loss: 1.5762 - val_acc: 0.4950\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9304 - acc: 0.7087 - val_loss: 1.5761 - val_acc: 0.4900\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.9287 - acc: 0.7112 - val_loss: 1.5779 - val_acc: 0.4950\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9265 - acc: 0.7100 - val_loss: 1.5736 - val_acc: 0.4800\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 26us/step - loss: 0.9257 - acc: 0.7087 - val_loss: 1.5756 - val_acc: 0.4900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x105426da0>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# START CODE HERE\n",
    "model.fit(x_train,y_train_oh,batch_size=32,epochs=200,verbose=1,validation_data=(x_test,y_test_oh))\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4H1x4qMfqsaO"
   },
   "source": [
    "# 4. On real data: reuters\n",
    "\n",
    "## Reuters newswire topics classification\n",
    "\n",
    "Reuters dataset is made of 11,228 newswires from Reuters, labeled over 46 topics. As with the IMDB dataset, each wire is encoded as a sequence of word indexes (same conventions).\n",
    "\n",
    "In this dataset, both inputs and outputs need to be encoded using one-hot-encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTWUv8F6qsaP"
   },
   "source": [
    "## Read the dataset and convert it to an \"easy to process\" format\n",
    "\n",
    "The code is provided but we ask you to explain it.\n",
    "\n",
    "### Question\n",
    "\n",
    "Explain what are the steps for converting the ```reuters.load_data``` to the correct format ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1063
    },
    "colab_type": "code",
    "id": "w1LAxfmwqsaQ",
    "outputId": "eafac802-2e2a-4b25-b76d-913658a7788c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "8982 train sequences\n",
      "2246 test sequences\n",
      "x_train type: <class 'numpy.ndarray'>\n",
      "x_train[0] type: <class 'list'>\n",
      "y_train type: <class 'numpy.ndarray'>\n",
      "x_train shape: (8982,)\n",
      "x_test shape: (2246,)\n",
      "x_train[0]: [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 2, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 2, 2, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 2, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
      "y_train[0]: 3\n",
      "46 classes\n",
      "Vectorizing sequence data...\n",
      "x_train shape: (8982, 1000)\n",
      "x_test shape: (2246, 1000)\n",
      "x_train[0]: [0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "y_train[0]: 3\n"
     ]
    }
   ],
   "source": [
    "'''Trains and evaluate a simple MLP\n",
    "on the Reuters newswire topic classification task.\n",
    "'''\n",
    "from keras.datasets import reuters\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_words = 1000\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.2)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('x_train type: {}'.format(type(x_train)))\n",
    "print(\"x_train[0] type: {}\".format(type(x_train[0])))\n",
    "print('y_train type: {}'.format(type(y_train)))\n",
    "\n",
    "print('x_train shape: {}'.format(x_train.shape))\n",
    "print('x_test shape: {}'.format(x_test.shape))\n",
    "\n",
    "print(\"x_train[0]: {}\".format(x_train[0]))\n",
    "print(\"y_train[0]: {}\".format(y_train[0]))\n",
    "\n",
    "\n",
    "word_index = reuters.get_word_index()\n",
    "#print(word_index)\n",
    "[word for word, index in word_index.items() if index == 2]\n",
    "index_word = {value: key for key, value in word_index.items()}\n",
    "\"-\".join([index_word[x] for x in x_train[0]])\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print(\"x_train[0]: {}\".format(x_train[0]))\n",
    "print(\"y_train[0]: {}\".format(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pCJHhAjZsTvA",
    "outputId": "1bd61b75-4a85-4d6e-8d5d-091d69724e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30979\n"
     ]
    }
   ],
   "source": [
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51pSoTWDqsaW"
   },
   "source": [
    "## Converting ground-truth output values to one-hot-vector\n",
    "\n",
    "So far  y(i) represent labels (values between 0 and 9); we need to convert it to one-hot-vector encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "mbX3y_uLqsaW",
    "outputId": "1fa77a93-bc5b-4239-cb3d-1bd669882b67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (8982, 46)\n",
      "y_test shape: (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "\n",
    "# START CODE HERE\n",
    "y_train=to_categorical(y_train,num_classes)\n",
    "y_test= to_categorical(y_test,num_classes)\n",
    "# END CODE HERE\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "scm2vMgeqsab"
   },
   "source": [
    "## Build the whole system: model + compile + fit\n",
    "\n",
    "We will use \n",
    "- a 3 layers Neural Network (2 hidden layers)\n",
    "- with 512 hidden units each and Relu activation\n",
    "- what is the number of output classes ?\n",
    "- what should be the acitvation function of the output class ?\n",
    "\n",
    "We will use\n",
    "- as ```optimizer``` a ```sgd``` with a learn_rate of 0.01, \n",
    "- which loss should be used for a binary classification problem ?\n",
    "- as ```metrics```to be displayed after each epoch the ```accuracy```\n",
    "\n",
    "We will use\n",
    "- batch_size of 32\n",
    "- 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7253
    },
    "colab_type": "code",
    "id": "UKi0cRytqsad",
    "outputId": "1087c8f5-868c-443b-811b-b9aa3e1fc9e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/200\n",
      "8982/8982 [==============================] - 2s 246us/step - loss: 2.3526 - acc: 0.4507 - val_loss: 1.8544 - val_acc: 0.5338\n",
      "Epoch 2/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 1.7139 - acc: 0.5816 - val_loss: 1.6490 - val_acc: 0.6247\n",
      "Epoch 3/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 1.5413 - acc: 0.6470 - val_loss: 1.5361 - val_acc: 0.6496\n",
      "Epoch 4/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 1.4167 - acc: 0.6823 - val_loss: 1.4386 - val_acc: 0.6781\n",
      "Epoch 5/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 1.3126 - acc: 0.7022 - val_loss: 1.3609 - val_acc: 0.6915\n",
      "Epoch 6/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 1.2277 - acc: 0.7198 - val_loss: 1.3066 - val_acc: 0.7061\n",
      "Epoch 7/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 1.1573 - acc: 0.7363 - val_loss: 1.2667 - val_acc: 0.7186\n",
      "Epoch 8/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 1.0990 - acc: 0.7483 - val_loss: 1.2152 - val_acc: 0.7266\n",
      "Epoch 9/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 1.0467 - acc: 0.7609 - val_loss: 1.1883 - val_acc: 0.7311\n",
      "Epoch 10/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.9997 - acc: 0.7728 - val_loss: 1.1623 - val_acc: 0.7427\n",
      "Epoch 11/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.9566 - acc: 0.7846 - val_loss: 1.1285 - val_acc: 0.7449\n",
      "Epoch 12/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.9163 - acc: 0.7892 - val_loss: 1.1050 - val_acc: 0.7444\n",
      "Epoch 13/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.8793 - acc: 0.7983 - val_loss: 1.0915 - val_acc: 0.7476\n",
      "Epoch 14/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.8439 - acc: 0.8065 - val_loss: 1.0656 - val_acc: 0.7484\n",
      "Epoch 15/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.8112 - acc: 0.8103 - val_loss: 1.0526 - val_acc: 0.7520\n",
      "Epoch 16/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.7798 - acc: 0.8177 - val_loss: 1.0343 - val_acc: 0.7551\n",
      "Epoch 17/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.7498 - acc: 0.8244 - val_loss: 1.0303 - val_acc: 0.7614\n",
      "Epoch 18/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.7234 - acc: 0.8297 - val_loss: 1.0161 - val_acc: 0.7618\n",
      "Epoch 19/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.6959 - acc: 0.8391 - val_loss: 1.0023 - val_acc: 0.7649\n",
      "Epoch 20/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.6700 - acc: 0.8449 - val_loss: 0.9981 - val_acc: 0.7627\n",
      "Epoch 21/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.6444 - acc: 0.8506 - val_loss: 0.9859 - val_acc: 0.7667\n",
      "Epoch 22/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.6226 - acc: 0.8559 - val_loss: 0.9895 - val_acc: 0.7694\n",
      "Epoch 23/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.6012 - acc: 0.8606 - val_loss: 0.9799 - val_acc: 0.7711\n",
      "Epoch 24/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.5793 - acc: 0.8647 - val_loss: 0.9656 - val_acc: 0.7707\n",
      "Epoch 25/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.5585 - acc: 0.8706 - val_loss: 0.9692 - val_acc: 0.7707\n",
      "Epoch 26/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.5394 - acc: 0.8755 - val_loss: 0.9676 - val_acc: 0.7734\n",
      "Epoch 27/200\n",
      "8982/8982 [==============================] - 1s 157us/step - loss: 0.5214 - acc: 0.8786 - val_loss: 0.9638 - val_acc: 0.7694\n",
      "Epoch 28/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.5036 - acc: 0.8845 - val_loss: 0.9662 - val_acc: 0.7752\n",
      "Epoch 29/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.4863 - acc: 0.8877 - val_loss: 0.9518 - val_acc: 0.7809\n",
      "Epoch 30/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.4704 - acc: 0.8910 - val_loss: 0.9667 - val_acc: 0.7720\n",
      "Epoch 31/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.4548 - acc: 0.8950 - val_loss: 0.9585 - val_acc: 0.7769\n",
      "Epoch 32/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.4395 - acc: 0.8994 - val_loss: 0.9805 - val_acc: 0.7707\n",
      "Epoch 33/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.4253 - acc: 0.9025 - val_loss: 0.9634 - val_acc: 0.7774\n",
      "Epoch 34/200\n",
      "8982/8982 [==============================] - 1s 157us/step - loss: 0.4116 - acc: 0.9041 - val_loss: 0.9654 - val_acc: 0.7765\n",
      "Epoch 35/200\n",
      "8982/8982 [==============================] - 1s 157us/step - loss: 0.3984 - acc: 0.9077 - val_loss: 0.9616 - val_acc: 0.7743\n",
      "Epoch 36/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.3869 - acc: 0.9108 - val_loss: 0.9629 - val_acc: 0.7725\n",
      "Epoch 37/200\n",
      "8982/8982 [==============================] - 1s 157us/step - loss: 0.3731 - acc: 0.9126 - val_loss: 0.9712 - val_acc: 0.7711\n",
      "Epoch 38/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.3620 - acc: 0.9158 - val_loss: 0.9699 - val_acc: 0.7743\n",
      "Epoch 39/200\n",
      "8982/8982 [==============================] - 1s 157us/step - loss: 0.3506 - acc: 0.9194 - val_loss: 0.9852 - val_acc: 0.7738\n",
      "Epoch 40/200\n",
      "8982/8982 [==============================] - 1s 158us/step - loss: 0.3397 - acc: 0.9203 - val_loss: 0.9835 - val_acc: 0.7698\n",
      "Epoch 41/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.3287 - acc: 0.9245 - val_loss: 1.0004 - val_acc: 0.7685\n",
      "Epoch 42/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.3193 - acc: 0.9252 - val_loss: 0.9883 - val_acc: 0.7720\n",
      "Epoch 43/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.3098 - acc: 0.9294 - val_loss: 0.9966 - val_acc: 0.7685\n",
      "Epoch 44/200\n",
      "8982/8982 [==============================] - 1s 157us/step - loss: 0.3005 - acc: 0.9300 - val_loss: 1.0035 - val_acc: 0.7698\n",
      "Epoch 45/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.2907 - acc: 0.9320 - val_loss: 1.0022 - val_acc: 0.7747\n",
      "Epoch 46/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.2835 - acc: 0.9313 - val_loss: 1.0102 - val_acc: 0.7698\n",
      "Epoch 47/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.2746 - acc: 0.9356 - val_loss: 1.0262 - val_acc: 0.7725\n",
      "Epoch 48/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.2657 - acc: 0.9383 - val_loss: 1.0178 - val_acc: 0.7707\n",
      "Epoch 49/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.2591 - acc: 0.9362 - val_loss: 1.0256 - val_acc: 0.7707\n",
      "Epoch 50/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.2517 - acc: 0.9381 - val_loss: 1.0319 - val_acc: 0.7716\n",
      "Epoch 51/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.2453 - acc: 0.9385 - val_loss: 1.0417 - val_acc: 0.7689\n",
      "Epoch 52/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.2392 - acc: 0.9397 - val_loss: 1.0616 - val_acc: 0.7685\n",
      "Epoch 53/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.2334 - acc: 0.9415 - val_loss: 1.0558 - val_acc: 0.7680\n",
      "Epoch 54/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.2254 - acc: 0.9432 - val_loss: 1.0655 - val_acc: 0.7663\n",
      "Epoch 55/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.2202 - acc: 0.9452 - val_loss: 1.0609 - val_acc: 0.7676\n",
      "Epoch 56/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.2136 - acc: 0.9463 - val_loss: 1.0749 - val_acc: 0.7689\n",
      "Epoch 57/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.2096 - acc: 0.9432 - val_loss: 1.0779 - val_acc: 0.7694\n",
      "Epoch 58/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.2045 - acc: 0.9463 - val_loss: 1.0783 - val_acc: 0.7671\n",
      "Epoch 59/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.1984 - acc: 0.9472 - val_loss: 1.1063 - val_acc: 0.7663\n",
      "Epoch 60/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.1948 - acc: 0.9468 - val_loss: 1.0934 - val_acc: 0.7720\n",
      "Epoch 61/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.1902 - acc: 0.9475 - val_loss: 1.0961 - val_acc: 0.7645\n",
      "Epoch 62/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.1855 - acc: 0.9483 - val_loss: 1.1159 - val_acc: 0.7654\n",
      "Epoch 63/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.1802 - acc: 0.9500 - val_loss: 1.1068 - val_acc: 0.7676\n",
      "Epoch 64/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.1780 - acc: 0.9495 - val_loss: 1.1149 - val_acc: 0.7649\n",
      "Epoch 65/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.1740 - acc: 0.9493 - val_loss: 1.1319 - val_acc: 0.7622\n",
      "Epoch 66/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.1697 - acc: 0.9513 - val_loss: 1.1238 - val_acc: 0.7663\n",
      "Epoch 67/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.1656 - acc: 0.9522 - val_loss: 1.1475 - val_acc: 0.7609\n",
      "Epoch 68/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.1641 - acc: 0.9526 - val_loss: 1.1377 - val_acc: 0.7658\n",
      "Epoch 69/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.1610 - acc: 0.9528 - val_loss: 1.1727 - val_acc: 0.7640\n",
      "Epoch 70/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.1585 - acc: 0.9530 - val_loss: 1.1527 - val_acc: 0.7640\n",
      "Epoch 71/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.1552 - acc: 0.9512 - val_loss: 1.1490 - val_acc: 0.7649\n",
      "Epoch 72/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.1520 - acc: 0.9535 - val_loss: 1.1587 - val_acc: 0.7658\n",
      "Epoch 73/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.1496 - acc: 0.9532 - val_loss: 1.1709 - val_acc: 0.7618\n",
      "Epoch 74/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.1473 - acc: 0.9531 - val_loss: 1.1780 - val_acc: 0.7582\n",
      "Epoch 75/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.1450 - acc: 0.9540 - val_loss: 1.1738 - val_acc: 0.7645\n",
      "Epoch 76/200\n",
      "8982/8982 [==============================] - 1s 151us/step - loss: 0.1424 - acc: 0.9546 - val_loss: 1.1800 - val_acc: 0.7622\n",
      "Epoch 77/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.1390 - acc: 0.9555 - val_loss: 1.1878 - val_acc: 0.7622\n",
      "Epoch 78/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.1372 - acc: 0.9562 - val_loss: 1.1934 - val_acc: 0.7609\n",
      "Epoch 79/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.1365 - acc: 0.9560 - val_loss: 1.1938 - val_acc: 0.7663\n",
      "Epoch 80/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.1342 - acc: 0.9560 - val_loss: 1.2120 - val_acc: 0.7627\n",
      "Epoch 81/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.1329 - acc: 0.9560 - val_loss: 1.2048 - val_acc: 0.7663\n",
      "Epoch 82/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.1301 - acc: 0.9565 - val_loss: 1.2174 - val_acc: 0.7658\n",
      "Epoch 83/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.1277 - acc: 0.9571 - val_loss: 1.2086 - val_acc: 0.7667\n",
      "Epoch 84/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.1260 - acc: 0.9575 - val_loss: 1.2130 - val_acc: 0.7658\n",
      "Epoch 85/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.1262 - acc: 0.9569 - val_loss: 1.2262 - val_acc: 0.7636\n",
      "Epoch 86/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.1231 - acc: 0.9556 - val_loss: 1.2317 - val_acc: 0.7614\n",
      "Epoch 87/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.1221 - acc: 0.9581 - val_loss: 1.2346 - val_acc: 0.7618\n",
      "Epoch 88/200\n",
      "8982/8982 [==============================] - 1s 157us/step - loss: 0.1190 - acc: 0.9578 - val_loss: 1.2470 - val_acc: 0.7582\n",
      "Epoch 89/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.1192 - acc: 0.9581 - val_loss: 1.2390 - val_acc: 0.7649\n",
      "Epoch 90/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.1196 - acc: 0.9569 - val_loss: 1.2492 - val_acc: 0.7622\n",
      "Epoch 91/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.1148 - acc: 0.9594 - val_loss: 1.2550 - val_acc: 0.7591\n",
      "Epoch 92/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.1158 - acc: 0.9577 - val_loss: 1.2939 - val_acc: 0.7560\n",
      "Epoch 93/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.1194 - acc: 0.9567 - val_loss: 1.2701 - val_acc: 0.7591\n",
      "Epoch 94/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.1139 - acc: 0.9595 - val_loss: 1.3107 - val_acc: 0.7569\n",
      "Epoch 95/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.1137 - acc: 0.9581 - val_loss: 1.2605 - val_acc: 0.7645\n",
      "Epoch 96/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.1121 - acc: 0.9567 - val_loss: 1.2926 - val_acc: 0.7622\n",
      "Epoch 97/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.1107 - acc: 0.9585 - val_loss: 1.2640 - val_acc: 0.7618\n",
      "Epoch 98/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.1103 - acc: 0.9587 - val_loss: 1.2748 - val_acc: 0.7614\n",
      "Epoch 99/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.1095 - acc: 0.9575 - val_loss: 1.2966 - val_acc: 0.7605\n",
      "Epoch 100/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.1090 - acc: 0.9574 - val_loss: 1.2930 - val_acc: 0.7618\n",
      "Epoch 101/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.1074 - acc: 0.9584 - val_loss: 1.2884 - val_acc: 0.7636\n",
      "Epoch 102/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.1067 - acc: 0.9568 - val_loss: 1.3041 - val_acc: 0.7640\n",
      "Epoch 103/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.1068 - acc: 0.9590 - val_loss: 1.2944 - val_acc: 0.7622\n",
      "Epoch 104/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.1073 - acc: 0.9577 - val_loss: 1.2982 - val_acc: 0.7609\n",
      "Epoch 105/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.1048 - acc: 0.9598 - val_loss: 1.3037 - val_acc: 0.7591\n",
      "Epoch 106/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.1035 - acc: 0.9593 - val_loss: 1.3009 - val_acc: 0.7614\n",
      "Epoch 107/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.1037 - acc: 0.9576 - val_loss: 1.3085 - val_acc: 0.7627\n",
      "Epoch 108/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.1035 - acc: 0.9581 - val_loss: 1.3038 - val_acc: 0.7600\n",
      "Epoch 109/200\n",
      "8982/8982 [==============================] - 1s 151us/step - loss: 0.1028 - acc: 0.9586 - val_loss: 1.3182 - val_acc: 0.7622\n",
      "Epoch 110/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.1035 - acc: 0.9576 - val_loss: 1.3284 - val_acc: 0.7600\n",
      "Epoch 111/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.1010 - acc: 0.9584 - val_loss: 1.3265 - val_acc: 0.7614\n",
      "Epoch 112/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.1004 - acc: 0.9586 - val_loss: 1.3140 - val_acc: 0.7600\n",
      "Epoch 113/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.1025 - acc: 0.9561 - val_loss: 1.3474 - val_acc: 0.7609\n",
      "Epoch 114/200\n",
      "8982/8982 [==============================] - 1s 157us/step - loss: 0.1018 - acc: 0.9582 - val_loss: 1.3389 - val_acc: 0.7600\n",
      "Epoch 115/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.1000 - acc: 0.9564 - val_loss: 1.3538 - val_acc: 0.7587\n",
      "Epoch 116/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0981 - acc: 0.9581 - val_loss: 1.3256 - val_acc: 0.7596\n",
      "Epoch 117/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.1000 - acc: 0.9574 - val_loss: 1.3194 - val_acc: 0.7618\n",
      "Epoch 118/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0980 - acc: 0.9570 - val_loss: 1.3333 - val_acc: 0.7569\n",
      "Epoch 119/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0972 - acc: 0.9587 - val_loss: 1.3373 - val_acc: 0.7645\n",
      "Epoch 120/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.0953 - acc: 0.9604 - val_loss: 1.3377 - val_acc: 0.7622\n",
      "Epoch 121/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.0975 - acc: 0.9577 - val_loss: 1.3301 - val_acc: 0.7618\n",
      "Epoch 122/200\n",
      "8982/8982 [==============================] - 1s 157us/step - loss: 0.0989 - acc: 0.9577 - val_loss: 1.3642 - val_acc: 0.7591\n",
      "Epoch 123/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.0947 - acc: 0.9577 - val_loss: 1.3694 - val_acc: 0.7578\n",
      "Epoch 124/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.0952 - acc: 0.9580 - val_loss: 1.3528 - val_acc: 0.7600\n",
      "Epoch 125/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.0970 - acc: 0.9557 - val_loss: 1.3611 - val_acc: 0.7609\n",
      "Epoch 126/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0931 - acc: 0.9595 - val_loss: 1.3734 - val_acc: 0.7551\n",
      "Epoch 127/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0952 - acc: 0.9584 - val_loss: 1.3491 - val_acc: 0.7636\n",
      "Epoch 128/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0953 - acc: 0.9576 - val_loss: 1.3551 - val_acc: 0.7591\n",
      "Epoch 129/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0942 - acc: 0.9572 - val_loss: 1.3724 - val_acc: 0.7596\n",
      "Epoch 130/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0952 - acc: 0.9584 - val_loss: 1.3734 - val_acc: 0.7560\n",
      "Epoch 131/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0918 - acc: 0.9599 - val_loss: 1.3546 - val_acc: 0.7605\n",
      "Epoch 132/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0922 - acc: 0.9599 - val_loss: 1.3847 - val_acc: 0.7631\n",
      "Epoch 133/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0914 - acc: 0.9601 - val_loss: 1.3612 - val_acc: 0.7627\n",
      "Epoch 134/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.0940 - acc: 0.9597 - val_loss: 1.3695 - val_acc: 0.7627\n",
      "Epoch 135/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0931 - acc: 0.9599 - val_loss: 1.3635 - val_acc: 0.7627\n",
      "Epoch 136/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0910 - acc: 0.9597 - val_loss: 1.3579 - val_acc: 0.7614\n",
      "Epoch 137/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0901 - acc: 0.9576 - val_loss: 1.3677 - val_acc: 0.7654\n",
      "Epoch 138/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0914 - acc: 0.9584 - val_loss: 1.3652 - val_acc: 0.7631\n",
      "Epoch 139/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0904 - acc: 0.9589 - val_loss: 1.3927 - val_acc: 0.7582\n",
      "Epoch 140/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0908 - acc: 0.9582 - val_loss: 1.4024 - val_acc: 0.7565\n",
      "Epoch 141/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0901 - acc: 0.9576 - val_loss: 1.4283 - val_acc: 0.7569\n",
      "Epoch 142/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.0908 - acc: 0.9580 - val_loss: 1.4045 - val_acc: 0.7578\n",
      "Epoch 143/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0894 - acc: 0.9587 - val_loss: 1.4289 - val_acc: 0.7556\n",
      "Epoch 144/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0893 - acc: 0.9596 - val_loss: 1.3915 - val_acc: 0.7600\n",
      "Epoch 145/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0896 - acc: 0.9594 - val_loss: 1.3989 - val_acc: 0.7618\n",
      "Epoch 146/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0912 - acc: 0.9582 - val_loss: 1.3716 - val_acc: 0.7649\n",
      "Epoch 147/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0901 - acc: 0.9591 - val_loss: 1.3982 - val_acc: 0.7640\n",
      "Epoch 148/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0901 - acc: 0.9581 - val_loss: 1.3933 - val_acc: 0.7600\n",
      "Epoch 149/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0912 - acc: 0.9570 - val_loss: 1.4000 - val_acc: 0.7596\n",
      "Epoch 150/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0885 - acc: 0.9580 - val_loss: 1.3808 - val_acc: 0.7658\n",
      "Epoch 151/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0889 - acc: 0.9581 - val_loss: 1.3811 - val_acc: 0.7622\n",
      "Epoch 152/200\n",
      "8982/8982 [==============================] - 1s 151us/step - loss: 0.0895 - acc: 0.9585 - val_loss: 1.3889 - val_acc: 0.7600\n",
      "Epoch 153/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0871 - acc: 0.9576 - val_loss: 1.3861 - val_acc: 0.7649\n",
      "Epoch 154/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0874 - acc: 0.9586 - val_loss: 1.3843 - val_acc: 0.7654\n",
      "Epoch 155/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0861 - acc: 0.9603 - val_loss: 1.3948 - val_acc: 0.7622\n",
      "Epoch 156/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0876 - acc: 0.9593 - val_loss: 1.3856 - val_acc: 0.7636\n",
      "Epoch 157/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0852 - acc: 0.9586 - val_loss: 1.4181 - val_acc: 0.7560\n",
      "Epoch 158/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.0874 - acc: 0.9588 - val_loss: 1.3897 - val_acc: 0.7627\n",
      "Epoch 159/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0872 - acc: 0.9582 - val_loss: 1.3971 - val_acc: 0.7618\n",
      "Epoch 160/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0860 - acc: 0.9587 - val_loss: 1.3946 - val_acc: 0.7627\n",
      "Epoch 161/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.0867 - acc: 0.9598 - val_loss: 1.3833 - val_acc: 0.7667\n",
      "Epoch 162/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0870 - acc: 0.9576 - val_loss: 1.3932 - val_acc: 0.7636\n",
      "Epoch 163/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0845 - acc: 0.9586 - val_loss: 1.4133 - val_acc: 0.7591\n",
      "Epoch 164/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0874 - acc: 0.9577 - val_loss: 1.4473 - val_acc: 0.7573\n",
      "Epoch 165/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0861 - acc: 0.9589 - val_loss: 1.3912 - val_acc: 0.7640\n",
      "Epoch 166/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0840 - acc: 0.9594 - val_loss: 1.4053 - val_acc: 0.7622\n",
      "Epoch 167/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0868 - acc: 0.9572 - val_loss: 1.3920 - val_acc: 0.7609\n",
      "Epoch 168/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0858 - acc: 0.9577 - val_loss: 1.4077 - val_acc: 0.7609\n",
      "Epoch 169/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0846 - acc: 0.9599 - val_loss: 1.3925 - val_acc: 0.7627\n",
      "Epoch 170/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.0832 - acc: 0.9604 - val_loss: 1.3909 - val_acc: 0.7671\n",
      "Epoch 171/200\n",
      "8982/8982 [==============================] - 1s 157us/step - loss: 0.0859 - acc: 0.9581 - val_loss: 1.3980 - val_acc: 0.7605\n",
      "Epoch 172/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0852 - acc: 0.9585 - val_loss: 1.4068 - val_acc: 0.7636\n",
      "Epoch 173/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0849 - acc: 0.9566 - val_loss: 1.4054 - val_acc: 0.7622\n",
      "Epoch 174/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0816 - acc: 0.9607 - val_loss: 1.3977 - val_acc: 0.7614\n",
      "Epoch 175/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0833 - acc: 0.9591 - val_loss: 1.4378 - val_acc: 0.7591\n",
      "Epoch 176/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0841 - acc: 0.9590 - val_loss: 1.4033 - val_acc: 0.7663\n",
      "Epoch 177/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0850 - acc: 0.9564 - val_loss: 1.4013 - val_acc: 0.7622\n",
      "Epoch 178/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0818 - acc: 0.9613 - val_loss: 1.4167 - val_acc: 0.7631\n",
      "Epoch 179/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0840 - acc: 0.9556 - val_loss: 1.4136 - val_acc: 0.7618\n",
      "Epoch 180/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.0837 - acc: 0.9580 - val_loss: 1.4135 - val_acc: 0.7645\n",
      "Epoch 181/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0812 - acc: 0.9599 - val_loss: 1.4138 - val_acc: 0.7631\n",
      "Epoch 182/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0848 - acc: 0.9585 - val_loss: 1.4127 - val_acc: 0.7636\n",
      "Epoch 183/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0811 - acc: 0.9606 - val_loss: 1.4063 - val_acc: 0.7645\n",
      "Epoch 184/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0830 - acc: 0.9588 - val_loss: 1.4253 - val_acc: 0.7578\n",
      "Epoch 185/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.0828 - acc: 0.9585 - val_loss: 1.4242 - val_acc: 0.7631\n",
      "Epoch 186/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0842 - acc: 0.9576 - val_loss: 1.4160 - val_acc: 0.7654\n",
      "Epoch 187/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0831 - acc: 0.9576 - val_loss: 1.4122 - val_acc: 0.7614\n",
      "Epoch 188/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0810 - acc: 0.9608 - val_loss: 1.4159 - val_acc: 0.7618\n",
      "Epoch 189/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0816 - acc: 0.9609 - val_loss: 1.4311 - val_acc: 0.7622\n",
      "Epoch 190/200\n",
      "8982/8982 [==============================] - 1s 157us/step - loss: 0.0818 - acc: 0.9589 - val_loss: 1.4237 - val_acc: 0.7618\n",
      "Epoch 191/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0798 - acc: 0.9587 - val_loss: 1.4324 - val_acc: 0.7596\n",
      "Epoch 192/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0817 - acc: 0.9571 - val_loss: 1.4509 - val_acc: 0.7582\n",
      "Epoch 193/200\n",
      "8982/8982 [==============================] - 1s 155us/step - loss: 0.0823 - acc: 0.9574 - val_loss: 1.4106 - val_acc: 0.7627\n",
      "Epoch 194/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0820 - acc: 0.9584 - val_loss: 1.4201 - val_acc: 0.7631\n",
      "Epoch 195/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.0784 - acc: 0.9613 - val_loss: 1.4179 - val_acc: 0.7663\n",
      "Epoch 196/200\n",
      "8982/8982 [==============================] - 1s 152us/step - loss: 0.0818 - acc: 0.9575 - val_loss: 1.4285 - val_acc: 0.7649\n",
      "Epoch 197/200\n",
      "8982/8982 [==============================] - 1s 154us/step - loss: 0.0801 - acc: 0.9587 - val_loss: 1.4240 - val_acc: 0.7631\n",
      "Epoch 198/200\n",
      "8982/8982 [==============================] - 1s 156us/step - loss: 0.0789 - acc: 0.9624 - val_loss: 1.4228 - val_acc: 0.7631\n",
      "Epoch 199/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0816 - acc: 0.9585 - val_loss: 1.4125 - val_acc: 0.7649\n",
      "Epoch 200/200\n",
      "8982/8982 [==============================] - 1s 153us/step - loss: 0.0807 - acc: 0.9586 - val_loss: 1.4193 - val_acc: 0.7618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe62bfaf6a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# START CODE HERE\n",
    "\n",
    "model2=Sequential()\n",
    "model2.add(Dense(512,activation='relu',input_dim=1000))\n",
    "model2.add(Dense(512,activation='relu'))\n",
    "model2.add(Dense(46,activation='softmax'))\n",
    "sgd=keras.optimizers.SGD(lr=0.01)\n",
    "model2.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model2.fit(x_train,y_train,epochs=200,batch_size=32,verbose=1,validation_data=(x_test,y_test))\n",
    "\n",
    "# START CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDE86k_1qsah"
   },
   "source": [
    "### Question\n",
    "\n",
    "What are the performances on the test data ? Loss and Accuracy ?\n",
    "\n",
    "### Question\n",
    "\n",
    "Write the same model using Keras functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sR2XNzwJzpoi"
   },
   "source": [
    "The loss and accuracy on the test data perform better firstly but as the process of traing, we meet the problem the overfitting where the performence of the training set becomes better but the loss in the test set increase and the accuracy decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7253
    },
    "colab_type": "code",
    "id": "1kXdPYGYqsaj",
    "outputId": "1dac15ab-6df5-4d4c-ee94-731ac0b28c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/200\n",
      "8982/8982 [==============================] - 2s 169us/step - loss: 2.4363 - acc: 0.4531 - val_loss: 1.9002 - val_acc: 0.5401\n",
      "Epoch 2/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 1.7453 - acc: 0.5707 - val_loss: 1.6776 - val_acc: 0.6118\n",
      "Epoch 3/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 1.5585 - acc: 0.6430 - val_loss: 1.5507 - val_acc: 0.6500\n",
      "Epoch 4/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 1.4304 - acc: 0.6765 - val_loss: 1.4576 - val_acc: 0.6745\n",
      "Epoch 5/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 1.3302 - acc: 0.6978 - val_loss: 1.3865 - val_acc: 0.6888\n",
      "Epoch 6/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 1.2481 - acc: 0.7151 - val_loss: 1.3284 - val_acc: 0.6972\n",
      "Epoch 7/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 1.1819 - acc: 0.7316 - val_loss: 1.2918 - val_acc: 0.7110\n",
      "Epoch 8/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 1.1253 - acc: 0.7449 - val_loss: 1.2483 - val_acc: 0.7173\n",
      "Epoch 9/200\n",
      "8982/8982 [==============================] - 1s 149us/step - loss: 1.0778 - acc: 0.7557 - val_loss: 1.2157 - val_acc: 0.7293\n",
      "Epoch 10/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 1.0327 - acc: 0.7678 - val_loss: 1.1917 - val_acc: 0.7289\n",
      "Epoch 11/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.9927 - acc: 0.7766 - val_loss: 1.1658 - val_acc: 0.7360\n",
      "Epoch 12/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.9560 - acc: 0.7840 - val_loss: 1.1421 - val_acc: 0.7422\n",
      "Epoch 13/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.9216 - acc: 0.7936 - val_loss: 1.1344 - val_acc: 0.7404\n",
      "Epoch 14/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.8872 - acc: 0.7990 - val_loss: 1.1074 - val_acc: 0.7480\n",
      "Epoch 15/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.8568 - acc: 0.8023 - val_loss: 1.0917 - val_acc: 0.7507\n",
      "Epoch 16/200\n",
      "8982/8982 [==============================] - 1s 149us/step - loss: 0.8264 - acc: 0.8081 - val_loss: 1.0779 - val_acc: 0.7524\n",
      "Epoch 17/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.7985 - acc: 0.8147 - val_loss: 1.0638 - val_acc: 0.7511\n",
      "Epoch 18/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.7717 - acc: 0.8202 - val_loss: 1.0558 - val_acc: 0.7529\n",
      "Epoch 19/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.7464 - acc: 0.8250 - val_loss: 1.0404 - val_acc: 0.7569\n",
      "Epoch 20/200\n",
      "8982/8982 [==============================] - 1s 149us/step - loss: 0.7216 - acc: 0.8321 - val_loss: 1.0304 - val_acc: 0.7600\n",
      "Epoch 21/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.6989 - acc: 0.8348 - val_loss: 1.0191 - val_acc: 0.7573\n",
      "Epoch 22/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.6771 - acc: 0.8415 - val_loss: 1.0226 - val_acc: 0.7573\n",
      "Epoch 23/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.6551 - acc: 0.8460 - val_loss: 1.0105 - val_acc: 0.7600\n",
      "Epoch 24/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.6359 - acc: 0.8503 - val_loss: 1.0033 - val_acc: 0.7645\n",
      "Epoch 25/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.6147 - acc: 0.8564 - val_loss: 1.0061 - val_acc: 0.7614\n",
      "Epoch 26/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.5965 - acc: 0.8586 - val_loss: 1.0025 - val_acc: 0.7649\n",
      "Epoch 27/200\n",
      "8982/8982 [==============================] - 1s 150us/step - loss: 0.5780 - acc: 0.8642 - val_loss: 0.9972 - val_acc: 0.7645\n",
      "Epoch 28/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.5618 - acc: 0.8685 - val_loss: 1.0138 - val_acc: 0.7605\n",
      "Epoch 29/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.5437 - acc: 0.8742 - val_loss: 0.9908 - val_acc: 0.7676\n",
      "Epoch 30/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.5279 - acc: 0.8764 - val_loss: 0.9857 - val_acc: 0.7734\n",
      "Epoch 31/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.5114 - acc: 0.8795 - val_loss: 0.9911 - val_acc: 0.7738\n",
      "Epoch 32/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.4969 - acc: 0.8833 - val_loss: 0.9861 - val_acc: 0.7676\n",
      "Epoch 33/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.4819 - acc: 0.8869 - val_loss: 0.9857 - val_acc: 0.7698\n",
      "Epoch 34/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.4679 - acc: 0.8911 - val_loss: 0.9851 - val_acc: 0.7738\n",
      "Epoch 35/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.4531 - acc: 0.8937 - val_loss: 0.9838 - val_acc: 0.7752\n",
      "Epoch 36/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.4405 - acc: 0.8979 - val_loss: 0.9874 - val_acc: 0.7765\n",
      "Epoch 37/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.4277 - acc: 0.8987 - val_loss: 0.9920 - val_acc: 0.7769\n",
      "Epoch 38/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.4152 - acc: 0.9026 - val_loss: 0.9984 - val_acc: 0.7671\n",
      "Epoch 39/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.4024 - acc: 0.9064 - val_loss: 0.9937 - val_acc: 0.7743\n",
      "Epoch 40/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.3914 - acc: 0.9074 - val_loss: 1.0022 - val_acc: 0.7703\n",
      "Epoch 41/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.3796 - acc: 0.9107 - val_loss: 1.0019 - val_acc: 0.7756\n",
      "Epoch 42/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.3694 - acc: 0.9127 - val_loss: 1.0057 - val_acc: 0.7689\n",
      "Epoch 43/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.3593 - acc: 0.9167 - val_loss: 1.0135 - val_acc: 0.7720\n",
      "Epoch 44/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.3490 - acc: 0.9173 - val_loss: 1.0075 - val_acc: 0.7720\n",
      "Epoch 45/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.3393 - acc: 0.9204 - val_loss: 1.0265 - val_acc: 0.7685\n",
      "Epoch 46/200\n",
      "8982/8982 [==============================] - 1s 151us/step - loss: 0.3307 - acc: 0.9226 - val_loss: 1.0203 - val_acc: 0.7738\n",
      "Epoch 47/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.3209 - acc: 0.9226 - val_loss: 1.0377 - val_acc: 0.7649\n",
      "Epoch 48/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.3129 - acc: 0.9247 - val_loss: 1.0297 - val_acc: 0.7769\n",
      "Epoch 49/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.3032 - acc: 0.9290 - val_loss: 1.0378 - val_acc: 0.7703\n",
      "Epoch 50/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.2965 - acc: 0.9282 - val_loss: 1.0392 - val_acc: 0.7725\n",
      "Epoch 51/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.2882 - acc: 0.9295 - val_loss: 1.0494 - val_acc: 0.7694\n",
      "Epoch 52/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.2797 - acc: 0.9321 - val_loss: 1.0563 - val_acc: 0.7685\n",
      "Epoch 53/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.2741 - acc: 0.9321 - val_loss: 1.0560 - val_acc: 0.7689\n",
      "Epoch 54/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.2650 - acc: 0.9359 - val_loss: 1.0613 - val_acc: 0.7689\n",
      "Epoch 55/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.2597 - acc: 0.9343 - val_loss: 1.0725 - val_acc: 0.7707\n",
      "Epoch 56/200\n",
      "8982/8982 [==============================] - 1s 149us/step - loss: 0.2524 - acc: 0.9367 - val_loss: 1.0723 - val_acc: 0.7703\n",
      "Epoch 57/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.2457 - acc: 0.9382 - val_loss: 1.0828 - val_acc: 0.7720\n",
      "Epoch 58/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.2409 - acc: 0.9395 - val_loss: 1.0980 - val_acc: 0.7698\n",
      "Epoch 59/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.2351 - acc: 0.9400 - val_loss: 1.0956 - val_acc: 0.7689\n",
      "Epoch 60/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.2304 - acc: 0.9423 - val_loss: 1.0922 - val_acc: 0.7694\n",
      "Epoch 61/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.2244 - acc: 0.9405 - val_loss: 1.1020 - val_acc: 0.7711\n",
      "Epoch 62/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.2185 - acc: 0.9436 - val_loss: 1.1166 - val_acc: 0.7671\n",
      "Epoch 63/200\n",
      "8982/8982 [==============================] - 1s 151us/step - loss: 0.2152 - acc: 0.9428 - val_loss: 1.1096 - val_acc: 0.7694\n",
      "Epoch 64/200\n",
      "8982/8982 [==============================] - 1s 149us/step - loss: 0.2085 - acc: 0.9472 - val_loss: 1.1306 - val_acc: 0.7676\n",
      "Epoch 65/200\n",
      "8982/8982 [==============================] - 1s 149us/step - loss: 0.2057 - acc: 0.9451 - val_loss: 1.1260 - val_acc: 0.7667\n",
      "Epoch 66/200\n",
      "8982/8982 [==============================] - 1s 150us/step - loss: 0.2002 - acc: 0.9486 - val_loss: 1.1603 - val_acc: 0.7600\n",
      "Epoch 67/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.1956 - acc: 0.9467 - val_loss: 1.1344 - val_acc: 0.7685\n",
      "Epoch 68/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1934 - acc: 0.9480 - val_loss: 1.1496 - val_acc: 0.7667\n",
      "Epoch 69/200\n",
      "8982/8982 [==============================] - 1s 143us/step - loss: 0.1882 - acc: 0.9481 - val_loss: 1.1600 - val_acc: 0.7654\n",
      "Epoch 70/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.1840 - acc: 0.9486 - val_loss: 1.1660 - val_acc: 0.7649\n",
      "Epoch 71/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.1812 - acc: 0.9490 - val_loss: 1.1696 - val_acc: 0.7658\n",
      "Epoch 72/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.1778 - acc: 0.9495 - val_loss: 1.1743 - val_acc: 0.7640\n",
      "Epoch 73/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.1747 - acc: 0.9497 - val_loss: 1.1667 - val_acc: 0.7680\n",
      "Epoch 74/200\n",
      "8982/8982 [==============================] - 1s 149us/step - loss: 0.1700 - acc: 0.9516 - val_loss: 1.1843 - val_acc: 0.7649\n",
      "Epoch 75/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.1683 - acc: 0.9522 - val_loss: 1.1839 - val_acc: 0.7654\n",
      "Epoch 76/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1648 - acc: 0.9522 - val_loss: 1.1948 - val_acc: 0.7631\n",
      "Epoch 77/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.1617 - acc: 0.9516 - val_loss: 1.1876 - val_acc: 0.7676\n",
      "Epoch 78/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.1578 - acc: 0.9530 - val_loss: 1.2036 - val_acc: 0.7640\n",
      "Epoch 79/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.1570 - acc: 0.9535 - val_loss: 1.2142 - val_acc: 0.7605\n",
      "Epoch 80/200\n",
      "8982/8982 [==============================] - 1s 143us/step - loss: 0.1548 - acc: 0.9531 - val_loss: 1.2100 - val_acc: 0.7622\n",
      "Epoch 81/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1526 - acc: 0.9523 - val_loss: 1.2245 - val_acc: 0.7622\n",
      "Epoch 82/200\n",
      "8982/8982 [==============================] - 1s 143us/step - loss: 0.1490 - acc: 0.9536 - val_loss: 1.2165 - val_acc: 0.7627\n",
      "Epoch 83/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.1477 - acc: 0.9538 - val_loss: 1.2248 - val_acc: 0.7622\n",
      "Epoch 84/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1442 - acc: 0.9548 - val_loss: 1.2322 - val_acc: 0.7618\n",
      "Epoch 85/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1407 - acc: 0.9548 - val_loss: 1.2398 - val_acc: 0.7614\n",
      "Epoch 86/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1401 - acc: 0.9538 - val_loss: 1.2495 - val_acc: 0.7618\n",
      "Epoch 87/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1380 - acc: 0.9549 - val_loss: 1.2390 - val_acc: 0.7600\n",
      "Epoch 88/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.1362 - acc: 0.9545 - val_loss: 1.2716 - val_acc: 0.7605\n",
      "Epoch 89/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.1373 - acc: 0.9539 - val_loss: 1.2511 - val_acc: 0.7618\n",
      "Epoch 90/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1334 - acc: 0.9542 - val_loss: 1.2599 - val_acc: 0.7614\n",
      "Epoch 91/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.1302 - acc: 0.9555 - val_loss: 1.2718 - val_acc: 0.7578\n",
      "Epoch 92/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.1305 - acc: 0.9550 - val_loss: 1.2756 - val_acc: 0.7649\n",
      "Epoch 93/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.1303 - acc: 0.9556 - val_loss: 1.2677 - val_acc: 0.7627\n",
      "Epoch 94/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.1266 - acc: 0.9570 - val_loss: 1.2689 - val_acc: 0.7605\n",
      "Epoch 95/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.1258 - acc: 0.9560 - val_loss: 1.2819 - val_acc: 0.7614\n",
      "Epoch 96/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.1231 - acc: 0.9571 - val_loss: 1.2802 - val_acc: 0.7605\n",
      "Epoch 97/200\n",
      "8982/8982 [==============================] - 1s 143us/step - loss: 0.1256 - acc: 0.9559 - val_loss: 1.2844 - val_acc: 0.7609\n",
      "Epoch 98/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.1228 - acc: 0.9557 - val_loss: 1.2814 - val_acc: 0.7627\n",
      "Epoch 99/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1188 - acc: 0.9569 - val_loss: 1.3158 - val_acc: 0.7582\n",
      "Epoch 100/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.1191 - acc: 0.9558 - val_loss: 1.3109 - val_acc: 0.7600\n",
      "Epoch 101/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.1172 - acc: 0.9584 - val_loss: 1.3029 - val_acc: 0.7631\n",
      "Epoch 102/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.1165 - acc: 0.9596 - val_loss: 1.3155 - val_acc: 0.7560\n",
      "Epoch 103/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1182 - acc: 0.9557 - val_loss: 1.3065 - val_acc: 0.7596\n",
      "Epoch 104/200\n",
      "8982/8982 [==============================] - 1s 149us/step - loss: 0.1154 - acc: 0.9562 - val_loss: 1.3102 - val_acc: 0.7618\n",
      "Epoch 105/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1155 - acc: 0.9575 - val_loss: 1.3245 - val_acc: 0.7587\n",
      "Epoch 106/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.1138 - acc: 0.9579 - val_loss: 1.3396 - val_acc: 0.7596\n",
      "Epoch 107/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.1151 - acc: 0.9562 - val_loss: 1.3223 - val_acc: 0.7582\n",
      "Epoch 108/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1097 - acc: 0.9587 - val_loss: 1.3318 - val_acc: 0.7600\n",
      "Epoch 109/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1107 - acc: 0.9581 - val_loss: 1.3272 - val_acc: 0.7587\n",
      "Epoch 110/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1103 - acc: 0.9584 - val_loss: 1.3298 - val_acc: 0.7614\n",
      "Epoch 111/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.1096 - acc: 0.9561 - val_loss: 1.3328 - val_acc: 0.7591\n",
      "Epoch 112/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.1078 - acc: 0.9587 - val_loss: 1.3560 - val_acc: 0.7560\n",
      "Epoch 113/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.1080 - acc: 0.9581 - val_loss: 1.3640 - val_acc: 0.7618\n",
      "Epoch 114/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.1074 - acc: 0.9569 - val_loss: 1.3487 - val_acc: 0.7573\n",
      "Epoch 115/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1064 - acc: 0.9587 - val_loss: 1.3646 - val_acc: 0.7560\n",
      "Epoch 116/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1077 - acc: 0.9576 - val_loss: 1.3611 - val_acc: 0.7596\n",
      "Epoch 117/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1081 - acc: 0.9568 - val_loss: 1.3714 - val_acc: 0.7529\n",
      "Epoch 118/200\n",
      "8982/8982 [==============================] - 1s 149us/step - loss: 0.1058 - acc: 0.9586 - val_loss: 1.3552 - val_acc: 0.7591\n",
      "Epoch 119/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.1038 - acc: 0.9603 - val_loss: 1.3483 - val_acc: 0.7636\n",
      "Epoch 120/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.1051 - acc: 0.9572 - val_loss: 1.3647 - val_acc: 0.7582\n",
      "Epoch 121/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.1034 - acc: 0.9578 - val_loss: 1.3735 - val_acc: 0.7591\n",
      "Epoch 122/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.1019 - acc: 0.9595 - val_loss: 1.3640 - val_acc: 0.7591\n",
      "Epoch 123/200\n",
      "8982/8982 [==============================] - 1s 143us/step - loss: 0.1038 - acc: 0.9593 - val_loss: 1.3829 - val_acc: 0.7551\n",
      "Epoch 124/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.0994 - acc: 0.9599 - val_loss: 1.3766 - val_acc: 0.7596\n",
      "Epoch 125/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.1000 - acc: 0.9581 - val_loss: 1.3996 - val_acc: 0.7591\n",
      "Epoch 126/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.1001 - acc: 0.9567 - val_loss: 1.3719 - val_acc: 0.7614\n",
      "Epoch 127/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.1003 - acc: 0.9595 - val_loss: 1.4043 - val_acc: 0.7551\n",
      "Epoch 128/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0993 - acc: 0.9586 - val_loss: 1.3869 - val_acc: 0.7596\n",
      "Epoch 129/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.0972 - acc: 0.9601 - val_loss: 1.3820 - val_acc: 0.7609\n",
      "Epoch 130/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.0988 - acc: 0.9594 - val_loss: 1.3987 - val_acc: 0.7565\n",
      "Epoch 131/200\n",
      "8982/8982 [==============================] - 1s 143us/step - loss: 0.0991 - acc: 0.9584 - val_loss: 1.3794 - val_acc: 0.7587\n",
      "Epoch 132/200\n",
      "8982/8982 [==============================] - 1s 143us/step - loss: 0.0981 - acc: 0.9575 - val_loss: 1.3970 - val_acc: 0.7605\n",
      "Epoch 133/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.0980 - acc: 0.9594 - val_loss: 1.4128 - val_acc: 0.7600\n",
      "Epoch 134/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.0981 - acc: 0.9581 - val_loss: 1.3989 - val_acc: 0.7618\n",
      "Epoch 135/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.0982 - acc: 0.9596 - val_loss: 1.3933 - val_acc: 0.7582\n",
      "Epoch 136/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.0950 - acc: 0.9585 - val_loss: 1.4062 - val_acc: 0.7596\n",
      "Epoch 137/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.0970 - acc: 0.9580 - val_loss: 1.4048 - val_acc: 0.7565\n",
      "Epoch 138/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.0964 - acc: 0.9589 - val_loss: 1.4166 - val_acc: 0.7587\n",
      "Epoch 139/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.0950 - acc: 0.9595 - val_loss: 1.4107 - val_acc: 0.7591\n",
      "Epoch 140/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0960 - acc: 0.9575 - val_loss: 1.4002 - val_acc: 0.7565\n",
      "Epoch 141/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.0936 - acc: 0.9590 - val_loss: 1.4232 - val_acc: 0.7614\n",
      "Epoch 142/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0938 - acc: 0.9586 - val_loss: 1.4251 - val_acc: 0.7587\n",
      "Epoch 143/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0927 - acc: 0.9595 - val_loss: 1.4356 - val_acc: 0.7587\n",
      "Epoch 144/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0955 - acc: 0.9580 - val_loss: 1.4183 - val_acc: 0.7596\n",
      "Epoch 145/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0935 - acc: 0.9569 - val_loss: 1.4158 - val_acc: 0.7556\n",
      "Epoch 146/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.0917 - acc: 0.9597 - val_loss: 1.4478 - val_acc: 0.7529\n",
      "Epoch 147/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0922 - acc: 0.9584 - val_loss: 1.4296 - val_acc: 0.7582\n",
      "Epoch 148/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.0913 - acc: 0.9596 - val_loss: 1.4501 - val_acc: 0.7569\n",
      "Epoch 149/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0931 - acc: 0.9587 - val_loss: 1.4082 - val_acc: 0.7591\n",
      "Epoch 150/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0942 - acc: 0.9570 - val_loss: 1.4425 - val_acc: 0.7591\n",
      "Epoch 151/200\n",
      "8982/8982 [==============================] - 1s 143us/step - loss: 0.0896 - acc: 0.9599 - val_loss: 1.4461 - val_acc: 0.7542\n",
      "Epoch 152/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.0917 - acc: 0.9586 - val_loss: 1.4369 - val_acc: 0.7609\n",
      "Epoch 153/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.0911 - acc: 0.9588 - val_loss: 1.4243 - val_acc: 0.7614\n",
      "Epoch 154/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.0905 - acc: 0.9598 - val_loss: 1.4364 - val_acc: 0.7600\n",
      "Epoch 155/200\n",
      "8982/8982 [==============================] - 1s 149us/step - loss: 0.0920 - acc: 0.9594 - val_loss: 1.4292 - val_acc: 0.7600\n",
      "Epoch 156/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0919 - acc: 0.9582 - val_loss: 1.4325 - val_acc: 0.7618\n",
      "Epoch 157/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.0918 - acc: 0.9577 - val_loss: 1.4498 - val_acc: 0.7591\n",
      "Epoch 158/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.0934 - acc: 0.9567 - val_loss: 1.4393 - val_acc: 0.7578\n",
      "Epoch 159/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.0918 - acc: 0.9571 - val_loss: 1.4365 - val_acc: 0.7538\n",
      "Epoch 160/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.0891 - acc: 0.9579 - val_loss: 1.4538 - val_acc: 0.7569\n",
      "Epoch 161/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.0887 - acc: 0.9587 - val_loss: 1.4369 - val_acc: 0.7631\n",
      "Epoch 162/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.0880 - acc: 0.9593 - val_loss: 1.4425 - val_acc: 0.7565\n",
      "Epoch 163/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.0889 - acc: 0.9599 - val_loss: 1.4460 - val_acc: 0.7538\n",
      "Epoch 164/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0870 - acc: 0.9601 - val_loss: 1.4822 - val_acc: 0.7529\n",
      "Epoch 165/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0891 - acc: 0.9579 - val_loss: 1.4474 - val_acc: 0.7569\n",
      "Epoch 166/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.0910 - acc: 0.9578 - val_loss: 1.4493 - val_acc: 0.7596\n",
      "Epoch 167/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0883 - acc: 0.9580 - val_loss: 1.4357 - val_acc: 0.7591\n",
      "Epoch 168/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.0897 - acc: 0.9580 - val_loss: 1.4445 - val_acc: 0.7636\n",
      "Epoch 169/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.0879 - acc: 0.9596 - val_loss: 1.4518 - val_acc: 0.7551\n",
      "Epoch 170/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.0877 - acc: 0.9594 - val_loss: 1.4330 - val_acc: 0.7605\n",
      "Epoch 171/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.0861 - acc: 0.9593 - val_loss: 1.4399 - val_acc: 0.7631\n",
      "Epoch 172/200\n",
      "8982/8982 [==============================] - 1s 142us/step - loss: 0.0895 - acc: 0.9579 - val_loss: 1.4412 - val_acc: 0.7569\n",
      "Epoch 173/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0888 - acc: 0.9577 - val_loss: 1.4341 - val_acc: 0.7573\n",
      "Epoch 174/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.0868 - acc: 0.9588 - val_loss: 1.4398 - val_acc: 0.7596\n",
      "Epoch 175/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.0868 - acc: 0.9603 - val_loss: 1.4562 - val_acc: 0.7591\n",
      "Epoch 176/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.0889 - acc: 0.9572 - val_loss: 1.4498 - val_acc: 0.7614\n",
      "Epoch 177/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.0867 - acc: 0.9586 - val_loss: 1.4506 - val_acc: 0.7605\n",
      "Epoch 178/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0877 - acc: 0.9576 - val_loss: 1.4614 - val_acc: 0.7538\n",
      "Epoch 179/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.0876 - acc: 0.9572 - val_loss: 1.4605 - val_acc: 0.7556\n",
      "Epoch 180/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.0862 - acc: 0.9576 - val_loss: 1.4630 - val_acc: 0.7538\n",
      "Epoch 181/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0868 - acc: 0.9582 - val_loss: 1.4668 - val_acc: 0.7578\n",
      "Epoch 182/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0850 - acc: 0.9605 - val_loss: 1.4474 - val_acc: 0.7569\n",
      "Epoch 183/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.0858 - acc: 0.9589 - val_loss: 1.5167 - val_acc: 0.7551\n",
      "Epoch 184/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.0852 - acc: 0.9591 - val_loss: 1.4628 - val_acc: 0.7600\n",
      "Epoch 185/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.0848 - acc: 0.9586 - val_loss: 1.4592 - val_acc: 0.7609\n",
      "Epoch 186/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.0850 - acc: 0.9575 - val_loss: 1.4572 - val_acc: 0.7609\n",
      "Epoch 187/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.0846 - acc: 0.9579 - val_loss: 1.4647 - val_acc: 0.7591\n",
      "Epoch 188/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.0836 - acc: 0.9598 - val_loss: 1.4625 - val_acc: 0.7622\n",
      "Epoch 189/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.0855 - acc: 0.9571 - val_loss: 1.4511 - val_acc: 0.7618\n",
      "Epoch 190/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.0828 - acc: 0.9595 - val_loss: 1.4763 - val_acc: 0.7542\n",
      "Epoch 191/200\n",
      "8982/8982 [==============================] - 1s 144us/step - loss: 0.0843 - acc: 0.9575 - val_loss: 1.4726 - val_acc: 0.7618\n",
      "Epoch 192/200\n",
      "8982/8982 [==============================] - 1s 149us/step - loss: 0.0859 - acc: 0.9590 - val_loss: 1.4808 - val_acc: 0.7578\n",
      "Epoch 193/200\n",
      "8982/8982 [==============================] - 1s 146us/step - loss: 0.0848 - acc: 0.9589 - val_loss: 1.4669 - val_acc: 0.7565\n",
      "Epoch 194/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0853 - acc: 0.9567 - val_loss: 1.4668 - val_acc: 0.7582\n",
      "Epoch 195/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.0845 - acc: 0.9584 - val_loss: 1.4566 - val_acc: 0.7618\n",
      "Epoch 196/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.0858 - acc: 0.9586 - val_loss: 1.4663 - val_acc: 0.7587\n",
      "Epoch 197/200\n",
      "8982/8982 [==============================] - 1s 145us/step - loss: 0.0847 - acc: 0.9588 - val_loss: 1.4708 - val_acc: 0.7596\n",
      "Epoch 198/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.0832 - acc: 0.9589 - val_loss: 1.4695 - val_acc: 0.7569\n",
      "Epoch 199/200\n",
      "8982/8982 [==============================] - 1s 147us/step - loss: 0.0823 - acc: 0.9580 - val_loss: 1.4821 - val_acc: 0.7609\n",
      "Epoch 200/200\n",
      "8982/8982 [==============================] - 1s 148us/step - loss: 0.0821 - acc: 0.9591 - val_loss: 1.4665 - val_acc: 0.7622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe62b350358>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense\n",
    "\n",
    "a = Input(shape=(1000,))\n",
    "b = Dense(256,activation='relu')(a)\n",
    "c = Dense(256,activation='relu')(b)\n",
    "d = Dense(46,activation='softmax')(c)\n",
    "\n",
    "model3=Model(inputs=a, outputs=d)\n",
    "sgd=keras.optimizers.SGD(lr=0.01)\n",
    "model3.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model3.fit(x_train,y_train,epochs=200,batch_size=32,verbose=1,validation_data=(x_test,y_test))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "TP_ke_MLP_empty_Jizhe.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
