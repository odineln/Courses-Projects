{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iafPdtuncbq7"
   },
   "source": [
    "<h1><center>Practice of Large Scale Machine Learning<center></h1>\n",
    " <h1><center>TP4.3 CIFAR10 classification using a CNNs<center></h1>\n",
    "<h2><center>ATHENS 2018<center></h2>\n",
    "Email address for sending back the TPs: attilio.fiandrotti@telecom-paristech.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I4VrCB5La5rD"
   },
   "source": [
    "# Installing and importing Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WMFnWz-UvCvq"
   },
   "outputs": [],
   "source": [
    "# This shell command will install the keras package into our VM (if not already installed)\n",
    "# Mind the \"!\" escape character at the beginning of the line\n",
    "!pip install -q keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OlKZ3Hnas7B4"
   },
   "outputs": [],
   "source": [
    "# Importing the Keras main module: different backends will have different data ordering:\n",
    "# theano backend: NCHW\n",
    "# tensorflow backend: NHWC \n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_QLz9_jbRZq"
   },
   "source": [
    "# Loading and preparing the CIFAR10 dataset\n",
    "Load the CIFAR10 dataset, normalize the input images with respect to mean and standard deviation and convert labels to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "gG83hGyVmijn"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# The CIFAR dataset is ready to be imported from Keras into RAM\n",
    "from keras.datasets import cifar10\n",
    "# START CODE HERE\n",
    "...\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5VAu7oW0Zu4"
   },
   "outputs": [],
   "source": [
    "# Let us visualize the first training sample using the Gnuplot library\n",
    "from matplotlib import pyplot as plt\n",
    "# START CODE HERE\n",
    "...\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQbkllF8mnaf"
   },
   "outputs": [],
   "source": [
    "# Conversion to one-hot encoding\n",
    "from keras.utils.np_utils import to_categorical\n",
    "# START CODE HERE\n",
    "...\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptTRSDo5nJyZ"
   },
   "outputs": [],
   "source": [
    "# Reshape to proper images with 1 color channel according to backend scheme\n",
    "img_rows, img_cols, channels = 32, 32, 3\n",
    "# START CODE HERE\n",
    "...\n",
    "# END CODE HERE\n",
    "\n",
    "# Now let us normalize the image in the [0-1] range\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "# START CODE HERE\n",
    "...\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwm1OFOtc4uU"
   },
   "source": [
    "# Defining the neural network architecture (i.e., the network model)\n",
    "Create an LeNet5-like convolutional neural network suitable to classify each image across 10 different classes and such that convolution layers do not yield reduced-size of feature maps.\n",
    "Then, instantiate a rmsprop optimizer with initial LR of 10^-4 and using the appropriate loss function and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pnd3q1V3nk8v"
   },
   "outputs": [],
   "source": [
    "# The Sequential module is sort of a container for more complex NN elements and\n",
    "# defines a loop-less NN architecture\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "\n",
    "# The optimizers module provides a number of optimization algorithms for updating\n",
    "# a netwok parameters accoridng to teh computed error gradints\n",
    "from keras import optimizers\n",
    "\n",
    "input_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (5, 5)\n",
    "# Number of filters in first convolutional layer\n",
    "num_kernel_first_conv_layer = ...\n",
    "# Number of filters in second convolutional layer\n",
    "num_kernel_second_conv_layer = ...\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# START CODE HERE\n",
    "...\n",
    "# END CODE HERE\n",
    "\n",
    "# Defining our SGD optimizer\n",
    "\n",
    "# START CODE HERE\n",
    "...\n",
    "# END CODE HERE\n",
    "\n",
    "# Let us have a look at the model topology\n",
    "#model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2uUNj3mU3Unn"
   },
   "source": [
    "# Creating preprocessor for image augmentation\n",
    "Instantiate an ImageDataGenerator called datagen from package keras.preprocessing.image using 10% random scaling and horizontal flipping as random transformations.\n",
    "Then, fit the instantiated ImageDataGenerator to the train images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "akj8_S5FGCCK"
   },
   "outputs": [],
   "source": [
    "# Creating a batch preprocessor for augmenting the trainig data\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ...\n",
    "# START CODE HERE\n",
    "...\n",
    "# END CODE HERE\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "# START CODE HERE\n",
    "...\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_AWUAW4idF3D"
   },
   "source": [
    "# Training the network\n",
    "Train the model for 10 epochs using batches of 32 images using the fit_generator() method against the instantiated ImageDataGenerator and validating the model at each epoch and keeping track of the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTHrbb7uFYWz"
   },
   "outputs": [],
   "source": [
    "# Dimension of the batch size (number of images over which error gradients are averaged)\n",
    "batch_size = 100\n",
    "# We train the model for 10 epochs\n",
    "epochs = 10\n",
    "\n",
    "# START CODE HERE\n",
    "...\n",
    "# END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODUc5Bq_dMEq"
   },
   "source": [
    "# Visualizing the network performance\n",
    "Visualize the training history using the pyplot package: plot in one graph the train and vaidation loss functions, in another graph the train and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QdJrRbyariEw"
   },
   "outputs": [],
   "source": [
    "# We now want to plot the train and validation loss functions and accuracy curves\n",
    "#print(history.history.keys())\n",
    "\n",
    "# summarize history for loss\n",
    "# START CODE HERE\n",
    "...\n",
    "# END CODE HERE\n",
    "\n",
    "# summarize history for accuracy\n",
    "# START CODE HERE\n",
    "...\n",
    "# END CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nr4TdWoEoDzi"
   },
   "source": [
    "# Experiments\n",
    "\n",
    "* Note down the performance of the network trained with data augmentation (horizontal flip) in terms of training and validation accuracy as a reference.\n",
    "* Disable data augmentation and measure what is the the loss in performance.\n",
    "* What happens if vertical image flipping is used to augment the dataset in place of horizontal image flipping ? Why ?\n",
    "* Save the trained model (architecture + learned weights to disk) using the os package functions to manipulate the file system if needed\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ikYfGryVIR5u"
   },
   "source": [
    "# Save the trained model\n",
    "Save the best trained model you found (architecture + learned weights to disk) using the os package functions to manipulate the file system if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BuUBZ9Jq-7do"
   },
   "outputs": [],
   "source": [
    "# Save model and weights\n",
    "import os\n",
    "model_name = 'keras_cifar10_trained.h5'\n",
    "# START CODE HERE\n",
    "...\n",
    "# END CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TP4_3_empty.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
