{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP4_2_empty.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "I4VrCB5La5rD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "iafPdtuncbq7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1><center>Practice of Large Scale Machine Learning<center></h1>\n",
        "  <h1><center>TP4.2 MNIST classification using a CNN<center></h1>\n",
        "<h2><center>ATHENS 2018<center></h2>\n",
        "  Email address for sending back the TPs: attilio.fiandrotti@telecom-paristech.fr"
      ]
    },
    {
      "metadata": {
        "id": "I4VrCB5La5rD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Installing and importing Keras"
      ]
    },
    {
      "metadata": {
        "id": "WMFnWz-UvCvq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This shell command will install the keras package into our VM (if not already installed)\n",
        "# Mind the \"!\" escape character at the beginning of the line\n",
        "!pip install -q keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OlKZ3Hnas7B4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Importing the Keras main module: different backends will have different data ordering:\n",
        "# theano backend: NCHW\n",
        "# tensorflow backend: NHWC \n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s_QLz9_jbRZq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading and preparing the MNIST dataset\n",
        "Load the MNIST dataset made available by keras.datasets\n",
        "Verify the amount of system memory available before and after loading the dataset."
      ]
    },
    {
      "metadata": {
        "id": "gG83hGyVmijn",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# The MNSIT dataset is ready to be imported from Keras into RAM\n",
        "# Warning: you cannot do that for larger databases (e.g., ImageNet)\n",
        "from keras.datasets import ...\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zF6aLKWsDJlN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the pyplot package, visualize the fist sample of the trainig set"
      ]
    },
    {
      "metadata": {
        "id": "x5VAu7oW0Zu4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let us visualize the first training sample using the Gnuplot library\n",
        "from matplotlib import pyplot as plt\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tqtZCOxcDS0U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Turn train and test labels to one-hot encoding"
      ]
    },
    {
      "metadata": {
        "id": "lQbkllF8mnaf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Do you remember about one-hot encoding ?\n",
        "from keras.utils.np_utils import to_categorical\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sTSbzJ8zDi0b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reshape the train and test images to the correct 4-dimensional NHWC format using the reshape() method.\n",
        "Then, normalize the images so that they have zero mean and standard deviation equal to 1 (also approximate solutions are acceptable)"
      ]
    },
    {
      "metadata": {
        "id": "ptTRSDo5nJyZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reshape to proper images with 1 color channel according to backend scheme\n",
        "img_rows, img_cols = 28, 28\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "\n",
        "# Now let us normalize the image in the [0-1] range\n",
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uwm1OFOtc4uU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Defining the neural network architecture (i.e., the network model)\n",
        "Create a LeNet5-like convolutional neural network taking in input the images as matrices of pixels and suitable to classify each image across 10 different classes.\n",
        "Then, instantiate a SGD optimizer with a tentative LR of 10^-3 and using the appropriate loss function and compile the model.\n",
        "Calculate the coputational complexity of the convolutional layer(s) and of the first fully connected layer and compare it with that of the previous exercise."
      ]
    },
    {
      "metadata": {
        "id": "Pnd3q1V3nk8v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The Sequential module is sort of a container for more complex NN elements and\n",
        "# defines a loop-less NN architecture\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "\n",
        "# The optimizers module provides a number of optimization algorithms for updating\n",
        "# a netwok parameters accoridng to teh computed error gradints\n",
        "from keras import optimizers\n",
        "\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "# size of pooling area for max pooling\n",
        "pool_size = (2, 2)\n",
        "# convolution kernel size\n",
        "kernel_size = (5, 5)\n",
        "# Number of filters in first convolutional layer\n",
        "num_kernel_first_conv_layer = 6\n",
        "# Number of filters in second convolutional layer\n",
        "num_kernel_second_conv_layer = 16\n",
        "\n",
        "nb_classes = 10\n",
        "\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "\n",
        "# Defining our SGD optimizer\n",
        "\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "\n",
        "# Let us have a look at the model topology\n",
        "#model.summary()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_AWUAW4idF3D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training the network\n",
        "Train the model for 10 epochs and over 10k training samples initially only using the fit() method, validating the model at each epoch and keeping track of the training history"
      ]
    },
    {
      "metadata": {
        "id": "gTHrbb7uFYWz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This is where the actual training-testing happens\n",
        "# Dimension of the batch size (number of images over which error gradients are averaged)\n",
        "batch_size = 100\n",
        "# Number of epochs we want to train\n",
        "epochs = 10\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ODUc5Bq_dMEq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Visualizing the network performance\n",
        "Visualize the training history using the pyplot package: plot in one graph the train and vaidation loss functions, in another graph the train and validation accuracy"
      ]
    },
    {
      "metadata": {
        "id": "QdJrRbyariEw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We now want to plot the train and validation loss functions and accuracy curves\n",
        "\n",
        "# summarize history for loss\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "plt.show()\n",
        "\n",
        "# summarize history for accuracy\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nr4TdWoEoDzi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiments\n",
        "\n",
        "Note down the performance of the trained network in terms of training and validation accuracy as a reference. Then, experiment as follow and compare performance with the reference scenario.\n",
        "\n",
        "*   **Filter size**: experiment with square filters of different size and compare performance with reference scenario.\n",
        "*   **Number of filters**: experiment increasing the number of filters in the first and second layer and find the maximum number of filters the network can tolerate before overfitting to the training samples.\n",
        "* **Padding**: experiment withnarrow and wide convolutions: what changes in terms of featuremap size ?\n",
        "*  **Pooling layers**: expeirment with different pooling layers (maxpooling and avgpooling): which one yield the best performance ?\n",
        "What happens if the pooling layers are removed altogether in terms of comlexity-performance tradeoff ?\n",
        "* **Pooling-less architectures**: Modify the network architecture to obtain a twofold reduction of each featuremap without resorting to pooling layers (hint: take insipiration from the ResNet architecture).\n",
        "* **Confusion analysis**: Using the proper metric  from sklearn, check which character is most frequently confused with which: can you explain why ?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "r0C1J6R1Elfc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Example of a confusion matrix using sklearn.metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "predictions = model.predict(...)\n",
        "# Mind that confusion_matrix requires\n",
        "# START CODE HERE\n",
        "...\n",
        "# END CODE HERE\n",
        "print (matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}